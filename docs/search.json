[
  {
    "objectID": "pages/guides/guides.html",
    "href": "pages/guides/guides.html",
    "title": "Guides",
    "section": "",
    "text": "Welcome to the Guides section of the website! This is the landing page for step-by-step guides and instructions for various tools and workflows that I‚Äôve used in the past or am currently exploring.",
    "crumbs": [
      "Guides"
    ]
  },
  {
    "objectID": "pages/guides/guides.html#future-guides-planned",
    "href": "pages/guides/guides.html#future-guides-planned",
    "title": "Guides",
    "section": "Future Guides (Planned)",
    "text": "Future Guides (Planned)\nHere are some topics I plan to cover in the future:\n\nQuarto\nDocker\nPostgreSQL\nuv\n\nStay tuned for updates!",
    "crumbs": [
      "Guides"
    ]
  },
  {
    "objectID": "pages/guides/guides.html#current-guides",
    "href": "pages/guides/guides.html#current-guides",
    "title": "Guides",
    "section": "Current Guides",
    "text": "Current Guides",
    "crumbs": [
      "Guides"
    ]
  },
  {
    "objectID": "pages/guides/posts/uv.html",
    "href": "pages/guides/posts/uv.html",
    "title": "uv, the Python Project and Package Manager",
    "section": "",
    "text": "A basic guide on using uv the package and projects manager for Python developers.",
    "crumbs": [
      "Guides",
      "uv"
    ]
  },
  {
    "objectID": "pages/guides/posts/uv.html#introduction",
    "href": "pages/guides/posts/uv.html#introduction",
    "title": "uv, the Python Project and Package Manager",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\nVS Code Shortcuts\n\n\n\nIf you‚Äôre using VS Code, here are some useful shortcuts. - Note, use CMD-K CMD-S to open the keyboard shortcuts. - SHFT-CMD-i inserts a code block\n\n\nuv is an Open Source project by Astral, the makers of ruff, that is self described (and worthy of the title) as an extremely fast Python package and project manager, written in Rust.\n\nüöÄ A single tool to replace pip, pip-tools, pipx, poetry, pyenv, twine, virtualenv, and more.\n‚ö°Ô∏è 10-100x faster than pip.\nüêç Installs and manages Python versions.\nüõ†Ô∏è Runs and installs Python applications.\n‚ùáÔ∏è Runs scripts, with support for inline dependency metadata.\nüóÇÔ∏è Provides comprehensive project management, with a universal lockfile.\nüî© Includes a pip-compatible interface for a performance boost with a familiar CLI.\nüè¢ Supports Cargo-style workspaces for scalable projects.\nüíæ Disk-space efficient, with a global cache for dependency deduplication.\n‚è¨ Installable without Rust or Python via curl or pip.\nüñ•Ô∏è Supports macOS, Linux, and Windows.\n\nI‚Äôm only just beginning to learn and use the tool in my own projects (including converting my existing project environments to uv) and from what I‚Äôve seen it‚Äôs going to make life much easier. That being said, while you overwrite the muscle memory developed for years with pip and venv, there will be some growing pains; however, for those who are less familiar with what I‚Äôm talking about, I‚Äôll still explain some basic concepts and snags that I both run and ran into.",
    "crumbs": [
      "Guides",
      "uv"
    ]
  },
  {
    "objectID": "pages/guides/posts/uv.html#basic-workflow-and-guide",
    "href": "pages/guides/posts/uv.html#basic-workflow-and-guide",
    "title": "uv, the Python Project and Package Manager",
    "section": "Basic workflow and guide",
    "text": "Basic workflow and guide\n\nConcepts to Know Before Getting Started\n\nBasic knowledge of directories, bash (zsh in the case of MacOS), and using the CLI bash\nBasic knowledge of Python, common project structures, and simple workflows Python\nBasic knowledge of git (for local version control) and GitHub (for collaboration) git and GitHub basics\n\n\n\nInitializing a Project\n\nLocal Repository\nThe nice thing about uv is that it‚Äôs designed to make Python development easier, so there aren‚Äôt any head-scratching gotchas.\nFor the sake of this example and entire template, let‚Äôs assume I‚Äôm currently sitting in my main directory. For some that might be home, others app, for MacOS the default is /usr/yourusername, or maybe you prefer to put all projects in a Documents or Projects folder. Anyways, to start up a project you can do one of two things:\n\nHave uv do everything, and then change directories\n\nuv init uv_basic\ncd uv_basic\n\nCreate the directory, change directories, and then have uv do everything\n\nmkdir uv_basic\ncd uv_basic\nuv init\n\n\nThis will create 4 files and initalize a local git repository:\n\n.python-version\n.pyproject.toml\nhello.py\nREADME.md\n.git\n.gitignore\n\n\n\n\n\n\n\nuv and the .gitignore file\n\n\n\nThe nice thing about uv is that it autopopulates your .gitignore file with a few files and patterns, not to mention, it provides some basic tagging for what it puts in there. Just open the file (it‚Äôs plain text) to see. Since I‚Äôm saving my progress with this repo using git, I want to keep the overall file size down. So, I also included the .html and .ipynb file that Quarto generates because they can get large fast. Additionally, when you initialize your GitHub repo with the CLI‚Äôs repo creation process, I don‚Äôt include a README or .gitignore, because those are included in uv init.\n\n\n\n\nRemote Repository\nFor anyone familiar with software development you‚Äôve probably heard of GitHub or GitLab. I‚Äôm more familiar, professionally and personally, with GitHub (which is what I‚Äôll be using in this example); however, there are a large amount of people that prefer GitLab because it is better for some enterprise and personal use cases‚Äì GitHub vs.¬†GitLab. For this, you‚Äôll want to install the GitHub CLI. Then, you can follow along.\n\nVerify the installations and make sure to get your credentials setup, in git\n\nwhich gh and which git\n\nAdd your name and email\n\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your-email@example.com\"\n\nAuthenticate access to GitHub\n\ngh auth login\nUsing the CLI option, follow the instructions\nSelect HTTPS for the easier connection option\n\nVerify you have proper access to your GitHub\n\ngh auth status\n\n\ngithub.com\n  ‚úì Logged in to github.com account itsmeChis (keyring)\n  - Active account: true\n  - Git operations protocol: https\n  - Token: gho_************************************\n  - Token scopes: 'delete_repo', 'gist', 'read:org', 'repo', 'workflow'\n\ngh repo list\nAssuming you haven‚Äôt, create your project repo from the CLI (you can also do so using the GitHub.com GUI, but I prefer this way to reinforce my learning)\n\ngh repo create\nCreate a new repository from scratch\nuv basic\noptional description\nPublic\nGNU Affero General Public License v3.0 Which license do you need? \n\nSet the newly created repo as the local git repo‚Äôs upstream\n\nThis will result in an error (git pull)\nSet the global config to merge git pull\ngit pull with a commit message\ngit status to verify\ngit push\n\n\n\n\n\nAdding and managing dependencies\nThus far, the workflow with uv isn‚Äôt too dissimilar from using pip and venv, but managing dependencies and testing scripts is where uv shines. As you‚Äôll see below, with pip and venv, you have to manually create the virtual environment, activate it, install dependencies, manage requirements files, and then run your script. With uv, however, almost all of that is done automatically and things like uv pip list or uv venv are only there for backwards compatibility. A lot of the tedious pieces of the DevOps workflow are now obsolete or handled in the background.\n\nUsing pip and venv\nWhen using a combination of pip and venv, your typical workflow is straightforward, but becomes complicated if you need to uninstall certain packages or make quick, iterative tests of code.\nmkdir uv_basic\ncd uv_basic\npython -m venv .venv\nsource .venv/bin/activate\npip install duckdb\npip install numpy\npip freeze &gt; requirements.txt\npython script.py\n\n# Realize you don't need numpy, so you want to uninstall it and keep your environment cleaner\ndeactivate\nrm -r .venv\npython -m venv .venv\nsource .venv/bin/activate\n# Two options here, delete numpy from requirements.txt, not scalable with many packages, or reinstall just duckdb, also not scaleable\npip install duckdb\npip freeze &gt; requirements.txt\npython script.py\nAs you can see, the initial workflow isn‚Äôt horrible, but if you need to make a change to the environment or just want to test something small, the number of steps quickly multiplies.\n\n\nUsing uv\nCompare that with the streamlined uv workflow.\nuv init uv_basic\ncd uv_basic\nuv add duckdb\nuv add numpy\nuv run script.py\nuv remove numpy\nuv run script.py\nThe workflow improvements and efficiency should be obvious. The nice thing is that uv functions as your standalone virtual environment, without the need for activation or deactivation. Using uv add will add a dependency to both your pyproject.toml file and your uv.lock file. Additionally, if you are more familiar with verifying using pip, running uv pip list will show that the package is there (although the pip functionality is obsolete and only for backwards compatibility at this point). If you want to remove a package, simply use uv remove and that will also remove it from the .toml and .lock files. The last feature you‚Äôll need to understand (to use uv at a basic level) is uv sync. Simply put, it syncs your environment with the project‚Äôs dependencies/lock file. This ensures that the exact versions specified in your lockfile are used in your environment‚Äì dependencies may be added, removed, or updated if there are updates to the declared dependencies.\nTo cap this off, here are some common use cases for uv sync: - Run uv sync (without ‚Äìfrozen) to keep dependencies up-to-date and to resolve changes. - Use uv sync ‚Äìfrozen to validate dependencies without altering them\n\n\n\nConverting your Legacy Projects to uv\nNow that you‚Äôve seen the benefits of uv, as well as the workflow differences, you probably want to give it a try or even convert entire projects to uv. The good news is that this is simple and only requires a few modifications to get things up and running. The general workflow is the same as I outlined above, you‚Äôll just be cleaning up your local environment and reinstalling things along the way. The project I converted to use uv for this example utilizes DuckDB and dbt for the database and data modeling/ETL. I‚Äôll include some dbt specific information, for example if you move your database file from a subdirectory to the main one, remember to update your dbt profiles in your global dbt location.\n\nChange directories to your specific project directory\nRun uv init, it will create any file or folder that isn‚Äôt currently in the main folder\n\nIf you already have a .git folder and commit history, uv will not delete or overwrite the original folder.\n\nAdd all of the dependencies you need, then remove your requirements file (it‚Äôs no longer needed)\n\nAs of writing this, I wasn‚Äôt sure how to use uv add with the legacy requirements file, uv pip install -r kind of worked, but didn‚Äôt actually add the dependencies to the .toml or .lock files\nThere must be an easier way to bulk add dependencies, but I manually did it\nIn my case, I had to remember to add both dbt and dbt-duckdb, so the adapter would work\n\nInstall all of the CLI tools that you need, and don‚Äôt want or use globally\n\nIn my case, I need jupyter, quarto, and dbt, but I also have the latter two installed globally\n\nVerify that uv can run things correctly\n\nI first used uv run hello.py to verify that the basic functionality is there\nThen, I ran a more complex script, that imports and uses duckdb, to ensure the packages are installing and running as intended\nThen, I used uv tool list to verify which CLI tools are installed\nFinally, I verified that the CLI tools work, by using uv run dbt run --select transform to test dbt model functionality in uv\n\n\n\n\nFinal Thoughts\nSo that‚Äôs it! Overall, uv is incredibly easy to setup and configure because it builds on the classic workflows, while simplifying or abstracting some of the process. You also saw how easy it is to start using uv with older projects that use the legacy workflow. At the time of writing this, I‚Äôve only been using uv for a few days, so I‚Äôm sure there are things I got wrong or missed, please comment to let me know!\nI‚Äôm happy to chat and love learning about data, as well as what folks in this space are working on. Connect with me on Bluesky @chriskornaros.bsky.social to follow along with what I‚Äôm working on, learning, or just to say hi! Below are some other notes and thoughts I had while working on this write up.\n\nGeneral Notes\n\nIt seems that while tools are specific to a uv project instance (i.e.¬†uv_basic returns the .venv dir when asking which jupyter, but test before intalling anything say it can‚Äôt be found), when you use uv tool install it installs it to the system wide uv\nuv pip list defaults to the global (non-uv or non-pip) python environment (in my case it‚Äôs pip and wheel), but once you install something (using add, pip install, etc.) it switches the context to the current parent uv dir (i.e.¬†test, instead of uv_basic)\n\nTools are still listed even after this\n\nuv tool install only works when installing python package specific tools, but DuckDB for Python (for example) doesn‚Äôt come packaged with the DuckDB CLI tools, so uv tool install duckdb won‚Äôt install the DuckDB CLI features\nIt seems that saving variable with duckdb.sql(‚Ä¶).show() and then printing the type of that, just prints the query output, insteaed of the type\nBased on tests, the workflow changes are as follows",
    "crumbs": [
      "Guides",
      "uv"
    ]
  },
  {
    "objectID": "pages/about/about.html",
    "href": "pages/about/about.html",
    "title": "About",
    "section": "",
    "text": "Thanks for visiting! As you can see, this site is pretty messy. That is because my programming skills lie with data, not web design. I‚Äôve thrown this together with the help of Quarto documentation, SASS documentation, Bootstrap Documentation, _brand.yml documentation, and of course LLMs. Feel free to report issue as you find them, or suggest improvements because there is a link to the source repo on every page."
  },
  {
    "objectID": "pages/about/about.html#about-me",
    "href": "pages/about/about.html#about-me",
    "title": "About",
    "section": "About Me",
    "text": "About Me\nI‚Äôm a 27 year old Data Scientist, Engineer, Analyst, Architect, etc. that has two degrees from Tulane University in New Orleans‚Äì BSM, Marketing and Asian Studies; MS, Business Analytics. During COVID, I did an extra year of school and received a Masters in Data Science. As a result of my education, I fell in love with data science and machine learning, but due to common data challenges, I developed a professional passion and skillset for database architecture and data engineering.\nI currently work as a Senior Data Analyst at US Foods in their enterprise data organization, primarily supporting the Safety and Warehousing domains. I previously worked at General Motors in the Global Markets IT organization. Most of the projects I work on outside of US Foods use different tools and skillsets than I do during my day job. The reason being that I want to stay up to date with all of the tools and changes in the industry. As a result, I get to play with all of the cool licensed tools like dbt,Snowflake, Databricks, and Power BI at work, but I get to build and learn with open source tools externally.\nSince really jumping in to open source development and learning in 2024, I‚Äôve come to enjoy the study and field of data even more. I know that long term I want to move into a data strategy and innovation role. I‚Äôm also interested in eventually pursuing a PhD, either in a Data Engineering capacity or looking at performance optimization for data applications‚Äì think using duckdb instead of pandas. Designing and architecting systems that enable people to get more out of their data is something I‚Äôm teaching myself now. That being said, I‚Äôm enthralled with both the process and my progress.\nIn the long run, I want to continue working full time as a Data Engineer (or something similar). That being said, for the time being, I am interested in advising or consulting both individuals and companies on data strategy.\nYou can take a look at my resume down below. My email is listed."
  },
  {
    "objectID": "pages/projects/data_engineering/data_engineering.html",
    "href": "pages/projects/data_engineering/data_engineering.html",
    "title": "Data Engineering and Architecture Projects",
    "section": "",
    "text": "Landing page for all my project posts related to Data Engineering and Architecture.",
    "crumbs": [
      "Data Engineering and Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/data_engineering.html#projects",
    "href": "pages/projects/data_engineering/data_engineering.html#projects",
    "title": "Data Engineering and Architecture Projects",
    "section": "Projects",
    "text": "Projects",
    "crumbs": [
      "Data Engineering and Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html",
    "title": "NFL Big Data Bowl 2025",
    "section": "",
    "text": "I will add to this later. Currently, this is a rough combination of my early notebooks.",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#introduction",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#introduction",
    "title": "NFL Big Data Bowl 2025",
    "section": "",
    "text": "I will add to this later. Currently, this is a rough combination of my early notebooks.",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#ongoing-thoughts",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#ongoing-thoughts",
    "title": "NFL Big Data Bowl 2025",
    "section": "Ongoing Thoughts",
    "text": "Ongoing Thoughts\nThis is the first time I‚Äôm adding to a unified document, it‚Äôs December 13th, or about 1 month into my project. As of now, Random Forest definitely seems like the best path forward; however, the intial version certainly overfit. I believe the model overfit because some of the plays columns are the pre/post snap home/away team win probability values. In my next iteration, I‚Äôm going to remove those values, and in the future I might even try to recreate them. That being said, there‚Äôs a little under one month to go, so I‚Äôm going to focus on putting together some kind of deliverable/submission, before I go off the deep end. That said, this page and the website in general are going to be sloppy as I figure things out and slowly improve the organization and UI.",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#exploratory-data-analysis-and-initial-thoughts",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#exploratory-data-analysis-and-initial-thoughts",
    "title": "NFL Big Data Bowl 2025",
    "section": "Exploratory Data Analysis and Initial Thoughts",
    "text": "Exploratory Data Analysis and Initial Thoughts\n\n\n\n\n\n\nNote\n\n\n\nThis was written on November 26th, 2024. It was later added to this site on December 13th, 2024. This is a general write up on the project, you can see the full notebooks below. The full repo is available here.\n\n\nCurrently, I‚Äôve made solid progress with my initial exploratory data analysis and project configuration. Here are some quick notes about the setup of my project environment (from IDE to tools/versions). - Using VS Code with the Jupyter, Jinja, YAML, Quarto (for notes/project submissions), and dbt extensions. - DuckDB is my primary database tool (for now), with dbt for the data modeling - Then, I‚Äôm using Python and Jupyter Notebooks for the analysis/ML component\nThe reason I may switch to PostgreSQL for the primary Database is to just gain experience with DuckDB as a DEV environement and Postgres for PROD. Realistically, however, for the scope of this project DuckDB accomplishes everything I need it to.\nFor the forseeable future, the only side project I‚Äôll be working on is this, so my next few posts will only look at the project progress and my thoughts about the Big Data Bowl, feel free to checkout the GitHub repository where I‚Äôm saving my work.\nSome notes about my current project progress: - The project folder has a few subdirectories, including nfl_dbt which is the dbt project folder - The raw data came in the form of 13 CSVs from Kaggle. 4 of which are 50mb or less, 9 of which are ~1gb. - I‚Äôm using Databricks‚Äô ‚ÄúMedallion Architecture‚Äù to guide my data modeling workflow. - I built the initial dbt models, using DuckDB as the DEV target (enabling 4 threads) and loaded the ‚Äúbronze‚Äù schema which contains the 13 raw tables - I aggregated the data into the ‚Äúsilver‚Äù schema, which contains an aggregated play data table - I further aggregated the data into the ‚Äúgold‚Äù schema, which provides basic analytic tables - Currently, I completed an initial analysis using an EDA notebook where I looked at using a LinearRegression and KNN to compare pre-snap play data with play outcomes. - I settled on a KNN model, but I‚Äôm only seeing about a 61.1% accuracy rate (confusion matrix and explanation below).\nSo, I‚Äôm at a bit of a crossroads, with a few ways forward. It may be simpler (for the initial project/submission) to build a linear regression model that takes pre-snap play data as features, and then looks at yards gained (or loss) for the output. Conversely, if I stick with the KNN model I‚Äôll need to make some changes. The majority of the outputs are either Gain or Completed, which refer to a positive rushing play and a completed pass, respectively. The issue here, the model overwhelmingly predicts those values, but fails to accurately predict things like Touchdowns, Sacks, or Interceptions.\nSo, I may need to limit possible play outcomes, or at least combine some categories (i.e.¬†Turnover for Fumble + Interception). Or, add some more presnap data, such as down and distance (I currently only use starting yard line, along with categorical data). If you made it this far, thank you! Below is the confusion matrix output from my current KNN model. I‚Äôll add some hashtags at the end as an experiment too, because I‚Äôm not sure if that will help with post discoverability and/or integrate with Bluesky feeds.",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#knn-classifier-notebook-first-model",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#knn-classifier-notebook-first-model",
    "title": "NFL Big Data Bowl 2025",
    "section": "KNN Classifier Notebook (First Model)",
    "text": "KNN Classifier Notebook (First Model)\n# Import dependencies\nimport duckdb\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n# Open the connection to the persistent database\ncon = duckdb.connect(\".config/nfl.duckdb\")\ncon.close()\n# Create the initial dataframe object with DuckDB\ndf = con.sql(\"\"\"\n    SELECT *\n    FROM gold.plays_numeric             \n\"\"\").df()\n\ndf.head()\n\n\n\n\n\n\n\n\ngameId\n\n\nplayId\n\n\npossessionTeam\n\n\nyardlineNumber\n\n\noffenseFormation\n\n\nreceiverAlignment\n\n\nplayType\n\n\ndefensiveFormation\n\n\npff_manZone\n\n\nyardsGained\n\n\nplayOutcome\n\n\n\n\n\n\n0\n\n\n2022102302\n\n\n2655\n\n\nCIN\n\n\n21\n\n\n3\n\n\n8\n\n\n2\n\n\n6\n\n\n2\n\n\n9\n\n\n3\n\n\n\n\n1\n\n\n2022091809\n\n\n3698\n\n\nCIN\n\n\n8\n\n\n3\n\n\n8\n\n\n2\n\n\n13\n\n\n2\n\n\n4\n\n\n3\n\n\n\n\n2\n\n\n2022103004\n\n\n3146\n\n\nHOU\n\n\n20\n\n\n6\n\n\n5\n\n\n2\n\n\n13\n\n\n2\n\n\n6\n\n\n3\n\n\n\n\n3\n\n\n2022110610\n\n\n348\n\n\nKC\n\n\n23\n\n\n6\n\n\n5\n\n\n2\n\n\n13\n\n\n2\n\n\n4\n\n\n3\n\n\n\n\n4\n\n\n2022102700\n\n\n2799\n\n\nBAL\n\n\n27\n\n\n4\n\n\n7\n\n\n1\n\n\n3\n\n\n1\n\n\n-1\n\n\n2\n\n\n\n\n\n# Split the table into features and target\nX = con.sql(\"\"\"\n    SELECT yardlineNumber, offenseFormation, receiverAlignment, playType, defensiveFormation, pff_manZone\n    FROM gold.plays_numeric\n\"\"\").df()\n\ny = np.array(con.sql(\"\"\"\n    SELECT playOutcome\n    FROM gold.plays_numeric\n\"\"\").df()).ravel()\n\nprint(X.shape, y.shape)\n(16124, 6) (16124,)\n# Instantiate the model and split the datasets into training/testing\nknn = KNeighborsClassifier(n_neighbors=7)\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.7, random_state=123)\n# Fit the model\nknn.fit(X_train, y_train)\n\n\n\nKNeighborsClassifier(n_neighbors=7)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n¬†¬†KNeighborsClassifier?Documentation for KNeighborsClassifieriFitted\n\nKNeighborsClassifier(n_neighbors=7)\n\n\n\n\n\n# Basic KNN Performance Metrics\ny_pred = knn.predict(X_val)\n\nprint(knn.score(X_val, y_val))\n0.6114096734187681\n# Datacamp Model performance Loop\n# Create neighbors\nneighbors = np.arange(1, 13)\ntrain_accuracies = {}\ntest_accuracies = {}\n\nfor neighbor in neighbors:\n  \n    # Set up a KNN Classifier\n    knn = KNeighborsClassifier(n_neighbors=neighbor)\n  \n    #¬†Fit the model\n    knn.fit(X_train, y_train)\n  \n    # Compute accuracy\n    train_accuracies[neighbor] = knn.score(X_train, y_train)\n    test_accuracies[neighbor] = knn.score(X_val, y_val)\nprint(neighbors, '\\n', train_accuracies, '\\n', test_accuracies)\n# Visualize model accuracy with various neighbors\n# Add a title\nplt.title(\"KNN: Varying Number of Neighbors\")\n\n#¬†Plot training accuracies\nplt.plot(neighbors, train_accuracies.values(), label=\"Training Accuracy\")\n\n# Plot test accuracies\nplt.plot(neighbors, test_accuracies.values(), label=\"Testing Accuracy\")\n\nplt.legend()\nplt.xlabel(\"Number of Neighbors\")\nplt.ylabel(\"Accuracy\")\n\n# Display the plot\nplt.show()\n# Map the original target variables to the KNN outputs\nplay_outcome_map = con.sql(\"\"\"\n    SELECT\n    CASE\n        WHEN playOutcome = 1 THEN 'Gain'\n        WHEN playOutcome = 2 THEN 'Loss'\n        WHEN playOutcome = 3 THEN 'Completed'\n        WHEN playOutcome = 4 THEN 'Incomplete'\n        WHEN playOutcome = 5 THEN 'Scrambled'\n        WHEN playOutcome = 6 THEN 'Touchdown'\n        WHEN playOutcome = 7 THEN 'Intercepted'\n        WHEN playOutcome = 8 THEN 'Fumbled'\n        WHEN playOutcome = 9 THEN 'Sacked'\n        WHEN playOutcome = 0 THEN 'Penalty'\n        ELSE 'Unknown'  -- Optional, in case there are values not matching any condition\n    END AS playOutcome\nFROM gold.plays_numeric\n\"\"\").df()['playOutcome'].tolist()\n\nplay_outcome_map = np.unique(play_outcome_map).tolist()\n# Create a dictionary to map playOutcome values to corresponding labels\nplay_outcome_dict = {i: play_outcome_map[i] for i in range(len(play_outcome_map))}\n\n# Generate a colormap for the string labels (use 'viridis' colormap)\ncolors = plt.cm.viridis(np.linspace(0, 1, len(play_outcome_map)))\nplay_colors = dict(zip(range(len(play_outcome_map)), colors))\n\n# Create legend patches for each class label\nlegend_patches = [mpatches.Patch(color=play_colors[i], label=play_outcome_map[i]) for i in range(len(play_outcome_map))]\n\n# Assuming `y_pred` is a list of predictions, map numeric predictions to string labels\npred_labels = [play_outcome_dict[val] for val in y_pred]\n# Attempting to conduct sensitivity analysis for feature importance\nfor feature in range(6):\n    plt.figure(figsize=(10, 6))\n    plt.scatter(X_val.iloc[:, feature], y_pred, c=[play_colors[val] for val in y_pred], cmap='viridis', edgecolor='k')\n    plt.xlabel(f\"Feature {feature + 1}\")\n    plt.ylabel(\"Predicted Class\")\n    plt.yticks(range(len(play_outcome_map)), play_outcome_map)\n    plt.title(f\"Predictions by Feature {feature + 1}\")\n    plt.legend(handles = legend_patches, title=\"Actual Class\", bbox_to_anchor=(1.05, 1), loc = 'upper left')\n    plt.tight_layout\n    plt.show()\n# Your play_outcome_dict with correct mapping\nplay_outcome_dict = {\n    1: 'Gain',\n    2: 'Loss',\n    3: 'Completed',\n    4: 'Incomplete',\n    5: 'Scrambled',\n    6: 'Touchdown',\n    7: 'Intercepted',\n    8: 'Fumbled',\n    9: 'Sacked',\n    0: 'Penalty'\n}\n\n# Map the y_pred values to the corresponding labels\npred_labels = [play_outcome_dict[val] for val in y_pred]\n\n# Define the colormap based on the labels\nplay_colors = plt.cm.viridis(np.linspace(0, 1, len(play_outcome_dict)))\n\n# Combine your features (X_val) and the predictions (y_pred) into a single DataFrame\ndf_features = X_val.copy()\ndf_features['Predicted Class'] = [play_outcome_dict[key] for key in y_pred]\n\n# Create a pairplot to visualize pairwise relationships between all features\nsns.pairplot(df_features, hue='Predicted Class', palette=dict(zip(play_outcome_dict.values(), play_colors)), markers='o')\n\n# Customize the plot\nplt.suptitle('Pairplot of Features Colored by Predicted Class', y=1.02)\nplt.legend(handles = legend_patches, title=\"Actual Class\", bbox_to_anchor=(1.05, 1), loc = 'upper left')\nplt.tight_layout()\nplt.show()\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Assuming y_true contains the true labels and y_pred contains the predicted labels\n# Map numerical values to their respective class labels\ny_true_labels = [play_outcome_dict[val] for val in y_val]  # Replace y_true with your actual true labels\ny_pred_labels = [play_outcome_dict[val] for val in y_pred]\n\n# Generate the confusion matrix\ncm = confusion_matrix(y_true_labels, y_pred_labels, labels=list(play_outcome_dict.values()))\n\n# Visualize the confusion matrix\nfig, ax = plt.subplots(figsize=(10, 8))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(play_outcome_dict.values()))\ndisp.plot(cmap='viridis', ax=ax, xticks_rotation=45)\n\n# Customize the plot\nplt.title(\"Confusion Matrix of KNN Model\")\nplt.show()\n\n\n\nKNN Classifier Confusion Matrix",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#linear-regression-notebook-second-model",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#linear-regression-notebook-second-model",
    "title": "NFL Big Data Bowl 2025",
    "section": "Linear Regression Notebook (Second Model)",
    "text": "Linear Regression Notebook (Second Model)\nimport duckdb\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n# Open the DuckDB connection, to the persistent database\ncon = duckdb.connect(\".config/nfl.duckdb\")\ncon.close()\n# Test converting the play outcomes to just yards gained or lost\ncon.sql(\"\"\"\n    SELECT *\n    FROM gold.plays_numeric   \n\"\"\")\n# Can still utilize plays_numeric, just won't use the categorical outcomes as the target\nX = con.sql(\"\"\"\n    SELECT yardlineNumber, offenseFormation, receiverAlignment, playType, defensiveFormation, pff_manZone\n    FROM gold.plays_numeric   \n\"\"\").df()\ny = con.sql(\"\"\"\n    SELECT yardsGained\n    FROM gold.plays_numeric   \n\"\"\").df()\n# Train test split\n# May need to come back and apply a Standard Scaler later\n\nlinreg = LinearRegression()\nscaler = StandardScaler()\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.7, random_state = 123)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\n# Fit the model\nlinreg.fit(X_train_scaled, y_train)\n\n\n\nLinearRegression()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n¬†¬†LinearRegression?Documentation for LinearRegressioniFitted\n\nLinearRegression()\n\n\n\n\n\n# Begin testing and scoring\ny_pred = linreg.predict(X_val_scaled)\n\nmse = mean_squared_error(y_val, y_pred)\nr2 = r2_score(y_val, y_pred)\n\nprint(f\"MSE: {mse}\")\nprint(f\"R2 Score: {r2}\")\nMSE: 80.63310622289697\nR2 Score: 0.02277208083008464\nplt.scatter(y_val, y_pred)\nplt.xlabel(\"Actual Values\")\nplt.ylabel(\"Predicted Values\")\nplt.title(\"Linear Regression: Actual vs Predicted\")\nplt.show()\n\ncoefficients = linreg.coef_\nprint(f\"Coefficients: {coefficients}\")\n\n\n\nLinear Regression Scatter Plot\n\n\nCoefficients: [[ 0.05041481  0.32550574  0.04088819  1.77381568 -0.0198335  -0.16104852]]\nridge = Ridge(alpha=1.0)\nridge.fit(X_train_scaled, y_train)\ny_pred_ridge = ridge.predict(X_val_scaled)\nprint(f\"Ridge MSE: {mean_squared_error(y_val, y_pred_ridge)}\")\nRidge MSE: 80.63311884052399",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#random-forest-notebook-third-model",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#random-forest-notebook-third-model",
    "title": "NFL Big Data Bowl 2025",
    "section": "Random Forest Notebook (Third Model)",
    "text": "Random Forest Notebook (Third Model)\nimport duckdb\nimport tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n# Create the database connection\ncon = duckdb.connect(\"nfl.duckdb\")\n#con.close()\n# Creating dataframes with DuckDB, plays and player_play both have 50 columns, more ideal for a broad random forest\nX = con.sql(\"\"\"\n    SELECT quarter, down, yardsToGo, yardlineNumber, preSnapHomeScore, preSnapVisitorScore,\n    playNullifiedByPenalty, absoluteYardlineNumber, preSnapHomeTeamWinProbability, preSnapVisitorTeamWinProbability, expectedPoints,\n    passResult_complete, passResult_incomplete, passResult_sack, passResult_interception, passResult_scramble, passLength, targetX, targetY,\n    playAction, passTippedAtLine, unblockedPressure, qbSpike, qbKneel, qbSneak, penaltyYards, prePenaltyYardsGained, \n    homeTeamWinProbabilityAdded, visitorTeamWinProbilityAdded, expectedPointsAdded, isDropback, timeToThrow, timeInTackleBox, timeToSack,\n    dropbackDistance, pff_runPassOption, playClockAtSnap, pff_manZone, pff_runConceptPrimary_num, pff_passCoverage_num, pff_runConceptSecondary_num\nFROM silver.plays_rf\n\"\"\").df()\ny = np.array(con.sql(\"\"\"\n    SELECT yardsGained\n    FROM silver.plays_rf\n\"\"\").df()).ravel()\n# Having issues with NA values, the below code does a simple count using pandas, will then go back and change the query\n# As of writing this, the issue is solved; however, the dbt model for this is far from efficient\nna_counts = (X == 'NA').sum()\n\n# Optionally, filter only columns with 'NA' values for easier review\nna_counts_filtered = na_counts[na_counts &gt; 0]\nprint(na_counts_filtered, \"\\n\", X.shape, \"\\n\", y.shape) # playClockAtSnap has only 1 NA value, will just drop that row\nSeries([], dtype: int64) \n (16124, 41) \n (16124,)\n# Instantiate the model and split the data\nrf = RandomForestRegressor(warm_start=True)\n\nselector = RFE(rf, n_features_to_select=10, step=1)\nX_selected = selector.fit_transform(X, y)\n# Begin Interpretation, first with feature importance\nselected_features = X.columns[selector.support_]\nprint(selected_features)\nIndex(['yardlineNumber', 'absoluteYardlineNumber',\n       'preSnapHomeTeamWinProbability', 'expectedPoints',\n       'passResult_scramble', 'penaltyYards', 'prePenaltyYardsGained',\n       'homeTeamWinProbabilityAdded', 'visitorTeamWinProbilityAdded',\n       'expectedPointsAdded'],\n      dtype='object')\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n\n# Train the model\nrf.fit(X_train, y_train)\n\n# Make predictions\ny_pred = rf.predict(X_test)\n\n# Calculate scores\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"R^2 Score: {r2}\")\nMean Squared Error: 1.7769936744186046\nR^2 Score: 0.9766614590863065\n# Continue with the GridSearch\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n}\n\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=4)\ngrid_search.fit(X_train, y_train)\nbest_rf = grid_search.best_estimator_\n\n# Wrap a progress bar for longer Grid Searches\n\"\"\"with tqdm(total=len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf']), desc=\"GridSearch Progress\") as pbar:\n    def callback(*args, **kwargs):\n        pbar.update(1)\n\n    # Add the callback to the grid search\n    grid_search.fit(X, y, callback=callback)\"\"\"\n\nprint(grid_search.best_params_)\n{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n# Continue with the Cross Validation Score\ncv_scores = cross_val_score(rf, X_selected, y, cv=5, scoring='neg_mean_squared_error')\nprint(f\"Cross-validated MSE: {-cv_scores.mean()}\")\nCross-validated MSE: 1.9303851017196607",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/blogs/blogs.html",
    "href": "pages/blogs/blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "A landing page for various blogs, journals, or random thoughts. Some of these will be focused on specific tools or technology, others will be random thoughts or research notes.\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Blogs"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chris Kornaros",
    "section": "",
    "text": "This site is the personal portfolio and homepage for Chris Kornaros. I‚Äôm a Tulane Graduate and professional Data Engineer. On here, you‚Äôll find projects and code samples that I can share publicly, guides for various tools or workflows, journal or blog posts, and any independent research or professional updates (including my resume).\nThis website is a work in progress, so things are going to move around and break. Additionally, there is a lot that I have to still add: past guides, projects, etc. Follow me on social media (links above) to stay up to date with what I‚Äôm working on. To learn more about my background visit About, for guides on various open source tools visit Guides, to see my current and past (public) work visit Projects, and to see any blog posts (available on WhiteWind) visit Blogs."
  },
  {
    "objectID": "pages/projects/data_science/posts/titanic.html#general-overview",
    "href": "pages/projects/data_science/posts/titanic.html#general-overview",
    "title": "Titanic Disaster",
    "section": "General Overview",
    "text": "General Overview\nThe Kaggle Titanic dataset and ML competition is one that many people are familiar with, and if they‚Äôre like me, it was also their first ML project. I redid this after 3 years to get familiar with my current workflow of using git, notebooks, venvs, etc. Below I included some notes to myself. While it was overkill, I used a Dockerized environment for this project, just to increase my familiarity with containers and the docker toolset.",
    "crumbs": [
      "Data Science and Machine Learning",
      "Titanic Disaster"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/titanic.html#notes-on-the-titanic-model-and-the-process-for-using-the-jupyter-kernel-on-the-ec2-server",
    "href": "pages/projects/data_science/posts/titanic.html#notes-on-the-titanic-model-and-the-process-for-using-the-jupyter-kernel-on-the-ec2-server",
    "title": "Titanic Disaster",
    "section": "Notes on the Titanic model and the process for using the Jupyter Kernel on the EC2 Server",
    "text": "Notes on the Titanic model and the process for using the Jupyter Kernel on the EC2 Server\n\n\nFirst, navigate to Local-Scripts/.AWS/.EC2_Scripts and run the ec2_start.sh script zsh ec2_start.sh\nNext, execute the unix_test_dns.sh script to store the EC2 public DNS in the /etc/hosts file ./unix_test_dns.sh\nUse the ssh -i command to connect to the EC2 server, then run the Jupyter kernel image docker run -p 8888:8888 titanic-env Look into adding a volume mount command here to persist model/file changes in the EC2\nNow that it‚Äôs running. Use a different terminal window (or the VS Code IDE) and test the DNS name. ping unix_test.local Need to make this DNS dynamic\nIn VS Code, open the .ipynb file in the model folder and continue work.\n\nIf you need to reconnect to a kernel, use the Titanic preset.\nVS Code connects to the EC2 IPv4 address, even though the Kernel tells you 127.0.0.1\n\nThe format for connecting to the Public IPv4 is http://IPv4:8888/\nTo pull the file out of the container and store it on the EC2 server\n\ndocker cp 786853360d97:/home/files/titanic_submission.csv files/titanic_submission.csv\ndocker copy instance-id:/path/to/file local/path\n\n\n\n\nIf you need to modify the container\n\nDo so locally, or anywhere, and then push the change to the GitHub repoistory\nThen, pull the changes into the EC2 server\nClear the Docker library/cache, and then rebuild the image from scratch, use the following docker build -t titanic-env -f .config/Dockerfile .\nEnsure this is done from the main project folder and uses those flags\n\nFor Titanic, this is in the admin/Kaggle/Titanic folder in the EC2 instance\n--no-cache ensures it‚Äôs a fresh build (This will take a while, not worth it in the smaller environment. Rebuild with cache)\n-t sets the name of the image\n-f lets you specify the Dockerfile location\n. lets Docker know that your current working directory is where the build context should take place",
    "crumbs": [
      "Data Science and Machine Learning",
      "Titanic Disaster"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/titanic.html#the-code-portion-of-this-notebook",
    "href": "pages/projects/data_science/posts/titanic.html#the-code-portion-of-this-notebook",
    "title": "Titanic Disaster",
    "section": "The Code Portion of this notebook",
    "text": "The Code Portion of this notebook\n\n\n\n\n\n\nCaution\n\n\n\nWhen I most recently completed this competition, I didn‚Äôt do it with the goal in mind of doing a nice write-up. This is really just an amalgamation of the notes I made to myself on how to use/modify the Docker container I ran my model on and the actual notebook + code + notes.\n\n\nimport numpy as np\nimport pandas as pd\nimport duckdb\nimport sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n\n# Initialize a connection and create a persistent database\n# Worth noting that due to the workflow I'm using, the database was/should be created externally, and then built into the Docker container\n# This way, the raw database files are saved locally, but file size won't grow exponentially\ncon = duckdb.connect(\"files/titanic.duckdb\")\n# Using this to DROP and Recreate train_raw, ensuring a fresh process\ncon.sql(\"DROP TABLE train_raw;\")\ncon.sql(\"DROP TABLE test_raw;\")\ncon.sql(\"CREATE TABLE train_raw AS SELECT * FROM 'files/train.csv'\")\ncon.sql(\"CREATE TABLE test_raw AS SELECT * FROM 'files/test.csv'\")\n# Create working tables\n#con.sql(\"DROP TABLE train;\")\n#con.sql(\"DROP TABLE test;\")\ncon.sql(\"CREATE TABLE train AS SELECT * FROM train_raw\")\ncon.sql(\"CREATE TABLE test AS SELECT * FROM test_raw\")\n# Verify the proper tables are loaded\ncon.sql(\"SELECT * FROM duckdb_tables()\")\n# Generate summary statistics\ncon.sql(\"SUMMARIZE train\")\n#con.sql(\"SUMMARIZE test\")\n# Examine Nulls for the Age, Cabin, and Embarked columns (do this for test as well)\ncon.sql(\"SELECT * FROM train WHERE Age IS NULL\") # Seems to make the most sense to use the average age here\ncon.sql(\"SELECT * FROM train WHERE Cabin IS NULL\") # Seems likely Cabins not as strictly recorded for lower class guests, probably unnecessary for model\ncon.sql(\"SELECT * FROM train WHERE Embarked IS NULL\") # This only comprises 2 records and it's unclear if they made it on in the first place, not a high enough percentage of 1st class survivors to consider keeping\n# Update the Age column, replace NULL values with the average Age\ncon.sql(\"\"\"UPDATE train AS train_clean\n        SET Age = (\n            SELECT\n                avg(raw.Age) AS cleanAge\n            FROM train as raw\n            WHERE raw.Age IS NOT NULL\n        )\n        WHERE Age IS NULL\"\"\")\n# Update the Sex column, change the VARCHAR type to BOOLEAN\ncon.sql(\"\"\"ALTER TABLE train ALTER Sex \n        SET DATA TYPE BOOLEAN USING CASE\n            WHEN Sex = 'female' THEN 1 ELSE 0 END\n        \"\"\")\n# Update the Age column in the test dataset\ncon.sql(\"\"\"UPDATE test AS test_clean\n        SET Age = (\n            SELECT\n                avg(raw.Age) AS cleanAge\n            FROM test as raw\n            WHERE raw.Age IS NOT NULL\n        )\n        WHERE Age IS NULL\"\"\")\n# Update the Sex column, change the VARCHAR type to BOOLEAN\ncon.sql(\"\"\"ALTER TABLE test ALTER Sex \n        SET DATA TYPE BOOLEAN USING CASE\n            WHEN Sex = 'female' THEN 1 ELSE 0 END\n        \"\"\")\n# Remove the PassengerId, Name, Cabin, Embarked, Fare, and Ticket columns\ncon.sql(\"ALTER TABLE train DROP PassengerId\") # Has no bearing on the outcome of the model\ncon.sql(\"ALTER TABLE train DROP Name\") # Has to be numeric data\ncon.sql(\"ALTER TABLE train DROP Cabin\")\ncon.sql(\"ALTER TABLE train DROP Embarked\")\ncon.sql(\"ALTER TABLE train DROP Fare\") # Dropping because there are nulls in the test file\ncon.sql(\"ALTER TABLE train DROP Ticket\") # Dropping because of inconsistent values\n\n# Remove the PassengerId, Name, Cabin, Embarked, Fare, and Ticket columns\ncon.sql(\"ALTER TABLE test DROP Name\") # Has to be numeric data\ncon.sql(\"ALTER TABLE test DROP Cabin\")\ncon.sql(\"ALTER TABLE test DROP Embarked\")\ncon.sql(\"ALTER TABLE test DROP Fare\") # Dropping because there are nulls in the test file\ncon.sql(\"ALTER TABLE test DROP Ticket\") # Dropping because of inconsistent values\n\n# Creating dataframes for testing/training, I'll be using sklearn here, which needs both\ntrain = con.sql(\"SELECT * FROM train\").df()\ntest = con.sql(\"SELECT * FROM test\").df()\n# Create features and target\nX = train.drop(\"Survived\", axis = 1).values\ny = train[\"Survived\"].values\n\nX_test = test.drop(\"PassengerId\", axis = 1).values\n# Initialize Regression object and split data\nlogreg = LogisticRegression(penalty = 'l2', tol = np.float64(0.083425), C = np.float64(0.43061224489795924), class_weight = 'balanced')\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.7, random_state = 123)\n# Fit and predict\nlogreg.fit(X_train, y_train)\n\n\n\nLogisticRegression(C=np.float64(0.43061224489795924), class_weight='balanced',\n                   tol=np.float64(0.083425))\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n¬†¬†LogisticRegression?Documentation for LogisticRegressioniFitted\n\nLogisticRegression(C=np.float64(0.43061224489795924), class_weight='balanced',\n                   tol=np.float64(0.083425))\n\n\n\n\n\n# Predict and measure output\ny_pred = logreg.predict(X_val)\ny_pred_probs = logreg.predict_proba(X_val)[:, 1]\nprint(roc_auc_score(y_val, y_pred_probs))\n0.8149262043998886\n# Create Parameter Dictionary for Model Tuning\nkf = KFold(n_splits = 5, shuffle = True, random_state = 123)\nparams = {\n    \"penalty\": [\"l1\", \"l2\"],\n    \"tol\": np.linspace(0.0001, 1.0, 25),\n    \"C\": np.linspace(0.1, 1.0, 50),\n    \"class_weight\": [\"balanced\", {0:0.8, 1:0.2}]\n}\nlogreg_cv = RandomizedSearchCV(logreg, params, cv=kf)\n# Run the parameter search, fit the object, print the output\nlogreg_cv = RandomizedSearchCV(logreg, params, cv=kf)\nlogreg_cv.fit(X_train, y_train)\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\nprint(\"Tuned Logistic Regression Best Accuracy Score: {}\".format(logreg_cv.best_score_))\n# Apply the model to the test set\npredictions = logreg.predict(X_test)\n\nsubmission = pd.DataFrame({\n    'PassengerId': test['PassengerId'],\n    'Survived': predictions\n})\n\nsubmission.head()\n\n\n\n\n\n\n\n\nPassengerId\n\n\nSurvived\n\n\n\n\n\n\n0\n\n\n892\n\n\n0\n\n\n\n\n1\n\n\n893\n\n\n1\n\n\n\n\n2\n\n\n894\n\n\n0\n\n\n\n\n3\n\n\n895\n\n\n0\n\n\n\n\n4\n\n\n896\n\n\n1\n\n\n\n\n\n# Write the file to .csv and submit\ncon.sql(\"SELECT * FROM submission\").write_csv(\"files/titanic_submission.csv\")",
    "crumbs": [
      "Data Science and Machine Learning",
      "Titanic Disaster"
    ]
  },
  {
    "objectID": "pages/projects/data_science/data_science.html",
    "href": "pages/projects/data_science/data_science.html",
    "title": "Data Science and Machine Learning Projects",
    "section": "",
    "text": "Landing page for all my project posts related to Data Science and Machine Learning.",
    "crumbs": [
      "Data Science and Machine Learning"
    ]
  },
  {
    "objectID": "pages/projects/data_science/data_science.html#projects",
    "href": "pages/projects/data_science/data_science.html#projects",
    "title": "Data Science and Machine Learning Projects",
    "section": "Projects",
    "text": "Projects",
    "crumbs": [
      "Data Science and Machine Learning"
    ]
  },
  {
    "objectID": "pages/projects/projects.html",
    "href": "pages/projects/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Landing page for the project portfolio portion of this website. Contains all of my public repositories and projects (for now, may include future consulting or paid side work, but I don‚Äôt do that at the moment), including both the code in repositories and write ups (where applicable).\nCurrently, there are two categories of projects I‚Äôm working on:\n\nData Engineering and Architecture\nData Science and Machine Learning",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html",
    "href": "pages/guides/posts/raspberry_pi_server.html",
    "title": "Setting Up A Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "",
    "text": "Building your own Raspberry Pi server is an excellent way to gain practical experience with Linux systems, networking, and server administration. This hands-on project will teach you valuable skills applicable to larger-scale environments while giving you a reliable platform for personal projects. Whether you‚Äôre a student, hobbyist, or IT professional looking to expand your knowledge, this guide will walk you through creating a robust, secure server environment from scratch.\nThis guide provides step-by-step instructions and explanations for configuring a Raspberry Pi 4 to learn about hardware, servers, containerization, and self-hosting principles. To be clear, this guide is not exhaustive, and I‚Äôm sure there were areas where I made mistakes or misunderstood topics. I‚Äôm encouraging you to let me know if you find any issues! You can submit feedback via GitHub on the guide on my website.\nThe primary purpose of this guide is to help me reference what I previously did and understand my thought process when I need to troubleshoot or recreate something. The secondary purpose is to provide a helpful resource for others in similar situations, as I struggled to find the comprehensive document I needed when starting this journey.\nEventually, I‚Äôd like to set up an actual server cluster and self-host some interesting, more resource-intensive applications. Before making that kind of commitment, I wanted to learn the basics and see if this was something I would enjoy‚Äîthe good news is that I learned I do. The great news is that Raspberry Pi makes their hardware very affordable and easy to purchase. Here‚Äôs the official webpage for the exact computer I bought.\nI purchased the 8GB Raspberry Pi 4 model. The price difference isn‚Äôt that significant compared to the lesser 2GB and 4GB models, but the performance improvement is substantial. Additionally, because I‚Äôm planning to host and experiment with CI/CD, I also bought a case and cooling fan to help with longevity. All in, the base price (including the power supply and HDMI cable) is $107.30 before taxes, shipping, and other fees.\nBelow you‚Äôll find an outline that provides a general idea of what we‚Äôll be covering and in what order. At the start of each section, I‚Äôll include a key terms list that covers the fundamental concepts important for that topic.\n\n\n1. Introduction\n\nPurpose and scope of the guide\nWhat you‚Äôll learn and build\nPrerequisites and assumptions\n\n2. Initial Setup\n\nHardware Requirements\n\nRaspberry Pi 4 specifications\nStorage devices (Thumbdrive, SSD, microSD cards)\nAccessories and peripherals (Keyboard, monitor, etc.)\n\nImage Requirements\n\nSelecting and downloading Ubuntu Server LTS\nUsing Raspberry Pi Imager\nInitial configuration options\n\nGetting Started\n\nThe physical setup of the Raspberry Pi\nWhat to expect during first boot\n\n\n3. Linux Server Basics\n\nFirst Boot Process\n\nConnection and startup\nUnderstanding initialization\n\nService Management with systemd\n\nUnderstanding systemd units and targets\nBasic service commands\n\nUnderstanding Your Home Directory\n\nShell configuration files\nHidden application directories\n\nThe Root Filesystem\n\nFilesystem Hierarchy Standard (FHS)\nKey directories and their purposes\n\nUser and Group Permissions\n\nBasic permission concepts\nchmod and chown usage\nUnderstanding advanced permissions\n\n\n4. Networking Basics\n\nComputer Networking Concepts\n\nOSI and TCP/IP models\nKey networking protocols\n\nNetwork Connections\n\nWired vs wireless configurations\nUnderstanding IP addressing\n\nUbuntu Server Networking Tools\n\nTesting connectivity\nViewing network statistics\n\nsystemd-networkd\n\nConfiguration file structure\nWired and wireless setup\n\nConverting Netplan to networkd\n\nWhy and how to transition\nTroubleshooting network issues\n\nAdvanced Networking\n\nSubnets and routing\nSecurity considerations\n\n\n5. SSH (Secure Shell)\n\nSSH Basics\n\nClient vs server setup\nKey-based authentication\n\nKey-Based Authentication\n\nTypes of SSH key encryption\nGenerating keys\nInstalling the public key\n\nServer-Side SSH Configuration\n\nHost keys and security options\nOptimizing for security\n\nClient-Side Configuration\n\nSetting up SSH config\nManaging known hosts\n\nAdditional Security Measures\n\nFirewall configuration with UFW\nIntrusion prevention with Fail2Ban\n\nSecure File Transfers\n\nUsing SCP (Secure Copy Protocol)\nUsing rsync for efficient transfers\n\n\n6. Remote Development with VS Code\n\nSetting Up VS Code Remote SSH\nManaging Remote Projects\nDebugging and Terminal Integration\n\n7. Partitions\n\nPartitioning Basics\n\nUnderstanding partition tables and types\nFilesystem options and considerations\n\nPartitioning Tools\n\nUsing parted and other utilities\n\nPartitioning for Backups\n\nSetting up microSD cards\nMount points and fstab configuration\n\nPartitioning your SSD\n\nBoot and root partitions\nFormatting and preparation\n\nAdvanced Partitioning\n\nMonitoring usage\nResizing partitions\n\n\n8. Backups and Basic Automation\n\nBackup Basics\n\nDirectory structure and permissions\n\nConfiguration Files Backup\n\nUsing rsync for system configurations\nRemote transfers of backups\n\nRestoring from Backup\n\nCreating restoration scripts\nTesting recovery procedures\n\nAutomating Backups with Crontab\n\nCreating a schedule for scripts\nVerifying the schedule\n\n\n9. Changing Your Boot Media Device\n\nBoot Configuration Transition\n\nFlashing OS to new media\nProper shutdown procedures\nPhysically changing boot devices\nTesting the new boot configuration\nRestoring configurations\n\n\n10. Monitoring and Maintenance\n\nMonitoring Basics\n\nsmartmon, vcgencmd\nResolving SSD health issues\n\nSecurity Updates and Patching\n\nPreventive Measures\nSystem patching schedules\n\nLog Management\n\nLog basics\nManagement tools and strategy",
    "crumbs": [
      "Guides",
      "Linux Server & Raspberry Pi"
    ]
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-introduction",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-introduction",
    "title": "Setting Up A Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "",
    "text": "Building your own Raspberry Pi server is an excellent way to gain practical experience with Linux systems, networking, and server administration. This hands-on project will teach you valuable skills applicable to larger-scale environments while giving you a reliable platform for personal projects. Whether you‚Äôre a student, hobbyist, or IT professional looking to expand your knowledge, this guide will walk you through creating a robust, secure server environment from scratch.\nThis guide provides step-by-step instructions and explanations for configuring a Raspberry Pi 4 to learn about hardware, servers, containerization, and self-hosting principles. To be clear, this guide is not exhaustive, and I‚Äôm sure there were areas where I made mistakes or misunderstood topics. I‚Äôm encouraging you to let me know if you find any issues! You can submit feedback via GitHub on the guide on my website.\nThe primary purpose of this guide is to help me reference what I previously did and understand my thought process when I need to troubleshoot or recreate something. The secondary purpose is to provide a helpful resource for others in similar situations, as I struggled to find the comprehensive document I needed when starting this journey.\nEventually, I‚Äôd like to set up an actual server cluster and self-host some interesting, more resource-intensive applications. Before making that kind of commitment, I wanted to learn the basics and see if this was something I would enjoy‚Äîthe good news is that I learned I do. The great news is that Raspberry Pi makes their hardware very affordable and easy to purchase. Here‚Äôs the official webpage for the exact computer I bought.\nI purchased the 8GB Raspberry Pi 4 model. The price difference isn‚Äôt that significant compared to the lesser 2GB and 4GB models, but the performance improvement is substantial. Additionally, because I‚Äôm planning to host and experiment with CI/CD, I also bought a case and cooling fan to help with longevity. All in, the base price (including the power supply and HDMI cable) is $107.30 before taxes, shipping, and other fees.\nBelow you‚Äôll find an outline that provides a general idea of what we‚Äôll be covering and in what order. At the start of each section, I‚Äôll include a key terms list that covers the fundamental concepts important for that topic.\n\n\n1. Introduction\n\nPurpose and scope of the guide\nWhat you‚Äôll learn and build\nPrerequisites and assumptions\n\n2. Initial Setup\n\nHardware Requirements\n\nRaspberry Pi 4 specifications\nStorage devices (Thumbdrive, SSD, microSD cards)\nAccessories and peripherals (Keyboard, monitor, etc.)\n\nImage Requirements\n\nSelecting and downloading Ubuntu Server LTS\nUsing Raspberry Pi Imager\nInitial configuration options\n\nGetting Started\n\nThe physical setup of the Raspberry Pi\nWhat to expect during first boot\n\n\n3. Linux Server Basics\n\nFirst Boot Process\n\nConnection and startup\nUnderstanding initialization\n\nService Management with systemd\n\nUnderstanding systemd units and targets\nBasic service commands\n\nUnderstanding Your Home Directory\n\nShell configuration files\nHidden application directories\n\nThe Root Filesystem\n\nFilesystem Hierarchy Standard (FHS)\nKey directories and their purposes\n\nUser and Group Permissions\n\nBasic permission concepts\nchmod and chown usage\nUnderstanding advanced permissions\n\n\n4. Networking Basics\n\nComputer Networking Concepts\n\nOSI and TCP/IP models\nKey networking protocols\n\nNetwork Connections\n\nWired vs wireless configurations\nUnderstanding IP addressing\n\nUbuntu Server Networking Tools\n\nTesting connectivity\nViewing network statistics\n\nsystemd-networkd\n\nConfiguration file structure\nWired and wireless setup\n\nConverting Netplan to networkd\n\nWhy and how to transition\nTroubleshooting network issues\n\nAdvanced Networking\n\nSubnets and routing\nSecurity considerations\n\n\n5. SSH (Secure Shell)\n\nSSH Basics\n\nClient vs server setup\nKey-based authentication\n\nKey-Based Authentication\n\nTypes of SSH key encryption\nGenerating keys\nInstalling the public key\n\nServer-Side SSH Configuration\n\nHost keys and security options\nOptimizing for security\n\nClient-Side Configuration\n\nSetting up SSH config\nManaging known hosts\n\nAdditional Security Measures\n\nFirewall configuration with UFW\nIntrusion prevention with Fail2Ban\n\nSecure File Transfers\n\nUsing SCP (Secure Copy Protocol)\nUsing rsync for efficient transfers\n\n\n6. Remote Development with VS Code\n\nSetting Up VS Code Remote SSH\nManaging Remote Projects\nDebugging and Terminal Integration\n\n7. Partitions\n\nPartitioning Basics\n\nUnderstanding partition tables and types\nFilesystem options and considerations\n\nPartitioning Tools\n\nUsing parted and other utilities\n\nPartitioning for Backups\n\nSetting up microSD cards\nMount points and fstab configuration\n\nPartitioning your SSD\n\nBoot and root partitions\nFormatting and preparation\n\nAdvanced Partitioning\n\nMonitoring usage\nResizing partitions\n\n\n8. Backups and Basic Automation\n\nBackup Basics\n\nDirectory structure and permissions\n\nConfiguration Files Backup\n\nUsing rsync for system configurations\nRemote transfers of backups\n\nRestoring from Backup\n\nCreating restoration scripts\nTesting recovery procedures\n\nAutomating Backups with Crontab\n\nCreating a schedule for scripts\nVerifying the schedule\n\n\n9. Changing Your Boot Media Device\n\nBoot Configuration Transition\n\nFlashing OS to new media\nProper shutdown procedures\nPhysically changing boot devices\nTesting the new boot configuration\nRestoring configurations\n\n\n10. Monitoring and Maintenance\n\nMonitoring Basics\n\nsmartmon, vcgencmd\nResolving SSD health issues\n\nSecurity Updates and Patching\n\nPreventive Measures\nSystem patching schedules\n\nLog Management\n\nLog basics\nManagement tools and strategy",
    "crumbs": [
      "Guides",
      "Linux Server & Raspberry Pi"
    ]
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-setup",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-setup",
    "title": "Setting Up A Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Initial Setup",
    "text": "Initial Setup\n\nKey Terms\nHardware Terminology:\n\nRaspberry Pi 4: A single-board computer developed by the Raspberry Pi Foundation, featuring a Broadcom BCM2711 processor, various RAM options, USB 3.0 ports, and GPIO pins.\nSoC (System on Chip): An integrated circuit that combines the components of a computer or electronic system into a single chip.\nGPIO (General Purpose Input/Output): Programmable pins on the Raspberry Pi that allow interaction with physical components and sensors.\nSSD (Solid State Drive): Storage device using flash memory that offers faster access times and better reliability than traditional hard drives.\nBoot Media: The storage device containing the operating system files from which the computer starts.\nUSB 3.0: A USB standard offering data transfer speeds up to 5 Gbps, significantly faster than previous versions.\nmicroSD Card: A small form factor removable flash memory card used as storage media.\neMMC (embedded MultiMediaCard): An integrated flash storage solution often found in compact devices.\n\nSoftware and Imaging Terminology:\n\nUbuntu Server LTS: A long-term support version of Ubuntu‚Äôs server operating system, maintaining security updates for 5 years.\nRaspberry Pi Imager: Official software tool for flashing operating system images to SD cards and other storage devices.\nImage: A file containing the complete contents and structure of a storage device or filesystem.\nFlashing: The process of writing an operating system image to a storage device.\nHeadless Setup: Configuring a device to operate without a monitor, keyboard, or mouse.\nPublic-key Authentication: An authentication method using cryptographic key pairs instead of passwords.\n\nFirst Boot Terminology:\n\nCloud-Init: A utility used by Ubuntu Server to handle early initialization when a server instance boots for the first time.\nFirst Boot Experience: The initial setup process that occurs the first time an operating system is booted.\nInitial RAM Disk (initrd): A temporary root filesystem loaded into memory during the boot process.\nBootloader: Software that loads the operating system kernel into memory.\n\n\n\nHardware Requirements\nThis section provides basic setup instructions, so you‚Äôll have the same tools I do and can follow along with this guide step-by-step.\n\nRaspberry Pi 4 8GB\n\nMicro HDMI to HDMI cord (for direct access)\nProtective case\nCooling fan\nAppropriate Power Supply (use an officially suggested one)\n\nKeyboard (connected via USB for direct access)\nMonitor (for direct access)\n1TB Samsung T7 SSD (connected via USB for boot media/core memory)\n64GB Generic Flash Drive (used as the boot media when partitioning the SSD)\nAmazon Basics 128GB microSD card (or other microSD cards for backups media)\nSSH-capable devices for headless access\n\nI‚Äôm using a MacBook Air\nI prefer macOS and Terminal for personal development, because I use Windows at my day job\n\n\n\n\nImage Requirements\nOnce you have your hardware ready, you can begin setting up the software. I‚Äôm using Ubuntu Server LTS because it‚Äôs a stable version of Linux intended for headless, server environments. LTS means long-term support, so unlike the more frequently updated versions, these OS versions are supported for 5 years. Additionally, you‚Äôll want to use public-key authentication for better security purposes, but more on that in the SSH Section.\n\nHave your Thumb Drive ready and able to connect to either a laptop or desktop (whichever you plan to use with SSH)\nDownload Raspberry Pi Imager from the official website\nRun the Imager and configure your installation of the most recent Ubuntu Server LTS image\n\nSelect your Raspberry Pi device\n\n\n\nSelect the OS Image you want to flash\n\n\n\nSelect the media storage device for the image\n\n\n\nConfigure settings\n\n Here, you‚Äôll configure your primary user ID and password; network connection; locale and timezone; and your hostname (the nickname your computer remembers the IP address as).\n Here, you‚Äôll configure your SSH settings. You should probably use public-key authentication only when dealing with SSH in production, but for learning purposes, you don‚Äôt need to at this time. Later in this guide, I‚Äôll walk you through the steps to manually configure SSH, if you are unfamiliar.\n These are more preference based, but it‚Äôs nice to have the storage device automatically eject once the flashing is complete. Then, you just need to unplug it and plug it into your Raspberry Pi to get going.\n\n\n\nGetting Started\nIt‚Äôs time to set up the actual Raspberry Pi device. For most of this guide, I recommend leaving the Pi outside of the case, because it‚Äôll be easier to plug and unplug some of the devices‚Äîthe microSD slot is not accessible while the case is on. Later, once we‚Äôve got everything configured and set up as we like, we will attach the fan and case, so it‚Äôs a bit safer and able to run in an always-on state. I‚Äôll share a picture of what my server looks like during the early development, and then later I‚Äôll show what it looks like with everything connected and set up.\n\nNow you‚Äôre ready to plug your boot media device (the Thumb Drive) into your Raspberry Pi. You should also connect a keyboard, monitor, and power supply. Once all of this is connected, your Raspberry Pi will boot up. Connecting a monitor and keyboard will allow you to directly interact with the system‚Äôs terminal. Ideally, you‚Äôll use SSH, but it may be helpful to have direct access in case there are any network issues. Eventually, the SSD will serve as the boot media and primary storage device for the server; however, we can‚Äôt modify its partitions while it‚Äôs serving as the boot device. So, we‚Äôll use a thumb drive as the boot media device until we complete the partitioning.\nWhen first connecting from the wired keyboard and monitor, let all of the startup processes finish running (these will hopefully have brackets with the word Success in green). If the first boot fails, try to redo the boot again (plug in the power and boot media). If that doesn‚Äôt work, try re-flashing the image and restarting the boot process. Then, type in the name of the User ID you wish to login with, in my case it‚Äôs chris. Then, enter the password (no characters will show up as you type it in) and hit enter. You‚Äôll see a plaintext message telling you the OS version, some system information (memory usage, temperature, etc.), and you‚Äôll see a line where you can enter commands (the CLI). In my case, it looks like this: chris@ubuntu-pi-server:~$\nNow you can run some basic commands to see where you are and what you have available to you. Spoiler alert, you‚Äôre in your home directory and have no files. In my case it‚Äôs /home/chris, where the /home directory is owned by root and /chris is owned by my user‚ÄîUID 1000 (the default for new users on a fresh system/image). Right now your directory will be empty, outside of some hidden folders like .ssh. More on this later.\nOne final note, on why I chose Ubuntu Server LTS over Raspberry Pi‚Äôs OS. The reason is pretty simple: Ubuntu is a true open source OS that is widely used personally, professionally, and academically. I wasn‚Äôt sure if Raspberry Pi‚Äôs OS options would provide a similar level of education. Furthermore, as a complete newbie to server computing, I wanted to use an operating system that I knew had a long history and devoted community, because it would have ample resources for self-led research and troubleshooting.\nNext we‚Äôll cover what happened during the boot process, the basic structure of the Linux Server OS, and some important information related to permissions, before we move on to basic networking concepts and configurations.",
    "crumbs": [
      "Guides",
      "Linux Server & Raspberry Pi"
    ]
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-linux_basics",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-linux_basics",
    "title": "Setting Up A Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Linux Server Basics",
    "text": "Linux Server Basics\n\nKey Terms\nSystem Initialization Concepts:\n\nInitialization: The process of starting up the operating system and bringing it to an operational state.\nBoot Process: The sequence of steps that occur from powering on a computer to loading the operating system.\nBIOS/UEFI: Firmware interfaces that initialize hardware and start the boot process.\nKernel: The core component of an operating system that manages system resources and hardware.\n\nSystemd Terminology:\n\nsystemd: The init system and system/service manager used by modern Linux distributions.\nUnit: Systemd‚Äôs representation of system resources, including services, devices, and mount points.\nService Unit: Configuration files with .service extension that define how to start, stop, and manage daemons.\nSocket Unit: Configuration files with .socket extension that define communication sockets.\nTimer Unit: Configuration files with .timer extension that trigger actions at specified times.\nTarget: A grouping of units that represents a system state (similar to runlevels in older systems).\nDaemon: A background process that runs continuously, providing services.\n\nFile System and Directory Terminology:\n\nFHS (Filesystem Hierarchy Standard): The standard directory structure and contents of Unix-like operating systems.\nRoot Directory (/): The top-level directory in a Linux filesystem hierarchy.\nhome Directory (/home): Contains user home directories where personal files are stored.\netc Directory (/etc): Contains system-wide configuration files.\nbin Directory (/bin): Contains essential command binaries needed for system functionality.\nHidden Files/Directories: Files or directories that begin with a dot (.) and don‚Äôt appear in default directory listings.\nShell Configuration Files: Files like .bashrc and .profile that configure the user‚Äôs command-line environment.\n\nUser and Permissions Terminology:\n\nUser: An account on a Linux system with specific privileges and access rights.\nGroup: A collection of users with shared permissions to files and directories.\nUID (User ID): A numerical identifier assigned to each user on a Linux system.\nGID (Group ID): A numerical identifier assigned to each group on a Linux system.\nPermission: Access rights assigned to users and groups determining what actions they can perform on files and directories.\nchmod: Command used to change file and directory permissions.\nchown: Command used to change file and directory ownership.\numask: Default permissions applied to newly created files and directories.\nACL (Access Control List): Extended permissions that provide more granular control than traditional Unix permissions.\nsetuid/setgid: Special permissions that allow users to execute files with the permissions of the file owner or group.\nSticky Bit: A special permission bit that restricts file deletion in shared directories.\n\n\n\nFirst Boot Process\nWhen you first boot a fresh Ubuntu Server LTS image on your Raspberry Pi, several important initialization processes occur that don‚Äôt happen during subsequent boots. The first boot of your Ubuntu Server LTS on the Raspberry Pi is fundamentally different from subsequent boots because it performs one-time initialization tasks. While later boots will simply load the configured system, this first boot sets up critical system components.\n\nHardware Detection: The system performs comprehensive hardware detection to identify and configure your Raspberry Pi‚Äôs components.\nInitial RAM Disk (initrd): The bootloader loads a temporary filesystem into memory that contains essential drivers and modules needed to mount the real root filesystem.\nFilesystem Check and Expansion: On first boot, the system checks the integrity of the filesystem and often expands it to utilize the full available space on your Flash Drive.\nCloud-Init Processing: Ubuntu Server uses cloud-init to perform first-boot configuration tasks (the processes you see running on the monitor on startup)\n\nSetting the hostname\nGenerating SSH host keys\nCreating the default user account\nRunning package updates\n\nMachine ID Generation: A unique machine ID is generated and stored in /etc/machine-id.\nNetwork Configuration: The system attempts initial network setup based on detected hardware.\n\nThe key difference is that subsequent boots skip these initialization steps since they‚Äôve already been completed, making them significantly faster.\n\n\nService Management with systemd\nSystemd is the modern initialization and service management system for Linux. It‚Äôs responsible for bootstrapping the user space and managing all processes afterward. Key components of systemd include:\n\nUnits: Everything systemd manages is represented as a ‚Äúunit‚Äù with a corresponding configuration file. Units include:\n\nService units (.service): Define how to start, stop, and manage daemons (background processes that are always on)\nSocket units (.socket): Manage network/IPC sockets\nTimer units (.timer): Trigger other units based on timers\nMount units (.mount): Control filesystem mount points\n\nTarget units: Represent system states (similar to runlevels in older systems)\n\nmulti-user.target: Traditional text-mode system\ngraphical.target: Graphical user interface\nnetwork.target: Network services are ready\n\n\nFor example, let‚Äôs take a look at a generic SSH service file:\n[Unit]\nDescription=OpenSSH server daemon\nDocumentation=man:sshd(8) man:sshd_config(5)\nAfter=network.target auditd.service\nWants=network.target\n\n[Service]\nEnvironmentFile=-/etc/default/ssh\nExecStartPre=/usr/sbin/sshd -t\nExecStart=/usr/sbin/sshd -D $SSHD_OPTS\nExecReload=/usr/sbin/sshd -t\nExecReload=/bin/kill -HUP $MAINPID\nKillMode=process\nRestart=on-failure\nRestartPreventExitStatus=255\nType=notify\n\n[Install]\nWantedBy=multi-user.target\nTo break this down:\n\n[Unit]: Metadata and dependencies\n\nDescription: Human-readable service description\nDocumentation: Where to find documentation\nAfter: Units that should be started before this one\nWants: Soft dependencies\n\n[Service]: Runtime behavior\n\nExecStart: Command to start the service\nExecReload: Command to reload the service\nRestart: When to restart the service\nType: How systemd should consider the service started\n\n[Install]: Installation information\n\nWantedBy: Target that should include this service\n\n\nUbuntu Server‚Äôs current standard is systemd, but previously it was SysV. A few key improvements include:\n\nParallel Service Startup: Systemd can start services in parallel, improving boot times.\nDependency Management: Systemd handles service dependencies more effectively.\nService Supervision: Systemd continuously monitors and can automatically restart services.\nSocket Activation: Services can be started on-demand when a connection request arrives.\n\nManaging services is easy using the command line, a crucial component of headless applications. A few examples are:\n\nView service status: systemctl status ssh\nStart a service: sudo systemctl start ssh\nStop a service: sudo systemctl stop ssh\nEnable at boot: sudo systemctl enable ssh\nDisable at boot: sudo systemctl disable ssh\nView logs: journalctl -u ssh\n\n\n\nUnderstanding Your Home Directory\nNow that you‚Äôve logged in and can work on your server, you may wonder where you are and what‚Äôs there. Running pwd will return the file path of your current location. Running ls -a will show you all available files and directories in your current location. Running these, you‚Äôll see a few things specifically for Shell configuration (your terminal/CLI):\n\n.bash_history: Contains a record of commands you‚Äôve executed in the bash shell. This helps with command recall using the up arrow or history command.\n.bash_logout: Executed when you log out of a bash shell. Often used for cleanup tasks like clearing the screen.\n.bashrc: The primary bash configuration file that‚Äôs loaded for interactive non-login shells. It defines aliases, functions, and shell behavior. When you open a terminal window, this file is read.\n.profile: Executed for login shells. It typically sets environment variables and executes commands that should run once per login session, not for each new terminal.\n\n# Sample .bashrc section\n# enable color support of ls and also add handy aliases\nif [ -x /usr/bin/dircolors ]; then\n    test -r ~/.dircolors && eval \"$(dircolors -b ~/.dircolors)\" || eval \"$(dircolors -b)\"\n    alias ls='ls --color=auto'\n    alias grep='grep --color=auto'\n    alias fgrep='fgrep --color=auto'\n    alias egrep='egrep --color=auto'\nfi\n\n# some more ls aliases\nalias ll='ls -alF'\nalias la='ls -A'\nalias l='ls -CF'\nBeyond those, you‚Äôll also find hidden application directories:\n\n.cache: Contains non-essential data that can be regenerated as needed. Applications store temporary files here to improve performance on subsequent runs.\n.dotnet: Contains .NET Core SDK and runtime files if you‚Äôve installed the .NET development platform.\n.ssh: Stores SSH configuration files and keys:\n\nauthorized_keys: Lists public keys that can authenticate to your account\nubuntu_pi_ecdsa & ubuntu_pi_ecdsa.pub: Your private and public ECDSA keys (More on this in the SSH Section)\nknown_hosts: Tracks hosts you‚Äôve connected to previously\nssh_config: Optional configuration file for SSH connections\n\n.sudo_as_admin_successful: A marker file created when you successfully use sudo. Its presence suppresses the ‚Äúsudo capabilities‚Äù message when opening a terminal.\n.vscode-server: Created when you connect to your server using Visual Studio Code‚Äôs remote development feature. Contains the VS Code server components. (More on this in the Remote Development with VS Code Section)\n.wget-hsts: Wget‚Äôs HTTP Strict Transport Security database. Tracks websites that require secure (HTTPS) connections.\n\n\n\nThe Root Filesystem\nThe Linux filesystem follows the Filesystem Hierarchy Standard (FHS), which defines the directory structure and contents of Unix-like systems. Here‚Äôs a breakdown of key directories:\n\n/bin: Contains essential command binaries (programs) needed for basic system functionality. These commands are available to all users and are required during boot or in single-user mode.\n\nHistorical note: Originally separated from /usr/bin because early Unix systems had limited disk space on the root partition.\n\n/boot: Contains boot loader files including the Linux kernel, initial RAM disk (initrd), and bootloader configuration (GRUB).\n\nFor Raspberry Pi, this contains the firmware and various boot-related files.\n\n/dev: Contains device files that represent hardware devices. These are not actual files but interfaces to device drivers in the kernel.\n\nExample: /dev/sda represents the first SATA disk.\n\n/etc: Contains system-wide configuration files. The name originated from ‚Äúet cetera‚Äù but is now often interpreted as ‚ÄúEditable Text Configuration.‚Äù Critical files include:\n\n/etc/fstab: Filesystem mount configuration\n/etc/passwd: User account information\n/etc/ssh/sshd_config: SSH server configuration\n\n/home: Contains user home directories where personal files and user-specific configuration files are stored.\n/lib: Contains essential shared libraries needed by programs in /bin and system boot.\n\nOn modern 64-bit systems, you‚Äôll also find /lib64 for 64-bit libraries.\n\n/media: Mount point for removable media like USB drives and DVDs.\n/mnt: Temporarily mounted filesystems. This is often used as a manual mount point.\n/opt: Optional application software packages. Used for third-party applications that don‚Äôt follow the standard file system layout.\n/proc: A virtual filesystem providing process and kernel information. Files here don‚Äôt exist on disk but represent system state.\n\nExample: /proc/cpuinfo shows CPU information.\n\n/root: Home directory for the root user. Separated from /home to ensure it‚Äôs available even if /home is on a separate partition.\n/run: Runtime data for processes started since last boot. This is a tmpfs (memory-based) filesystem.\n/sbin: System binaries for system administration tasks, typically only usable by the root user.\n/srv: Data for services provided by the system, such as web or FTP servers.\n/snap: The /snap directory is, by default, where the files and folders from installed snap packages appear on your system.\n/sys: Another virtual filesystem exposing device and driver information from the kernel. Provides a more structured view than /proc.\n/tmp: Temporary files that may be cleared on reboot. Applications should not rely on data here persisting.\n/usr: Contains the majority of user utilities and applications. Originally stood for ‚ÄúUnix System Resources.‚Äù\n\n/usr/bin: User commands\n/usr/lib: Libraries for the commands in /usr/bin\n/usr/local: Locally installed software\n/usr/share: Architecture-independent data\n\n/var: Variable data files that change during normal operation:\n\n/var/log: System log files\n/var/mail: Mail spool\n/var/cache: Application cache data\n/var/spool: Spool for tasks waiting to be processed (print queues, outgoing mail)\n\n\nThe core philosophy behind this structure separates:\n\nStatic vs.¬†variable content\nShareable vs.¬†non-shareable files\nEssential vs.¬†non-essential components\n\nUnderstanding this hierarchy helps you navigate any Linux system and locate important files intuitively.\n\n\nUser and Group Permissions\n\nBasics\nLinux inherits its permission system from Unix, providing a robust framework for controlling access to files and directories. Understanding this system is essential for maintaining security and proper functionality of your Raspberry Pi server, as well as any Linux-based system. At its core, the Linux permission model operates with three basic permission types applied to three different categories of users:\n\nPermission Types:\n\nRead (r): Allows viewing file contents or listing directory contents\nWrite (w): Allows modifying file contents or creating/deleting files within a directory\nExecute (x): Allows running a file as a program or accessing files within a directory\n\nUser Categories:\n\nOwner (u): The user who owns the file or directory\nGroup (g): Users who belong to the file‚Äôs assigned group\nOthers (o): All other users on the system\n\n\nIt‚Äôs not only important to know how to set permissions, but also how to view existing ones. When you run ls -l in a directory, you‚Äôll see a detailed listing including permission information:\n-rw-r--r-- 1 chris chris 1234 May 6 14:32 example.txt\nIn this example, the owner can read and write, while group members and others can only read. The first string of characters -rw-r--r-- represents the permissions:\n\nFirst character: File type (- for regular file, d for directory, l for symbolic link)\nCharacters 2-4: Owner permissions (rw-)\nCharacters 5-7: Group permissions (r‚Äì)\nCharacters 8-10: Others permissions (r‚Äì)\n\n\n\nchmod\nThe chmod command modifies file permissions in Linux. You can use it in two ways: symbolic mode or numeric (octal) mode.\nSymbolic mode uses letters to represent permission categories (u, g, o, a) and permissions (r, w, x):\n# Give the owner execute permission\nchmod u+x script.sh\n\n# Remove write permission from group and others\nchmod go-w important_file.txt\n\n# Set read and execute for everyone (a=all users)\nchmod a=rx application\n\n# Add write permission for owner and group\nchmod ug+w shared_document.txt\nEach symbol has a specific meaning:\n\nu: Owner permissions\ng: Group permissions\no: Other user permissions\na: All permissions\n+: Add permissions\n-: Remove permissions\n=: Set exact permissions\n\nOctal mode represents permissions as a 3-digit number, where each digit represents the permissions for owner, group, and others:\n\nRead (r) = 4\nWrite (w) = 2\nExecute (x) = 1\n\nPermissions are calculated by adding these values:\n\n7 (4+2+1) = Read, write, and execute\n6 (4+2) = Read and write\n5 (4+1) = Read and execute\n4 (4) = Read only\n0 = No permissions\n\n# rwxr-xr-x (755): Owner can read, write, execute; group and others can read and execute\nchmod 755 script.sh\n\n# rw-r--r-- (644): Owner can read and write; group and others can read only\nchmod 644 document.txt\n\n# rwx------ (700): Owner has all permissions; group and others have none\nchmod 700 private_directory\nBeyond the basic rwx permissions, Linux has three special permission bits:\n\nsetuid (4000): When set on an executable file, it runs with the privileges of the file owner instead of the user executing it.\nsetgid (2000): Similar to setuid but for group permissions. When set on a directory, new files created within inherit the directory‚Äôs group.\nsticky bit (1000): When set on a directory, files can only be deleted by their owner, the directory owner, or root (commonly used for /tmp).\n\n\n\nchown\nThe chown command changes the owner and/or group of files and directories. Do not change ownership in the root directories because many require specific ownership/permissions to function properly.\n# Change the owner of a file\nsudo chown chris file.txt\n\n# Change both owner and group\nsudo chown chris:developers project_files\n\n# Change only the group\nsudo chown :developers shared_documents\n\n# Change recursively for a directory and all its contents\nsudo chown -R chris:chris /home/chris/projects\nThe flags do the following:\n\n-R, --recursive: Change ownership recursively\n-c, --changes: Report only when a change is made\n-f, --silent: Suppress most error messages\n-v, --verbose: Output a diagnostic for every file processed\n\n# Verbose recursive ownership change\nsudo chown -Rv chris:developers /opt/application\n\n\nUnderstanding Permissions\nLinux manages permissions through users and groups:\n\nEach user has a unique User ID (UID)\nEach group has a unique Group ID (GID)\nUsers can belong to multiple groups\nThe first 1000 UIDs/GIDs are typically reserved for system users/groups\n\nImportant files include:\n\n/etc/passwd: Contains basic user account information\n\nFields: username, password placeholder, UID, primary GID, full name, home directory, login shell\n\n\nchris:x:1000:1000:Chris Kornaros:/home/chris:/bin/bash\n\n/etc/shadow: Contains encrypted passwords and password policy information\n\nFields: username, encrypted password, days since epoch of last change, min days between changes, max days password valid, warning days, inactive days, expiration date\n\n\nchris:$6$xyz...hash:19000:0:99999:7:::\n\n/etc/group: Contains group definitions\n\nFields: group name, password placeholder, GID, comma-separated list of members\n\n\ndevelopers:x:1001:chris,bob,alice\nThere are two categories of groups you should understand, Primary and Supplementary:\n\nPrimary Group: Set in /etc/passwd, used as the default group for new files\nSupplementary Groups: Additional groups a user belongs to, defined in /etc/group\n\nYou can view your current user‚Äôs groups with the groups command, or view them for a specific user with groups chris (replace chris with the name of the user). That being said, directory permissions differ slightly from file permissions:\n\nRead (r): List directory contents\nWrite (w): Create, delete, or rename files within the directory\nExecute (x): Access files within the directory (crucial for navigation)\n\n\n\n\n\n\n\nTip\n\n\n\nA common confusion: You may have read permission for a file but not execute permission for its parent directory, preventing access.\n\n\nThe umask (user mask) determines the default permissions for newly created files and directories:\n\nDefault for files: 666 (rw-rw-rw-)\nDefault for directories: 777 (rwxrwxrwx)\nThe umask is subtracted from these defaults, for example, a umask of 022 results in:\n\nFiles: 644 (rw-r‚Äìr‚Äì)\nDirectories: 755 (rwxr-xr-x)\n\n\n# View current umask (in octal)\numask\n\n# Set a new umask\numask 027  # More restrictive: owner full access, group read/execute, others no access\nTraditional Unix permissions have limitations regarding inheritance: new files don‚Äôt inherit permissions from parent directories and changing permissions doesn‚Äôt affect existing files. Modern solutions, however, include: the setgid bit on directories for group inheritance and ACLs (Access Control Lists) with default entries that apply to new files. To set up a collaborative directory with proper permissions:\n# Create a shared directory for developers\nsudo mkdir /opt/projects\nsudo chown chris:developers /opt/projects\nsudo chmod 2775 /opt/projects  # setgid bit ensures new files get 'developers' group\n\n\nAdvanced Permission Concepts\nAs I previously wrote, part of the modern permission solutions include ACLs, or Access Control Lists. ACLs extend the traditional permission model to allow specifying permissions for multiple users and groups. When ACLs are in use, ls -l will show a + after the permission bits. Here‚Äôs a basic example of how to create and manage an ACL:\n# Install ACL support (if not already installed)\nsudo apt install acl\n\n# Set an ACL allowing a specific user read access\nsetfacl -m u:chris:r file.txt\n\n# Set an ACL allowing a specific group write access\nsetfacl -m g:developers:rw file.txt\n\n# Set default ACLs on a directory (inherited by new files)\nsetfacl -d -m g:developers:rw directory/\n\n# View ACLs on a file\ngetfacl file.txt\n-rw-rw-r--+ 1 chris developers 1234 May 6 14:32 file.txt\nA few final notes on permissions that are especially relevant for this project, because you‚Äôll be working with external storage devices:\n\nNot all filesystems support the same permission features:\n\next4: Full support for traditional permissions, ACLs, and extended attributes\nNTFS (via NTFS-3G): Simulated Unix permissions, basic ACL support\nFAT32: No native permission support (mounted with fixed permissions)\nexFAT: No native permission support\n\nCommon Permission Patterns:\n\nConfiguration Files: 644 or 640 (owner can edit, restricted read access)\nProgram Binaries: 755 (everyone can execute, only owner can modify)\nWeb Content: 644 for regular files, 755 for directories\nSSH Keys: 600 for private keys (owner only), 644 for public keys\nScripts: 700 or 750 (executable by owner or group)",
    "crumbs": [
      "Guides",
      "Linux Server & Raspberry Pi"
    ]
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-network",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-network",
    "title": "Setting Up A Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Computer Networking",
    "text": "Computer Networking\nThis section provides a brief example of how to connect your server to WiFi. It assumes you are already connected using the wireless network settings you configured in the Requirements Section. That being said, I‚Äôll also go over some basic networking concepts and background information. As a result, some of the decisions and terminology in this guide will make more sense (it also helps me remember what I‚Äôm doing and why).\n\nKey Terms\nBasic Networking Concepts:\n\nProtocol: A set of rules that determine how data is transmitted between devices on a network. Examples include TCP, UDP, and HTTP.\nMAC Address: Media Access Control address; a unique hardware identifier assigned to network interfaces. It‚Äôs a 48-bit address (e.g., 00:1A:2B:3C:4D:5E) permanently assigned to a network adapter.\nIP Address: A numerical label assigned to each device on a network that uses the Internet Protocol. Functions like a postal address for devices.\nPacket: A unit of data transmitted over a network. Includes both the data payload and header information for routing.\nSubnet: A logical subdivision of an IP network that allows for more efficient routing and security segmentation.\nGateway: A network node that serves as an access point to another network, typically connecting a local network to the wider internet.\nDNS: Domain Name System; translates human-readable domain names (like google.com) into IP addresses computers can understand.\nDHCP: Dynamic Host Configuration Protocol; automatically assigns IP addresses and other network configuration parameters to devices.\nCIDR Notation: Classless Inter-Domain Routing; a method for representing IP addresses and their subnet masks in a compact format (e.g., 192.168.1.0/24). The number after the slash indicates how many bits are used for the network portion of the address.\nOSI Model: Open Systems Interconnection model; a conceptual framework that standardizes the functions of a communication system into seven abstraction layers, from physical transmission to application interfaces.\nTCP/IP Model: Transmission Control Protocol/Internet Protocol model; a four-layer practical implementation of network communications used as the foundation of the internet, simplifying the OSI model for real-world application.\n\nNetwork Types and Components:\n\nLAN: Local Area Network; a network confined to a small geographic area, like a home or office.\nWAN: Wide Area Network; connects multiple LANs across large geographic distances.\nRouter: A device that forwards data packets between computer networks, determining the best path for data transmission.\nSwitch: A networking device that connects devices within a single network and uses MAC addresses to forward data to the correct destination.\nBandwidth: The maximum data transfer rate of a network connection, measured in bits per second (bps).\nLatency: The delay between sending and receiving data, typically measured in milliseconds.\n\nLinux Networking Terminology:\n\nInterface: A connection between a device and a network. In Linux, these have names like eth0 (Ethernet) or wlan0 (wireless).\nNetplan: Ubuntu‚Äôs default network configuration tool that uses YAML files to define network settings.\nsystemd-networkd: A system daemon that manages network configurations in modern Linux distributions.\nNetworkManager: An alternative network management daemon that provides detection and configuration for automatic network connectivity.\nSocket: An endpoint for sending or receiving data across a network, defined by an IP address and port number.\nip: A powerful, modern Linux networking utility that replaces older commands like ifconfig, route, and arp. Used to show/manipulate routing, devices, policy routing, and tunnels.\n\nSecurity Concepts:\n\nFirewall: Software or hardware that monitors and filters incoming and outgoing network traffic based on predetermined security rules.\nfail2ban: An intrusion prevention software that protects servers from brute-force attacks by monitoring log files and temporarily banning IP addresses that show malicious behavior.\nufw: Uncomplicated Firewall; a user-friendly interface for managing iptables firewall rules in Linux, designed to simplify the process of configuring a firewall.\nSSH: Secure Shell; a cryptographic network protocol for secure data communication and remote command execution.\nEncryption: The process of encoding information to prevent unauthorized access.\nPort: A virtual point where network connections start and end. Ports are identified by numbers (0-65535).\nNAT: Network Address Translation; allows multiple devices on a local network to share a single public IP address.\nVPN: Virtual Private Network; extends a private network across a public network, enabling secure data transmission.\n\n\n\nComputer Networking\nSimply put, a computer network is a collection of interconnected devices that can communicate with each other using a set of rules called protocols. Networking allows devices to share resources, exchange data, and collaborate on tasks. On a deeper level, it helps to understand the conceptual models that describe how data moves through a network. Before we dive in, let‚Äôs go over some basic terminology.\n\nThe OSI Model\nNow that you understand some common terms and concepts, we can dive into the conceptual models. The Open Systems Interconnection (OSI) Model divides networking into seven layers, each handling specific aspects of network communication.\n\nPhysical Layer: Physical medium, electrical signals, cables, and hardware\nData Link Layer: Physical addressing (MAC addresses), error detection\nNetwork Layer: Logical addressing (IP addresses), routing\nTransport Layer: End-to-end connections, reliability (TCP/UDP)\nSession Layer: Session establishment, management, and termination\nPresentation Layer: Data translation, encryption, compression\nApplication Layer: User interfaces and services (HTTP, SMTP, etc.)\n\n\n\nThe TCP/IP Model\nThe OSI Model is conceptual, but the TCP/IP Model is more practical and has four layers.\n\nNetwork Access Layer: Combines OSI‚Äôs Physical and Data Link layers\nInternet Layer: Similar to OSI‚Äôs Network layer (IP)\nTransport Layer: Same as OSI‚Äôs Transport layer (TCP/UDP)\nApplication Layer: Combines OSI‚Äôs Session, Presentation, and Application layers\n\nUnderstanding these network models isn‚Äôt just theoretical‚Äîit provides a systematic approach to troubleshooting. When connection issues arise, you can methodically check each layer: Is the physical connection working? Is IP addressing correct? Is the transport protocol functioning? This layered approach helps isolate and resolve problems efficiently.\n\n\nNetwork Protocols\nRemember, a protocol is a set of rules that determine how data is transmitted between devices on a network. You can think of protocols in one of two camps, Connection-Oriented and Connectionless. Within these camps, two protocols stand out as the backbone of the internet‚Äôs data transfers: TCP and UDP.\nTCP (Transmission Control Protocol) is a connection-oriented protocol that establishes a dedicated end-to-end connection before transmitting data. TCP is used when reliability is more important than speed (e.g., web browsing, email, file transfers). It has four defining traits:\n\nReliability: Guarantees delivery of packets in the correct order\nFlow Control: Prevents overwhelming receivers with too much data\nError Detection: Identifies and retransmits lost or corrupted packets\nHandshake Process: Three-way handshake establishes connections\n\nCommon TCP Applications: Web browsing (HTTP/HTTPS), email (SMTP, IMAP), file transfers (FTP, SCP), and secure shell (SSH)\nUDP (User Datagram Protocol) is a connectionless protocol that sends data without establishing a dedicated connection. UDP is used for real-time applications (e.g., video streaming, VoIP, online gaming). It also has four defining traits:\n\nSimplicity: No connection setup or maintenance overhead\nSpeed: Faster than TCP due to fewer checks and guarantees\nLower Reliability: No guarantee of delivery or correct ordering\nEfficiency: Better for real-time applications where occasional data loss is acceptable\n\nCommon UDP Applications: DNS lookups, DHCP address assignment, streaming video, VoIP calls, and online gaming\nBeyond those, there are some other important protocols to know, because they provide the foundation for most of the user-friendly features we are used to today.\n\nIP (Internet Protocol)\n\nIP handles addressing and routing of packets across networks. There are two versions in common use:\nIPv4: 32-bit addresses (e.g., 192.168.1.1)\nIPv6: 128-bit addresses (e.g., 2001:0db8:85a3:0000:0000:8a2e:0370:7334)\n\nICMP (Internet Control Message Protocol)\n\nICMP helps diagnose network issues by sending error messages and operational information. The ping command uses ICMP to test connectivity.\n\nHTTP/HTTPS (Hypertext Transfer Protocol)\n\nHTTP and its secure variant HTTPS are application-layer protocols used for web browsing.\n\nDNS (Domain Name System)\n\nDNS translates human-readable domain names (like google.com) into IP addresses.\n\n\nMany standard protocols have secure variants that add encryption: HTTP becomes HTTPS via TLS/SSL, telnet is replaced by SSH, and FTP gives way to SFTP or FTPS. These secure protocols wrap the original protocol‚Äôs data in encryption layers, protecting sensitive information from interception or tampering.\n\n\n\nNetwork Connections\nThere are two ways for systems to connect to the internet: wired and wireless.\n\nWired Connections\nEthernet is the most common wired networking technology. Its name comes from the term ether referring to a theoretical medium that was believed to carry light waves through space. It was developed by Robert Metcalf and David Boggs at Xerox‚Äôs PARC facility in the 1970s. The goal was to provide a more stable LAN which could facilitate high-speed transfers between computers and laser printers. They succeeded, and had improved on a precursor‚Äôs, ALOHAnet, design by creating a system that could detect collisions‚Äîwhen two devices try to transmit at the same time. Here are some key traits:\n\nReliability: Less susceptible to interference\nSpeed: Typically faster and more stable than wireless\nSecurity: Harder to intercept without physical access\nConnectors: RJ45 connectors on Ethernet cables\nStandards: 10/100/1000 Mbps (Gigabit) are common speeds\n\n\n\n\n\n\n\n\n\n\nCable Type\nMax Speed\nMax Distance\nNotes\n\n\n\n\nCat 5e\n1 Gbps\n100 meters\nMinimum for modern networks\n\n\nCat 6\n10 Gbps\n55 meters\nBetter crosstalk protection\n\n\nCat 6a\n10 Gbps\n100 meters\nImproved shielding\n\n\nCat 7/8\n40+ Gbps\n100 meters\nFully shielded, enterprise use\n\n\n\nFor most Raspberry Pi projects, Cat 5e or Cat 6 cables are more than sufficient.\n\n\nWireless Connections\nWi-Fi allows devices to connect to networks without physical cables. Its name is not short for Wireless Fidelity, but actually a marketing choice by the brand-consulting firm Interbrand. They chose the name because it sounded similar to Hi-Fi. Wi-Fi was developed by numerous researchers and engineers, but the key breakthrough was by Dr.¬†John O‚ÄôSullivan from CSIRO in Australia. His work focused on a wireless LAN, which would eventually become the IEEE (Institute of Electrical and Electronics Engineers) 802.11 standard in 1997. Eventually, Apple would help with widespread adoption by including the AirPort feature on its laptops, enabling Wi-Fi connectivity out of the box. Here are some key traits:\n\nConvenience: No cables required, more flexible placement\nStandards: 802.11a/b/g/n/ac/ax (Wi-Fi 6) with varying speeds and ranges\nSecurity: WEP, WPA, WPA2, and WPA3 encryption standards (WPA2/WPA3 recommended)\n\n\n\nNetwork Interface Names in Linux\nIn Ubuntu Server, network interfaces follow a predictable naming convention:\n\neth0, eth1: Traditional Ethernet interface names\nenp2s0, wlp3s0: Modern predictable interface names (based on device location)\n\nUbuntu moved to predictable interface naming to solve a critical problem:\n\nIn traditional naming (eth0, eth1), names could change unexpectedly after hardware changes or reboots.\nModern names like enp2s0 encode the physical location of the network card (PCI bus 2, slot 0), ensuring the same interface always gets the same name regardless of detection order.\n\n\nwlan0, wlan1: Traditional wireless interface names\n\n\n\nIP Addressing\nAn IP (Internet Protocol) Address is a unique identifier for a device on the internet, or a LAN. There are two different kinds of addresses: IPv4 and IPv6.\nIPv4 uses 32-bit addresses, providing approximately 4.3 billion unique addresses (now largely exhausted):\n\nFormat: Four octets (numbers 0-255) separated by dots (e.g., 192.168.1.1)\nClasses: Traditionally divided into classes A, B, C, D, and E\nPrivate Ranges:\n\n10.0.0.0 to 10.255.255.255 (10.0.0.0/8)\n172.16.0.0 to 172.31.255.255 (172.16.0.0/12)\n192.168.0.0 to 192.168.255.255 (192.168.0.0/16)\n\nSubnet Masks: Used to divide networks (e.g., 255.255.255.0 or /24)\nIssues: IPv4 address exhaustion due to limited capacity\n\nIPv6 uses 128-bit addresses, providing approximately 3.4√ó10^38 unique addresses:\n\nFormat: Eight groups of four hexadecimal digits separated by colons (e.g., 2001:0db8:85a3:0000:0000:8a2e:0370:7334)\nShorthand: Leading zeros in a group can be omitted, and consecutive groups of zeros can be replaced with :: (only once)\n\nExample: 2001:db8:85a3::8a2e:370:7334\n\nAddress Types:\n\nUnicast: Single interface\nAnycast: Multiple interfaces (closest responds)\nMulticast: Multiple interfaces (all respond)\n\nBenefits: More addresses, improved security, simplified headers, no need for NAT\n\nOne final note, CIDR (Classless Inter-Domain Routing) notation represents IP addresses and their associated routing prefix:\n\nFormat: IP address followed by ‚Äú/‚Äù and prefix length (e.g., 192.168.1.0/24)\nCalculation: A prefix of /24 means the first 24 bits are the network portion, leaving 8 bits for hosts (allowing 2^8 = 256 addresses)\n\nFor a network using CIDR notation 192.168.1.0/24:\n\nThe /24 means the first 24 bits (3 octets) identify the network\nThis leaves 8 bits for hosts (2^8 = 256 potential addresses)\nSubtract 2 for network address (.0) and broadcast address (.255)\nResult: 254 usable IP addresses (192.168.1.1 through 192.168.1.254)\n\n\n\n\nUbuntu Server Networking Tools\nNow that we‚Äôve covered the basic concepts, it‚Äôs time to dive into the actual commands and tools that will let you configure and manage your server‚Äôs network. To start, you can view network interfaces and their statuses using the command ip link show, or ip addr show for your IP Address configuration. You can view only the IPv4 or IPv6 addresses using ip -4 addr or ip -6 addr, respectively.\n\nTesting Connectivity\nAlthough it seems redundant if you already viewed your IP addresses, you can also test connectivity using the ping and traceroute commands. These will be more useful for checking your server‚Äôs network status from your desktop or laptop.\nTest basic connectivity to a host:\nping -c 4 google.com\n\nTrace the route to a destination:\n# First update and install your packages\nsudo apt update && sudo apt upgrade -y\n\n# Install traceroute\nsudo apt install traceroute -y\n\n# Run traceroute\ntraceroute google.com\n\nCheck the DNS resolution:\nnslookup google.com\n# dig google.com\n\n\n\nViewing Network Statistics\nYou can view more specific network information with the ss command. This command‚Äôs name is an acronym for socket statistics and is used as a replacement for the older netstat plan because it offers faster performance and a more detailed output. Additionally, you can filter by specific protocol.\nss -tuln\nThe tuln flag is made up of four separate flags:\n\n-t, displays only TCP sockets\n-u, displays only UDP sockets\n-l, displays listening sockets\n-n, displays addresses numerically, instead of resolving them\n\n\n\nConfiguration Files\nFinally, there are a few crucial configuration files that will handle the bulk of your networking. In Ubuntu Server, network interfaces and DNS configurations are configured and stored in the /etc/ directory.\nNetwork Interfaces:\n\n/etc/netplan/: Contains YAML configuration files for Netplan\n/etc/network/interfaces: Configuration method (if NetworkManager is used)\n\nDNS Configuration:\n\n/etc/resolv.conf: DNS resolver configuration\n/etc/hosts: Static hostname to IP mappings\n/etc/hostname: System hostname\n\n\n\n\nsystemd-networkd\nsystemd-networkd is a system daemon that manages network configurations in modern Linux distributions. It‚Äôs part of the systemd suite and provides network configuration capabilities through simple configuration files.\nIt generally works using three key components:\n\nConfiguration Files: You define network settings in .network files located in /etc/systemd/network/\nService Management: systemd-networkd runs as a system service to apply and maintain network configurations\nIntegration: Works closely with other systemd components for DNS resolution and networking\n\n\nBasic Wired Configuration\nsystemd-networkd uses configuration files with .network extension. Each file consists of sections with key-value pairs. A basic configuration for a static IP would look like this:\n# /etc/systemd/network/20-wired.network\n[Match]\nName=eth0\n\n[Network]\nAddress=192.168.1.100/24\nGateway=192.168.1.1\nDNS=8.8.8.8\nDNS=8.8.4.4\nLet‚Äôs walk through the configuration file‚Äôs structure:\n\nFile Naming Convention:\n\nThe file is named 20-wired.network.\nThe number prefix (20-) determines the processing order (lower numbers processed first), allowing you to create prioritized configurations.\nThe suffix .network tells systemd-networkd that this is a network interface configuration file.\n\n[Match] Section:\n\nThis critical section determines which network interfaces the configuration applies to.\nName=eth0: This specifies that the configuration should apply to the eth0 interface only.\nYou can use wildcards (e.g., eth* would match all Ethernet interfaces) or match by other properties such as MAC address using MACAddress=xx:xx:xx:xx:xx:xx.\nBehind the scenes:\n\nsystemd-networkd scans all available network interfaces.\nCompares their properties against those specified in the [Match] section.\nIf all properties match, the configuration is applied to that interface.\n\n\n[Network] Section:\n\nThis section defines the network configuration parameters.\nAddress=192.168.1.100/24: Sets a static IPv4 address with CIDR notation. The /24 represents the subnet mask (equivalent to 255.255.255.0) and defines the network boundary.\nGateway=192.168.1.1: Specifies the default gateway for routing traffic outside the local network. All traffic not destined for the local subnet (192.168.1.0/24) will be sent to this IP address.\nDNS=8.8.8.8 and DNS=8.8.4.4: These are Google‚Äôs public DNS servers. When specified, systemd-networkd will automatically configure /etc/resolv.conf through systemd-resolved. You can specify multiple DNS servers, and they will be tried in order.\n\nBehind the scenes:\n\nsystemd-networkd identifies the eth0 interface\nAssigns the static IP address using kernel netlink sockets\nSets up the routing table to use the specified gateway\nCommunicates with systemd-resolved to configure DNS settings\nMaintains this configuration and reapplies it if the interface goes down and back up\n\n\nThis configuration example works well for server environments where static, predictable networking is preferable. This is a declarative configuration, it describes the desired state, rather than the steps to achieve it, so repeated application produces the same result.\n\n\nDHCP with a Wired Connection\nIf you want to add DHCP, you can use the following:\n# /etc/systemd/network/20-wired.network\n[Match]\nName=eth0\n\n[Network]\nDHCP=yes\nLet‚Äôs walk through the differences between a dynamic and static host configuration file structure:\n\nDHCP=yes: This single line replaces all the static configuration parameters from the previous example.\n\nIt instructs systemd-networkd to obtain IP address, subnet mask, gateway, DNS servers, and other network parameters automatically from a DHCP server.\nYou can also use DHCP=ipv4 to enable only IPv4 DHCP, or DHCP=ipv6 for only IPv6 DHCP, or DHCP=yes for both.\n\nBehind the scenes:\n\nsystemd-networkd identifies the eth0 interface\nInitiates the DHCP client process, which follows the DHCP protocol‚Äôs Discover-Offer-Request-Acknowledge (DORA) sequence:\n\nThe client broadcasts a DISCOVER message\nAvailable DHCP servers respond with OFFER messages\nThe client selects an offer and sends a REQUEST\nThe selected server sends an ACKNOWLEDGE\n\nApplies all the received network parameters (IP, subnet, gateway, DNS)\nSets up a lease timer to manage when the configuration needs renewal\nHandles DHCP lease renewals automatically\n\nAdvantages:\n\nSimplified configuration maintenance - no need to update parameters when network details change\nWorks well in networks where IP assignments are centrally managed\nAutomatically adapts to network changes\n\n\nThis configuration works well for environments where network parameters are dynamic or managed by a network admin through DHCP.\n\n\nWireless Configurations and wpa_supplicant\nWhile wired connections are a basic part of networking, wireless connections require some extra work. More specifically, with systemd-networkd, you‚Äôll need a tool like WPA. Wi-Fi Protected Access (WPA) emerged as a response to weaknesses in the original Wired Equivalent Privacy (WEP) security protocol. As wireless networks became ubiquitous, secure authentication and encryption mechanisms became essential. The Linux ecosystem offers several powerful tools for managing these connections:\n\nwpa_supplicant: The core daemon that handles wireless connections\nwpa_cli: A command-line interface for controlling wpa_supplicant dynamically\nwpa_passphrase: A utility for generating secure password hashes\n\nOn the systemd-networkd side of things, the configuration is simple, broken down in detail below.\n# /etc/systemd/network/25-wireless.network\n[Match]\nName=wlan0\n\n[Network]\nDHCP=yes\n\nWireless Interface:\n\nThe configuration targets wlan0, which is the traditional name for the first wireless network interface in Linux.\n\nMinimal Configuration:\n\nThe file only has the information needed by systemd-networkd to manage the IP addressing aspect of the wireless connection. Note what‚Äôs missing: there‚Äôs no SSID, password, or security protocol information. This is because:\nsystemd-networkd isn‚Äôt designed to handle wireless authentication and association\nThis separation of concerns is intentional in the systemd design philosophy - specialized tools should handle specialized tasks\n\nIntegration with wpa_supplicant:\n\nwpa_supplicant is the standard Linux utility for managing wireless connections\nsystemd-networkd handles the network layer (Layer 3) configuration once wpa_supplicant establishes the data link layer (Layer 2) connection\nThis division follows the OSI model‚Äôs separation of network layers\n\nBehind the scenes:\n\nwpa_supplicant handles wireless scanning, authentication, and association\nOnce a wireless link is established, it notifies the system\nsystemd-networkd detects the active interface that matches wlan0\nIt then initiates the DHCP client process to configure the network parameters\n\nThis separation provides flexibility and security\n\nThe wireless security operations are handled by a dedicated, well-tested component\nNetworking remains under systemd-networkd‚Äôs control for consistency with other interfaces\n\n\nWhile the systemd-networkd configuration is straightforward, things get more complicated with WPA. In standard wpa_supplicant configuration files, wireless passwords are often stored in plaintext. This creates a security vulnerability - anyone with access to the configuration file can view the password.\n# /etc/wpa_supplicant/wpa_supplicant-wlan0.conf\nctrl_interface=/run/wpa_supplicant\nupdate_config=1\n\nnetwork={\n    ssid=\"YourNetworkSSID\"\n    psk=\"YourWiFiPassword\"\n}\nThe wpa_passphrase tool solves this problem by generating a pre-computed hash of the password. Running this is straightforward as the basic syntax is wpa_passphrase [SSID] [passphrase]. Then, WPA outputs a hashed version of your password.\n# Generate a hashed passphrase\nwpa_passphrase \"MyHomeNetwork\" \"MySecurePassword123\"\nTo then use the hashed password in your configuration, you can run the following command, just make sure to remove the line with the plaintext password from the config file after running it:\n# Generate the hash and save directly to the configuration file\nwpa_passphrase \"MyHomeNetwork\" \"MySecurePassword123\" | sudo tee -a /etc/wpa_supplicant/wpa_supplicant-wlan0.conf\nWhen you use wpa_passphrase:\n\nIt combines the SSID and password using the PBKDF2 (Password-Based Key Derivation Function 2) algorithm\nIt applies 4096 iterations of HMAC-SHA1 for key strengthening\nThe result is a 256-bit (32-byte) hash represented in hexadecimal format\nThis hash is what‚Äôs actually used for the authentication process, not the original password\n\nThis approach makes it virtually impossible to reverse-engineer the original password from the hash.\n# /etc/wpa_supplicant/wpa_supplicant-wlan0.conf\nctrl_interface=/run/wpa_supplicant\nupdate_config=1\n\nnetwork={\n    ssid=\"MyHomeNetwork\"\n    #psk=\"MySecurePassword123\"\n    psk=a8e665b82929d810746c5a1208c472f9d2a25db67a6bc32a99fa4158aea02175\n}\nNow that you have an idea about the basic structure of this file, let‚Äôs go over some key points:\n\nFile Naming Convention:\n\nThe file wpa_supplicant-wlan0.conf is specifically named to associate with the wlan0 interface.\nThis naming allows different wireless interfaces to have different configurations.\n\nConfiguration Directives:\n\nctrl_interface=/run/wpa_supplicant: This specifies the control interface path, which is a socket that allows programs to communicate with wpa_supplicant. This enables tools like wpa_cli to connect and control wpa_supplicant dynamically.\nupdate_config=1: Allows wpa_supplicant to update the configuration file automatically, useful when network details change or when using wpa_cli to add networks interactively.\n\nNetwork Block:\n\nThe network={} block defines a single wireless network configuration.\nssid=\"YourNetworkSSID\": The Service Set Identifier - the name of the wireless network to connect to.\npsk=\"YourWiFiPassword\": The Pre-Shared Key - the password for the wireless network in plaintext.\n\nSecurity Considerations:\n\nWhen you enter the password in plaintext as shown, wpa_supplicant will automatically convert it to a hash during processing.\nFor better security, you can pre-hash the password using: wpa_passphrase ‚ÄúYourNetworkSSID‚Äù ‚ÄúYourWiFiPassword‚Äù and use the generated hash.\nThe configuration file should have restricted permissions (600) to prevent other users from reading the passwords.\n\nBehind the scenes:\n\nwpa_supplicant reads this configuration at startup\nIt scans for available wireless networks\nWhen it finds the specified SSID, it attempts to authenticate using the provided credentials\nIt handles all the wireless protocol handshakes, including:\n\nAuthentication and association with the access point\nNegotiation of encryption parameters\nEstablishment of the encrypted channel\n\n\n\nOnce connected, it maintains the connection and handles roaming between access points with the same SSID. This configuration represents the minimum needed for a WPA/WPA2 Personal network connection. For more complex scenarios like enterprise authentication (WPA-EAP), additional parameters would be needed in the network block.\nWhile the wpa_supplicant configuration files provide static configuration that saves when you write out, wpa_cli offers interactive, dynamic control over wireless connections. First ensure wpa_supplicant is running with a control interface by using ps aux | grep wpa_supplicant. If it‚Äôs running with the -c flag pointing to a config file that contains the ctrl_interface=/run/wpa_supplicant line, you can connect to it.\n\n\n\n\n\n\nImportant\n\n\n\nAs a heads up, because we already created the wlan0 configuration file manually, the following steps are just for your knowledge. You‚Äôll probably get some messages saying FAIL if you try to run some of the commands, but I think it‚Äôs good to learn them anyway‚Äîeven though they aren‚Äôt necessarily important right now.\n\n\nFirst, start the interactive mode with sudo wpa_cli, or specify the interface with sudo wpa_cli -i wlan0. Let‚Äôs go over some essential commands:\n# Show help\nhelp\n\n# List all available commands\nhelp all\n\n# List available networks\nscan\nscan_results\n\n# Show current status\nstatus\n\n# List configured networks\nlist_networks\n\n# Add a new network\nadd_network\nStep-By-Step: Adding a Network\n&gt; add_network\n0\n&gt; set_network 0 ssid \"MyNetwork\"\nOK\n&gt; set_network 0 psk \"MyPassword\"\nOK\n&gt; set_network 0 priority 5 \nOK\n&gt; enable_network 0\nOK\n&gt; save_config\nOK\n\n# For networks with hashed passwords\n&gt; add_network\n0\n&gt; set_network 1 ssid \"MyNetwork\"\nOK\n&gt; set_network 1 psk 0a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0u1v2w3x4y5z\nOK\n&gt; set_network 1 priority 10\nOK\n&gt; enable_network 1\nOK\n&gt; save_config\nOK\nIt‚Äôs good to know that higher priority values (like 10) are preferred over lower ones (like 5). Now, you can also use wpa by running one-off commands or writing scripts with non-interactive mode. Additionally, you should know that when you first boot up your Raspberry Pi, the internet will be managed by Netplan (more on that later in this section). So, if you try to use wpa_cli save_config after creating the config files, that will return FAIL. Instead, once you write the files in the proper directories, run the reconfigure command.\n# Scan for networks\nsudo wpa_cli scan\nsudo wpa_cli scan_results\n\n# Save the current configuration\nsudo wpa_cli save_config\n\n# Reconnect to the network\nsudo wpa_cli reconfigure\nFinally, you can monitor signal quality and connection status by using the signal_poll command. The RSSI (Received Signal Strength Indicator) shows connection quality in dBm. Values closer to 0 indicate stronger signals. Additionally, you can debug connection issues using status.\n&gt; signal_poll\nRSSI=-67\nLINKSPEED=65\nNOISE=9999\nFREQUENCY=5220\n\n&gt; status\nbssid=00:11:22:33:44:55\nfreq=5220\nssid=MyNetwork\nid=0\nmode=station\npairwise_cipher=CCMP\ngroup_cipher=CCMP\nkey_mgmt=WPA2-PSK\nwpa_state=COMPLETED\nip_address=192.168.1.100\nNow that we‚Äôve covered a lot of the great features available with wpa_cli, it‚Äôs time to continue configuring our server. You may remember me mentioning that the Raspberry Pi default networking tool is Netplan. Before we can enable and start the wlan0 service (meaning your primary wifi is on), we need to safely shut down Netplan.\n\n\n\nConverting Netplan to networkd\nIt‚Äôs no surprise that Raspberry Pi uses Netplan as the default network manager because it provides a consistent interface for network configuration; however, there are several reasons you might want to use systemd-networkd directly:\n\nSimplicity: Direct systemd-networkd configuration eliminates a layer of abstraction\nControl: Direct access to all of systemd-networkd‚Äôs features without Netplan‚Äôs limitations\nIntegration: Better alignment with other systemd components\nLearning: Understanding the underlying network configuration system\nPerformance: Potentially faster setup without the translation layer\n\nUbuntu Server uses a layered approach to network configuration:\n\nUser configuration layer: YAML files in /etc/netplan/\nTranslation layer: Netplan reads YAML files and generates configurations for a backend\nBackend layer: Either systemd-networkd or NetworkManager applies the actual configuration\n\nBy removing the middle layer (Netplan), we‚Äôre configuring the backend directly. As you can see from the previous parts of this Networking section in the guide, I like the learning value and long-term potential of systemd, which is why I went with it over Netplan.\n\nStep-by-step Migration\n\nBegin by creating backups of your current network configuration\n\n# Create a backup directory\nsudo mkdir -p /etc/netplan/backups\n\n# Copy all netplan config files\nsudo cp /etc/netplan/*.yaml /etc/netplan/backups/\n\n# Document the current network state\nsudo ip -c addr | sudo tee /etc/netplan/backups/current-ip-addr.txt\nsudo ip -c route | sudo tee /etc/netplan/backups/current-ip-route.txt\n\nReview your existing Netplan so you know what to recreate\n\n# View your current netplan configs\ncat /etc/netplan/*.yaml\n\nNow, create the corresponding systemd-networkd configuration files in /etc/systemd/network/.\n\nFor each interface (wired, wireless, etc.) in your Netplan configuration, create a corresponding .network file, with the appropriate configurations (i.e.¬†static vs.¬†DHCP).\nRemember: For wireless connections, you need both a systemd and a wpasupplicant configuration.\n\n\n# Create the directory if it doesn't exist\nsudo mkdir -p /etc/systemd/network/\n\n# For an Ethernet configuration\nsudo nano /etc/systemd/network/20-wired.network\n\n# For a Wireless configuration\nsudo nano /etc/systemd/network/25-wireless.network\nsudo nano /etc/wpa_supplicant/wpa_supplicant-wlan0.conf\n\nNow that your networkd configuration is in place, disable Netplan.\n\n# Ensure systemd-networkd is enabled\nsudo systemctl enable systemd-networkd\nsudo systemctl enable systemd-resolved\nsudo systemctl start systemd-networkd\n\n# Move the Netplan configurations to a disabled state\nsudo mkdir -p /etc/netplan/disabled\nsudo mv /etc/netplan/*.yaml /etc/netplan/disabled/\n\n# Create a minimal netplan configuration that defers to systemd-networkd\nsudo tee /etc/netplan/01-network-manager-all.yaml &gt; /dev/null &lt;&lt; EOF\nnetwork:\n  version: 2\n  renderer: networkd\nEOF\nLet‚Äôs take a look at the systemctl enable and start commands, because without them we will lose connectivity when turning off netplan. Before we do that, however, what the minimal netplan configuration file does is essentially tell Netplan to just use networkd. We‚Äôll remove it once we are sure everything is up and running.\n\nEnable Command:\n\nsudo systemctl enable systemd-networkd: This configures systemd-networkd to start automatically when the system boots.\nBehind the scenes:\n\nThis command creates the necessary symbolic links in systemd‚Äôs unit directories so that the network daemon will be started by systemd during the boot process.\nIt integrates the service into systemd‚Äôs dependency tree.\nWithout this step, you would need to manually start networkd after each reboot, which is impractical for a server environment.\n\n\nStart Command:\n\nsudo systemctl start systemd-networkd: This launches the systemd-networkd daemon immediately.\nBehind the scenes:\n\nsystemd spawns the networkd process, which then:\n\nReads all .network, .netdev, and .link configuration files in /etc/systemd/network/ and /usr/lib/systemd/network/\nApplies the configurations to matching interfaces\nSets up monitoring for network changes\n\n\n\nRestart Command:\n\nsudo systemctl restart systemd-networkd: This stops and then starts the daemon again, ensuring all configuration changes are applied.\nBehind the scenes:\n\nsystemd sends a termination signal to the running networkd process, waits for it to exit cleanly, and then starts a new instance.\nThe new instance repeats the initialization process, reading all configuration files again.\nThis is the command you‚Äôll use most frequently when making changes to network configurations.\n\n\nWhy Restart Is Necessary:\n\nWhile systemd-networkd does monitor for some changes, editing configuration files doesn‚Äôt automatically trigger a reconfiguration.\nThe restart ensures that:\n\nAll new or modified configuration files are re-read\nAny removed configurations are no longer applied\nAll interface configurations are freshly evaluated against the current state\n\n\nImpact on Network Connectivity:\n\nA restart will temporarily disrupt network connectivity as interfaces are reconfigured\nFor remote servers, use caution when restarting network services to avoid losing your connection\nFor critical remote systems, consider using a command pipeline, like:\n\nsudo systemctl restart systemd-networkd.service || (sleep 30 && sudo systemctl start systemd-networkd.service)\nWhich attempts to restart and then tries to start the service again after 30 seconds if connectivity is lost\n\n\n\n\nApply the systemd-networkd network configuration\n\n# Apply Netplan changes (this will do nothing as we now have a minimal config)\nsudo netplan apply\n\n# Restart systemd-networkd to apply our direct configuration\nsudo systemctl restart systemd-networkd\n\n# The next step is to enable and start the wpa_supplicant service.\n\nsudo systemctl enable wpa_supplicant@wlan0.service\nsudo systemctl start wpa_supplicant@wlan0.service\nThese commands are crucial for integrating wpa_supplicant with systemd, let‚Äôs break them down:\n\nService Template:\n\nThe wpa_supplicant@wlan0.service syntax uses systemd‚Äôs template unit feature.\nThe @ symbol indicates a template service, and wlan0 is the instance name that gets passed to the template.\nThis allows the same service definition to be used for different wireless interfaces.\n\nEnable Command:\n\nsudo systemctl enable wpa_supplicant@wlan0.service: This creates symbolic links from the system‚Äôs service definition directory to systemd‚Äôs active service directory, ensuring the service starts automatically at boot.\nBehind the scenes:\n\nThis modifies systemd‚Äôs startup configuration by adding the service to the correct target units. Typically multi-user.target.\nThe symbolic links created point to the wpa_supplicant service template file.\n\n\nStart Command:\n\nsudo systemctl start wpa_supplicant@wlan0.service: This immediately starts the service without waiting for a reboot.\nBehind the scenes:\n\nsystemd executes the wpa_supplicant binary with appropriate arguments\nDerived from the service template and the instance name (wlan0).\nThe command effectively executed is similar to: /usr/sbin/wpa_supplicant -c /etc/wpa_supplicant/wpa_supplicant-wlan0.conf -i wlan0\n\n\nIntegration with systemd-networkd:\n\nWhen wpa_supplicant successfully connects to a wireless network, it brings the interface up\nsystemd-networkd detects this state change through kernel events\nsystemd-networkd then applies the matching network configuration (our earlier 25-wireless.network file)\nIf DHCP is enabled, the DHCP client process begins\n\nBenefits of this systemd configuration:\n\nDependency management (services can start in the correct order)\nAutomatic restart if the service fails\nStandardized logging through journald\nConsistent management interface alongside other system services\nThe template approach allows for modular configuration that can be easily expanded if you add more wireless interfaces to your Raspberry Pi.\n\n\n\nVerify the new configuration by checking the systemctl status and running simple network check commands\n\n# Check systemd-networkd status\nsystemctl status systemd-networkd\n\n# Check interface status\nip addr show\n\n# Test connectivity\nping -c 4 google.com\nYou should see outputs that look like this: Note, I didn‚Äôt show the output of ip addr because I don‚Äôt want to accidentally post my actual IP address online.\n\n\n\nMake the change permanent\n\n# Remove the minimal Netplan configuration\nsudo rm /etc/netplan/01-network-manager-all.yaml\n\n# Mask the Netplan service to prevent it from running\nsudo systemctl mask netplan-wpa@.service\nsudo systemctl mask netplan-ovs-cleanup.service\nsudo systemctl mask netplan-wpa-wlan0.service\nOne final note before moving on, by the time I removed the generic netplan configuration, my system did not have netplan-wpa.service or netplan-wpa-wlan0.service. I forgot to look before I tested the previous steps, so I‚Äôm not sure if I did, but I‚Äôll leave them here just in case someone needs them. That being said, I was able to mask netplan-ovs-cleanup.service successfully.\n\n\nTroubleshooting\nOnce you‚Äôve finished making changes and applying them, verify that everything is up, running, and as you expect.\n# Check systemd-networkd status\nsystemctl status systemd-networkd\n\n# View network status\nnetworkctl status\n\n# List all network links\nnetworkctl list\nThese are crucial commands for troubleshooting and confirming your network configuration, let‚Äôs break them down:\n\nsystemd-networkd Status Check:\n\nsystemctl status systemd-networkd: This displays the current status of the systemd-networkd service. The output includes:\n\nWhether the service is active, inactive, or failed\nWhen it was started and how long it‚Äôs been running\nThe process ID and memory usage\nRecent log entries directly related to the service\n\nBehind the scenes:\n\nThis queries systemd‚Äôs internal service management database and pulls relevant information from the journal logging system.\nUseful pattern: Look for ‚ÄúActive: active (running)‚Äù to confirm the service is working properly and check the logs for any warning or error messages.\n\n\nNetwork Status Overview:\n\nnetworkctl status: This command provides a comprehensive overview of your system‚Äôs network state.\n\nThe output includes:\n\nHostname and domain information\nGateway and DNS server configurations\nCurrent network interfaces and their states\nNetwork addresses (IPv4 and IPv6)\n\n\nBehind the scenes:\n\nThis tool directly communicates with systemd-networkd using its D-Bus API to retrieve the current network state.\nThis command is particularly useful because it aggregates information that would otherwise require multiple different commands to collect.\n\n\nNetwork Links Enumeration:\n\nnetworkctl list: This lists all network interfaces known to systemd-networkd.\n\nThe output shows:\n\nInterface index numbers\nInterface names\nInterface types (ether, wlan, loopback, etc.)\nOperational state (up, down, dormant, etc.)\nSetup state (configured, configuring, unmanaged)\n\n\nBehind the scenes:\n\nLike the status command, this uses systemd-networkd‚Äôs D-Bus API to enumerate all network links and their current states.\nThis provides a quick way to verify which interfaces systemd-networkd is managing and their current status.\n\n\nTroubleshooting with These Commands:\n\nStart with systemctl status systemd-networkd to ensure the service is running\nUse networkctl list to see which interfaces are detected and their states\nIf an interface shows ‚Äúconfiguring‚Äù instead of ‚Äúconfigured,‚Äù check for configuration errors\nUse networkctl status to verify DNS settings and addressing\nFor more detailed logs: journalctl -u systemd-networkd shows all logs from the networkd service\n\n\nThese commands represent the primary diagnostic tools when working with systemd-networkd. They provide a layered approach to troubleshooting - from service-level status to detailed interface information - that helps pinpoint issues in your network configuration. If you need more:\n\nNetwork Connectivity Loss:\n\nConnect directly to the device via console or keyboard/monitor\nCheck logs with journalctl -u systemd-networkd\nRestore the Netplan configuration from your backup if needed\n\nDNS Resolution Issues:\n\nEnsure systemd-resolved is running: systemctl status systemd-resolved\nCheck /etc/resolv.conf is a symlink to /run/systemd/resolve/stub-resolv.conf\nIf not, create it: sudo ln -sf /run/systemd/resolve/stub-resolv.conf /etc/resolv.conf\n\nConfiguration Errors:\n\nVerify syntax with networkctl list to see if interfaces are ‚Äúconfigured‚Äù or ‚Äúconfiguring‚Äù\nCheck for errors with journalctl -u systemd-networkd -n 50\n\n\nsystemd-networkd and systemd-resolved: Working Together\nWhile systemd-networkd handles interface configuration and routing, systemd-resolved manages DNS resolution‚Äîthey‚Äôre designed as complementary components that work together for complete network functionality. When you specify DNS servers in a networkd configuration file, networkd communicates this information to resolved, which then manages the actual DNS queries and caching.\nThis separation follows systemd‚Äôs philosophy of modular components with specific responsibilities. You can see this relationship in action when networkd applies a configuration with DNS settings‚Äîit doesn‚Äôt directly modify /etc/resolv.conf, but instead passes the information to resolved, which creates a special version of resolv.conf (usually a symlink to /run/systemd/resolve/stub-resolv.conf).\nBoth services need to be enabled and running for full functionality:\nsudo systemctl enable systemd-networkd systemd-resolved\nsudo systemctl start systemd-networkd systemd-resolved\nIf you‚Äôre troubleshooting DNS issues, remember to check both services‚Äô status, as a problem with either one can affect name resolution. You can view resolved‚Äôs current DNS configuration with resolvectl status.\n\n\n\nAdvanced Networking\n\nSubnets\nA subnet is a logical subdivision of an IP network. Subnetting allows network administrators to partition a large network into smaller, more manageable segments. Subnetting serves several important functions:\n\nAddress Conservation: More efficient allocation of limited IPv4 address space\nSecurity Segmentation: Isolating sensitive systems from general network traffic\nBroadcast Domain Control: Reducing broadcast traffic by limiting its scope\nHierarchical Addressing: Simplifying routing tables and network management\nTraffic Optimization: Improving network performance by segregating traffic types\n\nA subnet mask determines which portion of an IP address refers to the network and which portion refers to hosts within that network. Consider an IPv4 address: 192.168.1.10 with subnet mask 255.255.255.0 (/24)\n\nIn binary:\n\nIP: 11000000.10101000.00000001.00001010\nMask: 11111111.11111111.11111111.00000000\nThe 1s in the mask represent the network portion, while the 0s represent the host portion.\n\nIn CIDR Notation:\n\n/24 means the first 24 bits identify the network (equivalent to 255.255.255.0)\n/16 means the first 16 bits identify the network (equivalent to 255.255.0.0)\n\nSubnet Calculations: For a /24 network\n\nNetwork address: First address in range (e.g., 192.168.1.0)\nBroadcast address: Last address in range (e.g., 192.168.1.255)\nAvailable host addresses: 2^(32-prefix) - 2 = 2^8 - 2 = 254 usable addresses\n\n\nCreating subnets involves both network design and interface configuration. Here‚Äôs how to implement subnetting on a Linux server using systemd-networkd:\n\nScenario 1: Simple Subnet Isolation\n\nThis configuration matches the eth1 interface\nAssigns it the IP 10.0.1.1 within a /24 subnet (255.255.255.0)\nEnables IP forwarding to allow traffic between this subnet and others\nWhen applied, this creates a subnet with 254 usable addresses (10.0.1.1 through 10.0.1.254, excluding the network address 10.0.1.0 and broadcast address 10.0.1.255).\n\n\n# /etc/systemd/network/25-subnet.network\n[Match]\nName=eth1\n\n[Network]\nAddress=10.0.1.1/24\nIPForward=yes\n\nScenario 2: Multiple Subnets on a Single Interface\n\nThis configuration creates three separate subnets accessed through the same physical interface\n\nA /24 subnet (256 addresses) in the 192.168.1.x range\nA /24 subnet (256 addresses) in the 10.10.10.x range\nA /16 subnet (65,536 addresses) in the 172.16.x.x range\n\n\n\nThe system serves as a router/gateway for all three networks simultaneously.\n# /etc/systemd/network/30-multi-subnet.network\n[Match]\nName=eth0\n\n[Network]\nAddress=192.168.1.10/24\nAddress=10.10.10.1/24\nAddress=172.16.1.1/16\n\nDHCP Server Configuration for Subnets\n\nThis configuration creates a subnet (10.0.1.0/24) on eth1\nEnables a DHCP server\nAllocates IPs from 10.0.1.11 (base + offset of 10) through 10.0.1.210 (for 200 addresses)\nProvides DNS server information to DHCP clients\n\n\n# /etc/systemd/network/25-dhcp-server.network\n[Match]\nName=eth1\n\n[Network]\nAddress=10.0.1.1/24\nIPForward=yes\nDHCPServer=yes\n\n[DHCPServer]\nPoolOffset=10\nPoolSize=200\nEmitDNS=yes\nDNS=8.8.8.8\nAlthough more complicated than simple networking, subnetting can enhance security when configured properly. It improves isolation by putting separate sensitive services onto different subnets, segmentation by limiting broadcast domains to reduce the potential attack surface, and access control by implementing filters between subnets at the router level. You can see an example of a security-enhanced subnet configuration below, as well as a list of commands to troubleshoot your subnet with.\n# /etc/systemd/network/25-secure-subnet.network\n[Match]\nName=eth2\n\n[Network]\nAddress=10.0.3.1/24\nIPForward=yes\nIPMasquerade=yes  # NAT for outgoing connections\nConfigureWithoutCarrier=yes\n\n[DHCPServer]\nPoolOffset=50\nPoolSize=100\nEmitDNS=yes\nDNS=1.1.1.1\n\n# Restrict routes between subnets for this segment\n[Route]\nGateway=_ipv4gateway\nDestination=0.0.0.0/0\n# Check interface configuration\nip addr show\n\n# Verify routing tables\nip route show\nip route show table 200  # For custom route tables\n\n# Test connectivity between subnets\nping 10.0.1.1  # From another subnet\n\n# View ARP table to verify proxy ARP functionality\nip neigh show\n\n# Check systemd-networkd logs for issues\njournalctl -u systemd-networkd -n 50",
    "crumbs": [
      "Guides",
      "Linux Server & Raspberry Pi"
    ]
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-ssh",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-ssh",
    "title": "Setting Up A Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "SSH",
    "text": "SSH\nNow that you have your basic Ubuntu Pi server configured and connected to a network, it‚Äôs time to do some final configurations before beginning the more coding-focused development. For the coding side of things, we‚Äôll want to remotely connect using a different computer and connection than the wired keyboard/monitor with our Raspberry Pi. To do this, we‚Äôll utilize VS Code, an open source IDE (integrated development environment) from Microsoft. Before that, we‚Äôll need to set up and configure SSH (Secure Shell), one of the most common ways to connect to a remote server. Simply put, SSH is a network protocol that creates an encrypted tunnel between computers, allowing secure remote management. Think of it as establishing a private, secure telephone line that only authorized parties can use to communicate.\nOnce we have SSH set up, configured, and secured, we‚Äôll use a feature in VS Code called Remote - SSH which lets us use the nice UI of an IDE while working on the actual server. This is really beneficial for a variety of reasons: one of them being the fantastic community-built extensions that drastically improve the development experience, another being the integration with other tools for things like CI/CD.\n\nKey Terms\nSSH Basic Concepts:\n\nSSH (Secure Shell): A cryptographic network protocol for secure communication between computers over an unsecured network.\nSSH Server: The computer or service that accepts incoming SSH connections.\nSSH Client: The application used to initiate connections to SSH servers.\nsshd: The SSH server daemon that listens for and handles SSH connections.\nssh_config: The client-side configuration file that controls outgoing SSH connections.\nsshd_config: The server-side configuration file that controls incoming SSH connections.\nHost Key: A cryptographic key that identifies an SSH server.\n\nAuthentication Methods:\n\nPassword Authentication: Authentication using a traditional username and password.\nPublic Key Authentication: Authentication using asymmetric cryptographic key pairs.\nPrivate Key: The secret half of a key pair that should never be shared.\nPublic Key: The shareable half of a key pair that can be distributed to servers.\nPassphrase: An optional password that encrypts and protects a private SSH key.\nauthorized_keys: A file containing public keys that are allowed to authenticate to an SSH server.\nknown_hosts: A file on the client side that stores server host keys to verify server identity.\n\nSSH Key Types and Security:\n\nRSA (Rivest-Shamir-Adleman): A widely used public-key cryptosystem for secure data transmission.\nECDSA (Elliptic Curve Digital Signature Algorithm): A cryptographic algorithm offering good security with shorter key lengths.\nEd25519: A modern, secure, and efficient public-key signature system.\nKey Length: The size of a cryptographic key, typically measured in bits.\nFingerprint: A short sequence used to identify a longer public key.\nSSH Agent: A program that holds private keys in memory to avoid repeatedly typing passphrases.\n\nSSH Security and Tools:\n\nPort Forwarding: The ability to tunnel network connections through an SSH connection.\nSSH Tunnel: An encrypted network connection established through SSH.\nSCP (Secure Copy Protocol): A means of securely transferring files between hosts based on SSH.\nSFTP (SSH File Transfer Protocol): A secure file transfer protocol that operates over SSH.\nUFW (Uncomplicated Firewall): A simplified firewall management interface for iptables.\nFail2Ban: An intrusion prevention software that protects servers from brute-force attacks.\n\n\n\nSSH Basics\n\nSSH Client vs Server Configuration\nThe SSH system uses two main configuration files with distinct purposes:\n\nssh_config:\n\nLives on your client machine (like your laptop)\nControls how your system behaves when connecting to other SSH servers\nAffects outgoing SSH connections\nLocated at /etc/ssh/ssh_config (system-wide) and ~/.ssh/config (user-specific)\n\nIf your server ever moves or connects to a new IP address, simply update it in the user config file\n\n\nsshd_config:\n\nLives on your server (the Raspberry Pi)\nControls how your SSH server accepts incoming connections\nDetermines who can connect and how\nLocated at /etc/ssh/sshd_config\nRequires root privileges to modify\nChanges require restarting the SSH service\n\n\n\n\n\nKey-Based Authentication Setup\n\nUnderstanding SSH Keys and Security\nThis guide uses ECDSA-384 keys, which offer several advantages:\n\nUses the NIST P-384 curve, providing security equivalent to 192-bit symmetric encryption\nBetter resistance to potential quantum computing attacks compared to smaller key sizes\nStandardized under FIPS 186-4\nExcellent balance between security and performance\n\n\n\nGenerating Your SSH Keys\nYou might remember from the beginning of this guide that you can generate an SSH key-pair when flashing the image using RPi Imager. Even if you did that, PasswordAuthentication will still be enabled in the server‚Äôs /etc/ssh/sshd_config. That being said, if you didn‚Äôt do that and want to learn how you can handle this all the old fashioned way, then on your laptop, generate a new SSH key pair:\n# Generate a new SSH key pair using ECDSA-384\nssh-keygen -t ecdsa -b 384 -C \"ubuntu-pi-server\"\nThis command:\n\n-t ecdsa: Specifies the ECDSA algorithm\n-b 384: Sets the key size to 384 bits\n-C \"ubuntu-pi-server\": Adds a descriptive comment\n\nThe command generates two files:\n\n~/.ssh/ubuntu_pi_ecdsa: Your private key (keep this secret!)\n~/.ssh/ubuntu_pi_ecdsa.pub: Your public key (safe to share)\n\n\n\nInstalling Your Public Key on the Raspberry Pi\nTransferring your public key to your Raspberry Pi is easy, just know the following will only work if you currently have password authentication enabled.\nssh-copy-id -i ~/.ssh/ubuntu_pi_ecdsa.pub chris@ubuntu-pi-server\nThis command:\n\nConnects to your Pi using password authentication\n\nIf you‚Äôre restoring your config, you‚Äôll need to temporarily set PasswordAuthentication in /etc/ssh/sshd_config to yes\n\nCreates the .ssh directory if needed\nAdds your public key to authorized_keys\nSets appropriate permissions automatically\n\n\n\n\nServer-Side SSH Configuration\nA client-server relationship is a fundamental computing model that underpins most network communications and distributed systems. This architecture divides computing responsibilities between service requestors (clients, your laptop in this case) and service providers (servers, the Raspberry Pi in this case).\nA server is a computer program or device that provides functionality, resources, or services to multiple clients. The Raspberry Pi in this case.\n\nService Provider: Responds to client requests rather than initiating communication\nResource Management: Manages shared resources (files, databases, computational power)\nContinuous Operation: Typically runs continuously, waiting for client requests\nScalability: Often designed to handle multiple concurrent client connections\nExamples: Web servers, database servers, file servers, mail servers, authentication servers\n\nClient-server communication follows a request-response pattern:\n\nConnection: The client establishes a connection to the server\nRequest: The client sends a formatted request for a specific service\nProcessing: The server processes the request according to its business logic\nResponse: The server returns appropriate data or status information\nDisconnection or Persistence: The connection may be terminated or maintained for future requests\n\nThis communication typically occurs over TCP/IP networks using standardized protocols that define the format and sequence of messages exchanged.\n\nUnderstanding Server Host Keys\nYour Pi‚Äôs /etc/ssh and /home/chris/.ssh directories contain several important files:\n\nAuthorized keys (in /home/chris/.ssh/authorized_keys)\nHost key pairs (public and private) for different algorithms (in /etc/ssh)\nConfiguration files and directories\nThe moduli file for key exchange\n\n\n\n\nClient-Side Configuration\nA client is a computer program or device that requests services, resources, or information from a server.\n\nRequest Initiator: Clients always initiate communication with servers\nUser Interface: Often provides the interface through which users interact with remote services\nLimited Resources: Typically has fewer resources than servers and offloads intensive processing\nDependency: Relies on servers to fulfill requests and cannot function independently for networked operations\nExamples: Web browsers, email clients, SSH clients, mobile applications\n\nFrom a technical perspective, clients:\n\nFormulate and send requests using specific protocols (HTTP, FTP, SMTP, etc.)\nWait for and process server responses\nPresent results to users or use them for further operations\n\n\nSetting Up Your SSH Config\nCreate or edit ~/.ssh/config on your laptop:\nHost ubuntu-pi-server\n    HostName ubuntu-pi-server\n    User chris\n    IdentityFile ~/.ssh/ubuntu_pi_ecdsa\n    Port 45000\n\n\n\n\n\n\nSSH Config: Include\n\n\n\nIf your ssh isn‚Äôt picking up on the ~/.ssh/ssh_config then you might need to specify it in the system config. Find the line in /etc/ssh/ssh_config that says Include and add the absolute file path. If you need to include more than your user specific config, such as the default /etc/ssh/ssh_config.d/* just add that absolute path separated by a space from any other path included.\n\n\n\n\nManaging Known Hosts\n\nBack up your current known_hosts file:\n\ncp ~/.ssh/known_hosts ~/.ssh/known_hosts.backup\n\nView current entries:\n\nssh-keygen -l -f ~/.ssh/known_hosts\n\nRemove old entries:\n\n# Remove specific host\nssh-keygen -R ubuntu-pi-server\n\nHash your known_hosts file for security:\n\nssh-keygen -H -f ~/.ssh/known_hosts\n\n\nSecuring the Key File\nWhen using SSH key-based authentication, adding a password to your key enhances security by requiring a passphrase to use the key. This guide explains how to add and remove a password from an existing SSH key.\nAdding a Password to an SSH Key\nIf you already have an SSH key and want to add a password to it, use the following command:\nssh-keygen -p -f ~/.ssh/id_rsa\nExplanation:\n\n-p: Prompts for changing the passphrase.\n-f ~/.ssh/id_rsa: Specifies the key file to modify (adjust if your key has a different name). You will be asked for the current passphrase (leave blank if none) and then set a new passphrase.\n\nRemoving a Password from an SSH Key\nIf you want to remove the passphrase from an SSH key, run:\nssh-keygen -p -f ~/.ssh/id_rsa -N \"\"\nExplanation:\n\n-N \"\": Sets an empty passphrase (removes the password).\nThe tool will ask for the current passphrase before removing it.\n\nVerifying the Changes\nAfter modifying the key, test the SSH connection from your CLI, or using an SSH tunnel.\nssh -i ~/.ssh/id_rsa user@your-server\nIf you added a passphrase, you‚Äôll be prompted to enter it when connecting.\nBy using a passphrase, your SSH key is protected against unauthorized use in case it gets compromised. If you frequently use your SSH key, consider using an SSH agent (ssh-agent) to cache your passphrase securely.\n\n\n\nImplementing SSH Security Measures\nSecuring SSH is critical because it serves as the primary gateway for remote server management, making it a prime target for attackers. A compromised SSH connection can lead to unauthorized access, data breaches, privilege escalation, and complete system takeover. By implementing robust SSH security measures like key-based authentication, non-standard ports, and intrusion prevention systems, you significantly reduce your attack surface while maintaining convenient remote access. Proper SSH hardening also helps meet compliance requirements for many industries while providing detailed audit logs for security monitoring. For a 24/7 self-hosted server exposed to the internet, optimized SSH security isn‚Äôt optional‚Äîit‚Äôs essential for protecting your system and the data it contains.\n\nBack up the original configuration:\n\nsudo cp /etc/ssh/sshd_config /etc/ssh/sshd_config.backup-$(date +%Y%m%d)\n\nOptimize host key settings in sshd_config:\n\n# Specify host key order (prioritize ECDSA)\nHostKey /etc/ssh/ssh_host_ecdsa_key\nHostKey /etc/ssh/ssh_host_ed25519_key\nHostKey /etc/ssh/ssh_host_rsa_key\n\nStrengthen the moduli file:\n\n# Back up the existing file\nsudo cp /etc/ssh/moduli /etc/ssh/moduli.backup\n\n# Remove moduli less than 3072 bits\nsudo awk '$5 &gt;= 3072' /etc/ssh/moduli &gt; /tmp/moduli\nsudo mv /tmp/moduli /etc/ssh/moduli\n\nApply changes:\n\n# Test the configuration\nsudo sshd -t\n\n# Restart the SSH service (on Ubuntu Server)\nsudo systemctl restart ssh\n\n# Verify the service status\nsudo systemctl status ssh\nJust note, you‚Äôll probably need to reboot (sudo reboot) your server before all of the changes fully take place. Once you‚Äôve done that, you may need to run sudo systemctl start ssh.\n\nFirewall Configuration with ufw\nA firewall acts as a barrier between your server and potentially hostile networks by controlling incoming and outgoing traffic based on predetermined rules. UFW provides a user-friendly interface to the underlying iptables firewall system in Linux.\n\nDefault Deny Policy: Start with blocking all connections and only allow specific permitted traffic\nStateful Inspection: Track the state of active connections rather than just examining individual packets\nPort Control: Allow or block access based on specific network ports\nSource Filtering: Control traffic based on originating IP addresses or networks\n\n# Install UFW (if it isn't already)\nsudo apt install ufw\n\n# Allow SSH connections\nsudo ufw allow ssh\n\n# Enable the firewall\nsudo ufw enable\nBehind the scenes, UFW translates these simple commands into complex iptables rules, making firewall management accessible without sacrificing security. The underlying iptables system uses a chain-based architecture to process packets through INPUT, OUTPUT, and FORWARD chains. Now, you‚Äôll want to add rules for example, allowing traffic on a specific port if you took the step to choose a nonstandard, one that isn‚Äôt the default Port 22, for this guide, I‚Äôm choosing 45000.\n# Add a new rule in the port/protocol format\nsudo ufw add 45000/tcp\n\n# Allow traffic between subnets 10.0.1.0/24 and 10.0.2.0/24\nsudo ufw allow from 10.0.1.0/24 to 10.0.2.0/24\n\n# See a list of all rules\nsudo ufw status numbered\n\n# Remove the default rules\nsudo ufw delete 1\n\n\nFail2Ban\nFail2Ban is a security tool designed to protect servers from brute force attacks. It works by monitoring log files for specified patterns, identifying suspicious activity (like multiple failed login attempts), and banning the offending IP addresses using firewall rules for a set period. It‚Äôs especially useful for securing SSH, FTP, and web services.\nThe best part is the project is entirely open source, you can view the source code and contribute here.\n# Install Fail2Ban\nsudo apt update\nsudo apt install fail2ban\n\n# Start and enable Fail2Ban\nsudo systemctl start fail2ban\nsudo systemctl enable fail2ban\n\n# Check the status of all jails\nsudo fail2ban-client status\n\n# Check the status of a specific jail\nsudo fail2ban-client status sshd\n\n# View banned IPs\nsudo iptables -L -n | grep f2b\nI want to add that fail2ban automatically pulls values for its jails depending on how you‚Äôve configured things on your system, at least I assume so. I‚Äôm assuming that because I never configured specific ssh rules for fail2ban, but it knows to allow the port I set in my sshd_config. That being said, you can see how simple it was to setup these tools, and how they work together to create a comprehensive security system:\n\nUFW establishes the baseline by controlling which ports are accessible\nFail2Ban adds behavioral analysis by monitoring authentication attempts\nTogether they provide both static and dynamic protection\n\nThis layered approach follows the defense-in-depth principle essential to modern cybersecurity. By combining a properly configured firewall with an intrusion prevention system, you significantly reduce the attack surface of your Ubuntu Pi Server.\n\n\nRegular Security Checks\n\nMonitor SSH login attempts:\n\nsudo journalctl -u ssh\n\nCheck authentication logs:\n\nsudo tail -f /var/log/auth.log\n\n\n\nSCP (Secure Copy Protocol) and rsync (Remote Sync)\nThis section outlines the process of securely copying files between your Ubuntu Pi Server and your Client machine. I‚Äôll cover two powerful methods: SCP (Secure Copy Protocol) and rsync. Both tools operate over SSH, ensuring your file transfers remain encrypted and secure.\nSCP is a simple file transfer utility built on SSH that allows you to copy files between computers. It‚Äôs straightforward for basic transfers but lacks advanced features for large or frequent transfers.\nrsync is a more sophisticated file synchronization and transfer utility that offers several advantages over SCP:\n\nIncremental transfers: Only sends parts of files that changed\nResume capability: Can continue interrupted transfers\nBandwidth control: Can limit how much network it uses\nPreservation options: Maintains file timestamps, permissions, etc.\nDirectory synchronization: Can mirror directory structures\nExclusion patterns: Can skip specified files/directories\n\n\nEnsuring Your SSH Configuration Works\nBefore attempting file transfers, verify your SSH connection is properly configured:\nssh -F ~/.ssh/config chris@ubuntu-pi-server\nThis command explicitly specifies the user configuration file location with the -F flag.\nNote: To ensure SSH always uses your user-specific config:\n\nSet proper permissions on your config file:\n\nchmod 600 ~/.ssh/config\n\nUpdate the system-wide SSH config to include your user config:\n\nsudo nano /etc/ssh/ssh_config\nAdd this line:\nInclude ~/.ssh/config\nAfter applying these changes, you should be able to connect using the simplified command:\nssh ubuntu-pi-server\n\n\nCopying Individual Files from Server to Client\nThe basic syntax for copying files from your server to your local machine is shown below. Know that in all code examples in this section, you should run it in a terminal on your client/local machine:\nscp ubuntu-pi-server:~/configs/wpa_supplicant-wlan0.conf ~/Documents/raspberry_pi_server/configs\nscp ubuntu-pi-server:~/configs/25-wireless.network ~/Documents/raspberry_pi_server/configs\nEach command performs the following actions:\n\nscp: Invokes the secure copy program\nubuntu-pi-server:~/configs/wpa_supplicant-wlan0.conf: Specifies the source file on the remote server\n~/Documents/raspberry_pi_server/25-wireless.network: Specifies the destination directory on your local machine\n\n\n\nCopying Multiple Files at Once\nTo copy all Bash scripts from a directory in one command:\nscp chris@ubuntu-pi-server:~/configs/*.sh ~/Documents/raspberry_pi_server/configs\nThe wildcard pattern *.sh tells SCP to match all files with the .sh extension. Here, I‚Äôve included the username chris@ explicitly, which can help resolve connection issues if your SSH config isn‚Äôt being properly recognized.\n\n\nRecursively Copying Directories\nTo copy entire directories with their contents:\nscp -r chris@ubuntu-pi-server:~/mnt/backups/ ~/Documents/raspberry_pi_server/backups/configs\nThe -r flag (recursive) tells SCP to copy directories and their contents.\n\n\nCopying Files from Client to Server\nTo send files in the opposite direction (local to remote):\nscp -r ~/Documents/pi-scripts chris@ubuntu-pi-server:~/scripts\n\n\nTransferring Files with rsync\nFor larger files or when you need to synchronize directories, rsync offers significant advantages over SCP.\nTo copy a single file from server to client:\nrsync -avz ubuntu-pi-server:~/configs/25-wireless.network ~/Documents/raspberry_pi_server/configs\nLet‚Äôs break down these common flags:\n\n-a: Archive mode, preserves permissions, timestamps, etc. (shorthand for -rlptgoD)\n-v: Verbose, shows detailed progress\n-z: Compresses data during transfer, saving bandwidth\n\n\n\nSyncing Directories with rsync\nTo sync an entire directory from server to client:\nrsync -avz --progress ubuntu-pi-server:~/configs/ ~/Documents/raspberry_pi_server/configs\nThe --progress flag shows a progress bar for each file transfer, which is particularly useful for large files.\nImportant Note: The trailing slash on the source path (~/configs/) is significant\n\nWith trailing slash: Copies the contents of the directory\nWithout trailing slash: Copies the directory itself and its contents\n\n\n\nSyncing in Reverse (Client to Server)\nTo sync files from your client to the server:\nrsync -avz --progress ~/Documents/raspberry_pi_server/configs chris@ubuntu-pi-server:~/configs/\n\n\nUsing rsync with Dry Run\nBefore performing large transfers, you can see what would happen without actually making changes:\nrsync -avzn --progress ~/Documents/raspberry_pi_server/configs chris@ubuntu-pi-server:~/configs/\nThe -n flag (or --dry-run) simulates the transfer without changing any files, letting you verify what would happen.\n\n\nIncremental Backups with rsync\nrsync excels at keeping directories in sync over time. After the initial transfer, subsequent runs only transfer what‚Äôs changed:\nrsync -avz --delete ~/Documents/raspberry_pi_server/configs chris@ubuntu-pi-server:~/configs/\nThe --delete flag removes files from the destination that no longer exist in the source, creating a perfect mirror. Use with caution!\n\n\nAdvanced rsync Examples\n\n\nCustom SSH Parameters\nTo specify specific ssh paramters, such as key file or port:\nrsync -avz --progress -e 'ssh -p 45000 -i ~/.ssh/ubuntu_pi_ecdsa'  chris@192.168.1.151:/mnt/backups/configs/master backups/configs/\nThe e flag tells rsync to execute ssh with those specific flags, when it initiates the connection.\n\n\nExcluding Files or Directories\nTo skip certain files or directories during transfer:\nrsync -avz --exclude=\"*.tmp\" --exclude=\"node_modules\" ~/Documents/raspberry_pi_server/configs chris@ubuntu-pi-server:~/configs/\nThis command excludes all .tmp files and the node_modules directory.\n\n\nSetting Bandwidth Limits\nIf you need to limit how much network bandwidth rsync uses:\nrsync -avz --bwlimit=1000 ~/Documents/raspberry_pi_server/configs chris@ubuntu-pi-server:~/configs/\nThe --bwlimit=1000 restricts transfer speed to 1000 KB/s (approximately 1 MB/s).\n\n\nPreserving Hard Links\nWhen backing up systems that use hard links (like Time Machine or some backup solutions):\nrsync -avH ~/Documents/raspberry_pi_server/configs chris@ubuntu-pi-server:~/configs/\nThe -H flag preserves hard links, which can save significant space in backups.\n\n\nChoosing Between SCP and rsync\nUse SCP when: - You need a quick, one-time file transfer - You want a simple command with minimal options - The files are small and not changing frequently\nUse rsync when: - You need to synchronize directories - You‚Äôre transferring large files that might get interrupted - You want to maintain exact mirrors of directory structures - You‚Äôre setting up automated backups - You need to preserve file attributes like permissions and timestamps - You need to exclude certain files or patterns\n\nSSH Configuration: Ensure your SSH config is properly set up before attempting file transfers\nSCP: Simple, straightforward file copying between systems\nrsync: More powerful synchronization tool with many options for efficiency\n\nSSH is now correctly configured and working using ssh ubuntu-pi-server.\nBash scripts can be securely copied from the Ubuntu Pi Server to the client machine using scp.\n\nJust take note of the specific syntax used, namely server-name:path/to/files\n\nThe user can now maintain local backups of important scripts efficiently.\n\nEnables you to develop where you‚Äôd like and then easily move files to test scripts\n\n\nTrailing Slashes: Pay attention to trailing slashes in paths, as they change behavior\nDry Run: Use --dry-run with rsync to preview what will happen\nAutomation: Consider creating scripts for routine backup tasks\n\nBoth SCP and rsync are invaluable tools for managing files on your Raspberry Pi server. While SCP is perfect for quick, simple transfers, rsync provides the power and flexibility needed for maintaining backups and keeping systems synchronized.",
    "crumbs": [
      "Guides",
      "Linux Server & Raspberry Pi"
    ]
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-remote_vs_code",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-remote_vs_code",
    "title": "Setting Up A Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Remote Development with VS Code",
    "text": "Remote Development with VS Code\n\nKey Terms\nRemote Development Concepts:\n\nIDE (Integrated Development Environment): A software application that provides comprehensive facilities for software development.\nRemote Development: Developing code on a remote machine while using local tools and interfaces.\nHeadless Development: Writing code on a system without a graphical interface, often remotely.\nExtension: Add-ons that enhance the functionality of development tools.\nWorkspace: A collection of files and folders that make up a development project.\nSync: The process of keeping files consistent between local and remote systems.\nPort Forwarding: Redirecting communication from one network port to another.\nDevelopment Container: A container configured specifically for development purposes.\n\nVS Code Specific Terminology:\n\nVS Code: Visual Studio Code, a code editor developed by Microsoft.\nRemote - SSH Extension: A VS Code extension that allows connecting to and developing on remote machines over SSH.\nRemote Explorer: A VS Code interface for managing remote connections.\nSSH Target: A remote machine configured for SSH access in VS Code.\nSSH Config: Configuration file defining SSH connection properties.\nDev Container: A containerized development environment defined for a VS Code project.\nWorkspace Settings: Project-specific configurations in VS Code.\nSettings Sync: A feature that synchronizes VS Code settings across different instances.\nTask: Configured commands that can be executed within VS Code.\nLaunch Configuration: Settings that define how to debug applications in VS Code.\n\n\n\nVS Code and IDE-based Development\nRemote development with Visual Studio Code transforms your Raspberry Pi server from a command-line environment into a fully-featured development platform. While terminal-based SSH access provides basic server management capabilities, VS Code Remote SSH creates a seamless bridge between your local machine‚Äôs powerful editor interface and your remote server‚Äôs computing resources. This approach combines the convenience of a graphical editor with syntax highlighting, intelligent code completion, and integrated debugging while executing code directly on your Raspberry Pi server‚Äîperfect for resource-intensive tasks or when working with server-specific configurations.\nThe VS Code Remote SSH extension uses your existing SSH configurations to establish a secure connection to your server. Once connected, the extension installs a lightweight VS Code server component on your Raspberry Pi, enabling real-time file editing, terminal access, and extension functionality‚Äîall while maintaining the security of your SSH connection. This method eliminates the need to constantly transfer files between local and remote systems, making development significantly more efficient.\n\nInstalling VS Code and the Remote - SSH Extension\nFirst, you‚Äôll need to install VS Code on your local machine (MacBook Air for me):\n\nDownload the appropriate version for your operating system from VS Code‚Äôs official website\nInstall VS Code following the standard installation process for your operating system\nLaunch VS Code\n\nNext, install the Remote SSH extension by selecting the VS Code extensions tab and then search for Remote - SSH.\n\nAfter that, install the extension.\n\nNow, it‚Äôs time to actually connect remotely. We‚Äôll do this using VS Code‚Äôs command palette. The MacBook shortcut is Cmd+Shift+P. You‚Äôll see that you can also open and edit your ssh_config in VS Code, using the command palette to select your configuration. Then, you‚Äôll essentially interact with your server the same way you would a client-locally developed application in VS Code. First, select the folder you want to open, for the time being, I‚Äôm working out of my home directory /home/chris. Then, you can open up one of your backup scripts, notice how much nicer the UI is when dealing with bash scripts. Finally, you can also still run commands from the command line (obviously, otherwise it‚Äôd be hard to use a headless server), notice here I was copying some of the configuration files we‚Äôve modified to my home. I do that so I can back those up to my laptop.\n\nSelect the Connect to Host... option.\n\nSelect the host you want to connect to, in my case ubuntu-pi-server.\n\nYou‚Äôll see this along the bottom of your window, while VS Code connects to your remote server.\n\nThe first time you connect, VS Code will:\n\nInstall the VS Code server component on your Raspberry Pi\nCreate a .vscode-server directory in your home folder\nEstablish a secure connection\nLoad the remote workspace\n\nIf you set up SSH key authentication as described earlier in the guide, the connection should establish without requiring a password. If you‚Äôre using a passphrase-protected SSH key, you‚Äôll be prompted to enter it. Then, Select the folder option, same as you would locally with VS Code, and then select what you would like to work out of.\n\nAfter connecting successfully, the remote connection is indicated in the bottom-left corner of the VS Code window where you‚Äôll see ‚ÄúSSH: ubuntu-pi-server‚Äù. VS Code will operate as if you were working directly on the Raspberry Pi. Open a file by clicking on it in the file explorer, or type code filename.sh into your terminal CLI to open a file. You can open a new terminal by going to the command palette and selecting Terminal: Create New Terminal or, on MacOS, use Ctrl+backtick. Note: I haven‚Äôt been able to use this to edit files that require root privileges just yet. It seems like my issue is with how/where the code command is stored, and how sudo tries to interpret it.\nThe explorer view will now display the file system of your Raspberry Pi, not your local machine. Any file operations (create, edit, delete) will happen directly on the remote server.\n\nTo install an extension that should run on the remote server (like language servers, debugging tools, etc.):\n\nClick on the Extensions icon in the activity bar\nFind the extension you want to install\nClick the ‚ÄúInstall in SSH: ubuntu-pi-server‚Äù button instead of the regular ‚ÄúInstall‚Äù button\n\nExtensions are categorized into:\n\nUI Extensions: Run on your local machine (themes, UI enhancements)\nWorkspace Extensions: Run on the remote server (language servers, debuggers)\n\nThis separation ensures optimal performance while providing a complete development experience.\n\nFinally, remember to close your connection, although closing the window does the same.\nUsing VS Code with Remote SSH fundamentally transforms your development experience with the Raspberry Pi in several ways:\n\nEdit with confidence: No more struggling with terminal-based editors like nano or vim when complex editing is needed\nSeamless navigation: Easily browse the server‚Äôs filesystem with familiar GUI tools\nIntegrated tools: Git integration, debugging, and terminal access all in one environment\nExtension ecosystem: Leverage thousands of VS Code extensions while working on your remote project\nProductive workflow: Maintain your preferred development environment regardless of the target platform",
    "crumbs": [
      "Guides",
      "Linux Server & Raspberry Pi"
    ]
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-partitions",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-partitions",
    "title": "Setting Up A Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Partitions",
    "text": "Partitions\n\nKey Terms\nPartition Basics:\n\nPartition: A logical division of a physical storage device.\nDisk: A physical storage device (HDD, SSD, etc.).\nPartition Table: A data structure on a disk that describes how the disk is divided.\nMBR (Master Boot Record): A traditional partition scheme limited to 2TB drives and 4 primary partitions.\nGPT (GUID Partition Table): A modern partition scheme supporting larger drives and more partitions.\nPrimary Partition: A partition that can be bootable and hold an operating system.\nExtended Partition: A special type of partition that acts as a container for logical partitions (MBR only).\nLogical Partition: A partition created within an extended partition (MBR only).\nBoot Partition: A partition containing files needed to start the operating system.\nRoot Partition: The primary partition containing the operating system and most files.\n\nFilesystem Types:\n\nFilesystem: The method used to organize and store data within a partition.\next4: The fourth extended filesystem, a journaling filesystem commonly used in Linux.\nFAT32: File Allocation Table 32-bit, a simple filesystem compatible with most operating systems.\nexFAT: Extended File Allocation Table, designed for flash drives with support for larger files than FAT32.\nNTFS: New Technology File System, primarily used by Windows.\nBtrfs: B-tree File System, a modern Linux filesystem with advanced features like snapshots.\nJournaling: A technique that maintains a record of filesystem changes before committing them.\nMounting: The process of making a filesystem accessible through the file hierarchy.\nMount Point: A directory where a filesystem is attached to the system‚Äôs file hierarchy.\n\nPartitioning Tools:\n\nfdisk: A traditional command-line utility for disk partitioning.\nparted: A more powerful partitioning tool with support for larger drives and GPT.\ngdisk: A GPT-focused partitioning utility.\nsfdisk: A scriptable version of fdisk for automation.\ngparted: A graphical partition editor for Linux.\nmkfs: Command used to create a filesystem on a partition.\nfsck: Filesystem consistency check and repair tool.\nblkid: Command that displays attributes of block devices like UUID.\nlsblk: Command that lists information about block devices.\nfstab: System configuration file that defines how filesystems are mounted.\n\n\n\nPartitioning Basics\nPartitions are logical divisions of a physical storage device. Think of a storage device like a large piece of land, and partitions as fenced areas within that land dedicated to different purposes. Each partition appears to the operating system as a separate disk, even though physically they‚Äôre on the same device. Remember from the beginning of this guide, I‚Äôm currently using a Flash Drive for my primary memory and a microSD card for backups; however, the SSD is what I want to serve as the boot device. Once we complete the partitioning, we can flash the base image from RPi onto the SSD and then reboot, with the SSD as the boot device.\n\nSeparation of concerns: Isolate the operating system from user data, which improves security and simplifies backups\nPerformance optimization: Different filesystems can be used for different workloads\nMulti-boot capability: Install multiple operating systems on the same physical device\nData protection: Limiting the scope of filesystem corruption to a single partition\nResource management: Setting size limits for specific system functions\n\nFor our Raspberry Pi server, proper partitioning creates a solid foundation for everything else you‚Äôll build. We‚Äôll primarily use ext4 for Linux partitions and FAT32 for the microSD card that needs broader compatibility.\n\n\n\n\n\n\n\n\nFilesystem\nBest For\nFeatures\n\n\n\n\next4\nLinux\n\nJournaling\nLarge file support\nBackwards compatible\n\n\n\nFAT32\nCross-platform compatibility\n\nWorks with virtually all operating systems\nLimited to 4GB Files\n\n\n\nexFAT\nModern cross-platform\n\nSupports large files\nNo built-in journaling\n\n\n\nNTFS\nWindows compatibility\n\nJournaling\nPermissions\nCompression\n\n\n\nBtrfs\nAdvanced Linux systems\n\nSnapshots\nChecksums\nCompression\n\n\n\n\nFinally, let‚Äôs cover some important terms:\n\nPartition Table: A data structure on a disk that describes how the disk is divided\n\nMBR (Master Boot Record): Traditional partition scheme limited to 2TB drives and 4 primary partitions\nGPT (GUID Partition Table): Modern scheme supporting larger drives and more partitions\n\nPartition Types:\n\nPrimary: Can be bootable and hold an operating system\nExtended: Acts as a container for logical partitions (MBR only)\nLogical: Created within an extended partition (MBR only)\n\nFilesystem: The method used to organize and store data within a partition\n\nCommon Linux filesystems: ext4, Btrfs\nCross-platform filesystems: FAT32, exFAT\n\n\n\n\nPartitioning Tools\nSeveral command-line tools are available for disk partitioning on Linux. Each has strengths for different scenarios:\n\n\n\n\n\n\n\n\n\nTool\nStrengths\nLimitations\nBest For\n\n\nfdisk\n\nSimple interface\nWidely available\n\n\nLimited GPT support in older versions\n\n\nBasic partitioning tasks\n\n\n\nparted\n\nFull GPT support\nHandles large drives\n\n\nMore complex syntax\n\n\nAdvanced partitioning needs\n\n\n\ngdisk\n\nGPT focused\nSimilar to fdisk\n\n\nLess common on minimal installations\n\n\nGPT-specific operations\n\n\n\nsfdisk\n\nScriptable for automation\n\n\nLess user-friendly\n\n\nAutomated deployments\n\n\n\n\nFor this project, and after doing some research, I chose parted for both the microSD card and SSD partitioning because:\n\nIt fully supports both MBR and GPT partition tables\nIt can handle drives larger than 2TB (relevant for the SSD)\nIt provides a more consistent interface across different partition table types\nIt supports both interactive and command-line usage\nIt‚Äôs included in most Ubuntu installations\n\n\n\nPartitioning a MicroSD Card for Backups\nLet‚Äôs partition our microSD card to serve as backup media. You can get great quality cards from Amazon Basics that are perfect for this use case. We‚Äôll use a simple, effective partition scheme. Before we dive into the actual commands, it‚Äôs important to remember that you can‚Äôt modify the memory of the active primary drive. Meaning, that you‚Äôll need to use an SSD or thumb drive as the boot media while you modify the SD card. Similarly, you‚Äôll need to use a different piece of boot media (you could use the micro SD) when partitioning the SSD.\nNow, let‚Äôs walk through this step-by-step:\n\nIdentify the device name of the microSD. Your microSD card will typically appear as something like /dev/mmcblk0 (what mine showed as) or /dev/sdX (where X is a letter like a, b, c). This command lists block devices with key information:\n\nsudo lsblk -o NAME,SIZE,FSTYPE,TYPE,MOUNTPOINT\nThe lsblk command lists all block devices, which includes your storage devices. The -o flag specifies which columns to display in the output. This final check ensures you‚Äôre working with the correct device and helps you confirm the partition structure you just created.\n\nNAME: Device identifier\nSIZE: Storage capacity\nFSTYPE: Current filesystem type\nTYPE: Whether it‚Äôs a disk or partition\nMOUNTPOINT: Where it‚Äôs currently mounted (if applicable)\n\n\nFor a backup microSD card, we‚Äôll use a simple partition layout with a single partition using ext4 filesystem, which provides good performance and Linux compatibility.\n\n# Start parted on the microSD card (replace /dev/mmcblk0 with your device)\nsudo parted /dev/mmcblk0\n\n# View the partition table for a specific device, or all\nprint mmcblk0\nprint all\n\n# Inside parted, create a new GPT partition table\n&gt; (parted) mklabel gpt\nWarning: The existing disk label on /dev/mmcblk0 will be destroyed and all data on this disk will be lost. Do you want to continue?\nYes/No? Yes\n\n# Create a single partition using the entire card\n&gt; (parted) mkpart primary ext4 0% 100%\n\n# Set a name for easy identification\n&gt; (parted) name 1 backups\n\n# Verify the partition layout\n&gt; (parted) print\n\n# Exit parted\n&gt; (parted) quit\n\nmklabel gpt: Creates a new GPT partition table (preferred over MBR for modern systems)\nmkpart primary ext4 0% 100%: Creates a primary partition using the ext4 filesystem that spans the entire device\nname 1 backup: Names the first partition ‚Äúbackup‚Äù for easy identification\nprint: Shows the current partition layout\nquit: Exits the parted utility\n\n\nAfter creating the partition, we need to format it with the ext4 filesystem. Double check the current layout of memory on your system with sudo lsblk -o NAME,SIZE,FSTYPE,TYPE,MOUNTPOINT before formatting the filesystem, to get the specific SD Card partitions device name:\n\n# Format the partition (adjust if your device/partition is different)\nsudo mkfs.ext4 -L backups /dev/mmcblk0p1\n\n-L backup: Sets the filesystem label to ‚Äúbackup‚Äù\n/dev/mmcblk0p1: The partition we just created (p1 indicates the first partition)\nYou‚Äôll see an output similar to this:\n\n\n\nNow, we need to prepare the SD card for backups. You can make those changes with the following commands:\n\n# Create a mount point\nsudo mkdir -p /mnt/backups\n\n# Add an entry to /etc/fstab for automatic mounting\necho \"UUID=$(sudo blkid -s UUID -o value /dev/mmcblk0p1) /mnt/backups ext4 defaults,noatime 0 2\" | sudo tee -a /etc/fstab\n\n# Restart the systemd daemon to get the changes made to fstab\nsudo systemctl daemon-reload\n\n# Mount the filesystem from fstab\nsudo mount /dev/mmcblk0\n\n# Create backup directories\nsudo mkdir -p /mnt/backups/{configs,logs}\n\n# Set ownership (replace 'chris' with your username)\nsudo chown -R chris:chris /mnt/backups/\n\n# Set secure permissions\nsudo chmod -R 700 /mnt/backups/\n\nmkdir -p: Creates directories and parent directories if they don‚Äôt exist\nblkid -s UUID -o value: Gets the UUID (unique identifier) of the partition\ndefaults,noatime: Mount options for good performance (noatime disables recording access times)\n0 2: The fifth field (0) disables dumping, the sixth field (2) enables filesystem checks\nmount -a: Mounts all filesystems specified in fstab\nchmod -R 700: Sets permissions so only the owner can read/write/execute\n\n\n\nPartitioning your SSD\nFor a Raspberry Pi server, a two-partition scheme offers the perfect balance of simplicity and functionality. This approach mirrors what RPi Imager creates automatically, but gives us control over the sizes:\n\nA small FAT32 boot partition for firmware and boot files\nA large ext4 root partition for the entire operating system and data\n\nThis simplified structure eliminates the complexity of separate swap and data partitions while maintaining full functionality. The Raspberry Pi can use swap files instead of dedicated partitions, which provides more flexibility for managing memory as your needs change.\n\nFor the Samsung T7 SSD, we‚Äôll follow a similar workflow. The Samsung T7 SSD will likely appear as /dev/sdX (where X is a letter like a, b, c), mine is /dev/sdb.\n\nsudo lsblk -o NAME,SIZE,FSTYPE,TYPE,MOUNTPOINT\n\nIn this code block, I‚Äôll show you a way to use parted without entering the interactive mode.\n\n# Create a new GPT partition table\nsudo parted /dev/sdb mklabel gpt\n\n# Create the EFI System Partition (ESP)\nsudo parted /dev/sdb mkpart boot fat32 1MiB 513MiB\nsudo parted /dev/sdb set 1 esp on\nsudo parted /dev/sdb set 1 boot on\n\n# Create the root partition\nsudo parted /dev/sdb mkpart ubuntu-root ext4 513MiB 100%\n\n# Verify the partition layout\nsudo parted /dev/sdb print\n\n/dev/sdc1: 512MB FAT32 partition for boot files.\n/dev/sdc2: Remaining space (about 931GB) ext4 partition for the entire system.\nThe set 1 boot on command marks the partition as bootable.\nThe set 1 esp on marks it as an EFI System Partition, ensuring compatibility with both legacy and UEFI boot methods.\n\n\nNow we need to format each partition.\n\n# Format the ESP partition\nsudo mkfs.fat -F32 -n BOOT /dev/sdb1\n\n# Format the root partition\nsudo mkfs.ext4 -L ubuntu-root /dev/sdb2\n\nmkfs.fat -F32: Creates a FAT32 filesystem\n-n ESP: Sets the volume label to ‚ÄúESP‚Äù\nmkfs.ext4: Creates an ext4 filesystem\n-L ubuntu-root: Sets the filesystem label\n/dev/sdb1, /dev/sdb2, etc.: The specific partitions we created\n\n\nNow, we will verify that the partitions went as we hoped\n\n# Check the partition layout\nsudo lsblk -o NAME,SIZE,FSTYPE,LABEL,TYPE,MOUNTPOINT\n\n# Verify the filesystem types and labels\nsudo blkid | grep sdc\nThis approach to partitioning offers several advantages:\n\nMatches RPi Imager default: Aligns with what users expect from standard Raspberry Pi installations\nEasier to manage: Fewer partitions mean simpler maintenance and troubleshooting\nThe Raspberry Pi firmware requires a FAT32 boot partition to find and load the kernel.\nThe 512MB size ensures plenty of space for kernel updates and multiple kernel versions if needed.\n\nNow you‚Äôre ready to flash Ubuntu Server to these properly prepared partitions! The RPi Imager will use this partition structure and write the system files to the correct locations.\n\nNow we‚Äôll need to mount the partitions by setting up mount points and telling the system to use them.\n\n# Check the partition layout\nsudo lsblk -o NAME,SIZE,FSTYPE,LABEL,TYPE,MOUNTPOINT\n\n# Verify the filesystem types and labels\nsudo blkid | grep sdc\n\n\nAdvanced Partitioning\nAs you begin to utilize your server more, you‚Äôre bound to use up more memory. So, it‚Äôs important to monitor your partition space usage.\n# View disk usage\ndf -h\n\n# View inode usage (for number of files)\ndf -i\n\n# View detailed filesystem information\nsudo tune2fs -l /dev/sda2 | grep -E 'Block count|Block size|Inode count|Inode size'\n\ndf -h: Shows disk usage in human-readable format\ndf -i: Shows inode usage (inode = index node, representing a file)\ntune2fs -l: Lists filesystem information for ext2/3/4 filesystems\ngrep -E: Filters output for specified patterns\n\nFurthermore, you may realize that you want to reformat your SSD at some point because your storage needs changed. You can reformat the partitions using the following code.\n# For online resizing of ext4 (unmounting not required)\nsudo parted /dev/sda\n(parted) resizepart 4 100%  # Resize partition 4 to use all available space\n(parted) quit\n\n# After resizing the partition, expand the filesystem\nsudo resize2fs /dev/sda4\n\nresizepart 4 100%: Resizes partition 4 to use 100% of the remaining available space\nresize2fs: Resizes an ext2/3/4 filesystem to match the partition size",
    "crumbs": [
      "Guides",
      "Linux Server & Raspberry Pi"
    ]
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-backups",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-backups",
    "title": "Setting Up A Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Backups and Basic Automation",
    "text": "Backups and Basic Automation\nNow that we‚Äôve configured the basics, from permissions to networking and ssh to partitions, we‚Äôll want to save those changes in case something happens and to ensure a seamless transition to the SSD for boot media. You‚Äôve already seen some basic backups. The process is the same, essentially creating a folder and then putting a copy of the current file into it and maybe adding a .bak extension to make it clear this is a previous version. That being said, to go through and do this for each and every folder we‚Äôve made changes in is impractical now, let alone in the future when more complex configurations are done. So, in this section, we‚Äôll go over creating a basic script to backup all of our configs and automating the backups.\nFor this section, we‚Äôll use rsync because it provides several important advantages over simple copy commands, that you may remember from the section on ssh:\n\nIncremental backups that only transfer changed files\nPreservation of file permissions, ownership, and timestamps\nBuilt-in compression for efficient transfers\nDetailed progress information and logging\nThe ability to resume interrupted transfers\n\nBefore we start, make sure you have:\n\nA mounted backup drive at /mnt/backups/\n\n\nKey Terms\nBackup Concepts:\n\nBackup: A copy of data that can be recovered if the original is lost or damaged.\nFull Backup: A complete copy of all selected data.\nIncremental Backup: A backup of only the data changed since the last backup.\nDifferential Backup: A backup of all data changed since the last full backup.\nSnapshot: A point-in-time copy of data, often using filesystem features for efficiency.\nRestoration: The process of recovering data from a backup.\nRetention Policy: Rules determining how long backups should be kept.\nBackup Rotation: A systematic approach to reusing backup media over time.\nOffsite Backup: Backups stored in a different physical location for disaster recovery.\n\nBackup Tools and Methods:\n\nrsync: A utility for efficiently copying and synchronizing files locally or remotely.\ntar: Tape Archive, a utility for collecting multiple files into a single archive file.\ndd: A low-level utility that can copy data at the block level.\ncron: A time-based job scheduler in Unix-like systems.\nanacron: A job scheduler that doesn‚Äôt require the system to be running continuously.\nsystemd timers: An alternative to cron for scheduling recurring tasks.\nArchive: A single file containing multiple files, often compressed.\nCompression: Reducing the size of data to save storage space.\nDeduplication: Eliminating duplicate copies of repeating data to save space.\nChecksums: Values calculated from file contents to verify data integrity.\n\nAutomation Concepts:\n\nScript: A file containing a series of commands to be executed.\nShell Script: A script written in a shell language like Bash.\nCrontab: A configuration file specifying scheduled tasks.\nScheduler: A system component that executes tasks at specified times.\nEnvironment Variable: A named value that can affect the behavior of running processes.\nExit Code: A value returned by a command indicating its success or failure.\nRedirection: Changing where command input comes from or output goes to.\nPipeline: Connecting multiple commands by passing the output of one as input to another.\nBackground Process: A process that runs without user interaction, often denoted by an ampersand (&).\nJob Control: Managing the execution of multiple processes from a shell.\n\n\n\nBackup Basics\nFirst, in case you didn‚Äôt do this earlier, we‚Äôll prepare the backup directory structure and set appropriate permissions:\n# Create backup directories if they don't exist\nsudo mkdir -p /mnt/backups/configs\nsudo mkdir -p /mnt/backups/system\n\n# Change ownership to your user (replace 'chris' with your username)\nsudo chown -R chris:chris /mnt/backups\n\n# Set appropriate permissions\nsudo chmod -R 700 /mnt/backups  # Only owner can read/write/execute\nWhile it‚Äôs definitely beneficial to have a local copy of your backups to easily roll back changes, it isn‚Äôt the most secure solution to have all of your information in one place. Furthermore, the SSD is partitioned, but it doesn‚Äôt currently have an OS or any files stored. So, now it‚Äôs time to take advantage of the microSD card we formatted earlier.\nFor the purpose of this guide, I‚Äôll be showing you how to use rsync for a remote transfer to your client machine and how to automatically store backups on the SD card. The script we‚Äôll use saves all of the key user and system information (things like passwords), as well as the configuration changes we made. Additionally, as long as your SD card is mounted to /mnt/backups the backup will automatically be saved to the external memory.\n\n\nConfig Backups\nThe following script demonstrates how to perform the backup while preserving all file attributes:\n#!/bin/bash\n# Using the {} around DATEYMD in the file path ensure it's specified as the variable's value, and the subsequent parts are not included\n\nDATEYMD=$(date +%Y%m%d)\nBACKUP_DIR=\"/mnt/backups/configs/$DATEYMD\"\nLOG_DIR=\"/mnt/backups/logs\"\nLOG_FILE=\"$LOG_DIR/${DATEYMD}_config_backup.log\"\n\n# Check if script is run as root\nif [ \"$(id -u)\" -ne 0 ]; then\n    echo \"Error: This script must be run as root. Try using sudo.\"\n    exit 1\nfi\n\n# Create backup directory\nmkdir -p \"$BACKUP_DIR\"\n\n{\n    # 1. User and Group Information\n    echo \"Backing up User and Group configuration...\"\n    sudo rsync -aAXv /etc/passwd \"$BACKUP_DIR/passwd.bak\"\n    sudo rsync -aAXv /etc/group \"$BACKUP_DIR/group.bak\"\n    sudo rsync -aAXv /etc/shadow \"$BACKUP_DIR/shadow.bak\"\n    sudo rsync -aAXv /etc/gshadow \"$BACKUP_DIR/gshadow.bak\"\n\n    # 2. Crontab Configurations\n    echo \"Backing up Crontab configuration...\"\n    sudo rsync -aAXv /etc/crontab \"$BACKUP_DIR/\"\n    sudo rsync -aAXv /var/spool/cron/crontabs/. \"$BACKUP_DIR/crontabs/\"\n\n     # 3. SSH Configuration\n    echo \"Backing up SSH configuration...\"\n    sudo rsync -aAXv /etc/ssh/. \"$BACKUP_DIR/ssh/\"\n    \n    # Create user_ssh directory\n    mkdir -p \"$BACKUP_DIR/user_ssh\"\n    \n    # Copy SSH user configuration with explicit handling of authorized_keys\n    rsync -aAXv /home/chris/.ssh/config \"$BACKUP_DIR/user_ssh/\" 2&gt;/dev/null || true\n    rsync -aAXv /home/chris/.ssh/id_* \"$BACKUP_DIR/user_ssh/\" 2&gt;/dev/null || true\n    rsync -aAXv /home/chris/.ssh/known_hosts \"$BACKUP_DIR/user_ssh/\" 2&gt;/dev/null || true\n    \n    # Explicitly backup authorized_keys if it exists\n    if [ -f /home/chris/.ssh/authorized_keys ]; then\n        echo \"Backing up authorized_keys file...\"\n        rsync -aAXv /home/chris/.ssh/authorized_keys \"$BACKUP_DIR/user_ssh/\"\n    else\n        echo \"No authorized_keys file found in /home/chris/.ssh/\"\n    fi\n\n    # 4. UFW (Uncomplicated Firewall) Configuration\n    echo \"Backing up ufw configuration...\"\n    sudo rsync -aAXv /etc/ufw/. \"$BACKUP_DIR/ufw/\"\n    sudo ufw status verbose &gt; \"$BACKUP_DIR/ufw_rules.txt\"\n\n    # 5. Fail2Ban Configuration\n    echo \"Backing up fail2ban configuration...\"\n    sudo rsync -aAXv /etc/fail2ban/. \"$BACKUP_DIR/fail2ban/\"\n\n    # 6. Network Configuration\n    echo \"Backing up Network configuration...\"\n    sudo rsync -aAXv /etc/network/. \"$BACKUP_DIR/network/\"\n    sudo rsync -aAXv /etc/systemd/network/. \"$BACKUP_DIR/systemd/network/\"\n    sudo rsync -aAXv /etc/netplan/. \"$BACKUP_DIR/netplan/\"\n    sudo rsync -aAXv /etc/hosts \"$BACKUP_DIR/hosts.bak\"\n    sudo rsync -aAXv /etc/hostname \"$BACKUP_DIR/hostname.bak\"\n    sudo rsync -aAXv /etc/resolv.conf \"$BACKUP_DIR/resolv.conf.bak\"\n    sudo rsync -aAXv /etc/wpa_supplicant/. \"$BACKUP_DIR/wpa_supplicant/\"\n\n    # 7. Systemd Services and Timers\n    echo \"Backing up Systemd Timers configuration...\"\n    sudo rsync -aAXv /etc/systemd/system/. \"$BACKUP_DIR/systemd/\"\n\n    # 8. Logrotate Configuration\n    echo \"Backing up Logrotate configuration...\"\n    sudo rsync -aAXv /etc/logrotate.conf \"$BACKUP_DIR/logrotate.conf.bak\"\n    sudo rsync -aAXv /etc/logrotate.d/. \"$BACKUP_DIR/logrotate.d/\"\n\n    # 9. Timezone and Locale\n    echo \"Backing up Timezone and Locale configuration...\"\n    sudo rsync -aAXv /etc/timezone \"$BACKUP_DIR/timezone.bak\"\n    sudo rsync -aAXv /etc/localtime \"$BACKUP_DIR/localtime.bak\"\n    sudo rsync -aAXv /etc/default/locale \"$BACKUP_DIR/locale.bak\"\n\n    # 10. Keyboard Configuration\n    echo \"Backing up Keyboard configuration...\"\n    sudo rsync -aAXv /etc/default/keyboard \"$BACKUP_DIR/keyboard.bak\"\n\n    # 11. Filesystem Table (fstab)\n    echo \"Backing up filesystem table (fstab)...\"\n    sudo rsync -aAXv /etc/fstab \"$BACKUP_DIR/fstab.bak\"\n    \n    # 12. Backup Package List\n    echo \"Backing up package list...\"\n    dpkg --get-selections &gt; \"$BACKUP_DIR/package_list.txt\"\n\n    # Set appropriate permissions\n    echo \"Configuring backup directory permissions...\"\n    sudo chown -R chris:chris \"$BACKUP_DIR\"\n    sudo chmod -R 600 \"$BACKUP_DIR\"\n\n    echo \"Configuration backup completed at: $BACKUP_DIR\"\n\n} &gt; \"$LOG_FILE\" 2&gt;&1\n\necho \"Logs available at: $LOG_FILE\"\n# Make the script executable\nchmod +x /scripts/config_backup.sh\n\n# Run the script\n./scripts/config_backup.sh\nThe rsync commands use several important options:\n\n-a: Archive mode, preserves almost everything\n-A: Preserve ACLs (Access Control Lists)\n-X: Preserve extended attributes\n-v: Verbose output\n--one-file-system: Don‚Äôt cross filesystem boundaries\n--hard-links: Preserve hard links\n--exclude: Skip specified directories\n\nThe package backup commands use specific flags as well:\n\ndpkg --get-selections: outputs a list of all packages and their status (installed, deinstall, purge)\nThis creates a complete snapshot of your system‚Äôs package state\n\n\nRemote Transfers of Backups\nWe covered rsync vs.¬†scp earlier, so remember that rsync is specifically designed for copying and transferring files, so it offers more sophisticated file synchronization capabilities than basic tools like SCP. If you need a refresher, run the following command from your client machine (laptop), just change the paths to match what your system uses.\nrsync -avz --partial --progress --update chris@ubuntu-pi-server:/mnt/backups/configs/master/ ~/Documents/raspberry_pi_server/backups/configs/master\nThe flags do the following:\n\n-a: Archive mode, which preserves permissions, timestamps, symbolic links, etc.\n-v: Verbose output, showing what files are being transferred\n-z: Compress data during transfer for faster transmission\n--partial: Keep partially transferred files, allowing you to resume interrupted transfers\n--progress: Show progress during transfer\n--update: Skip files that are newer on the receiver (only transfer if source is newer)\n\n\n\n\nRestoring from Backup\nNow that we‚Äôve backed up all of the configurations we‚Äôve made so far, it‚Äôs time to create a script that restores that backup. At the time of writing this, I‚Äôve probably had to reflash a fresh image and reconfigure things between 10 and 20 times. I‚Äôm so good at it, that I can now do it all in under 20 minutes. That being said, it‚Äôs much easier to do when you can just run a script that takes all of the configurations from your Master backup and overwrites the defaults.\nFirst, here are some important things to remember:\n\nThe --delete option during restore will remove files at the destination that don‚Äôt exist in the backup. Use with caution.\nConsider using rsync‚Äôs --dry-run option to test backups and restores without making changes.\nThe backup includes sensitive system files. Store it securely and restrict access.\nConsider encrypting the backup directory for additional security.\nTest the restore process in a safe environment before using in production.\n\nAfter writing this, you can test the script by running it on your server with the boot media you‚Äôve been using (not the SSD)‚Äì just make sure you save the master/ backup and any scripts/configs externally first. You‚Äôll know this succeeds, if nothing changes after the reboot. When you‚Äôve verified that‚Äôs done, we‚Äôll shutdown the server and make the SSD the boot media. For now, let‚Äôs write the config_restore script.\n#!/bin/bash\n\n# Simple Configuration Restoration Script for Ubuntu Pi Server\nBACKUP_DIR=${1:-\"/mnt/backups/configs/master\"}\n\n# Check if script is run as root\nif [ \"$(id -u)\" -ne 0 ]; then\n    echo \"Error: This script must be run as root. Try using sudo.\"\n    exit 1\nfi\n\n# Check if the backup directory exists\nif [ ! -d \"$BACKUP_DIR\" ]; then\n    echo \"Error: Backup directory not found: $BACKUP_DIR\"\n    echo \"Usage: $0 [backup_directory_path]\"\n    exit 1\nfi\n\n# Begin restoration process\necho \"Starting configuration restoration from $BACKUP_DIR...\"\necho \"This will overwrite current system configurations with those from the backup.\"\nread -p \"Continue with restoration? (y/n): \" CONFIRM\nif [[ \"$CONFIRM\" != \"y\" && \"$CONFIRM\" != \"Y\" ]]; then\n    echo \"Restoration aborted by user.\"\n    exit 0\nfi\n\n# 1. Restore User and Group Information\necho \"Restoring user and group information...\"\n[ -f \"$BACKUP_DIR/passwd.bak\" ] && rsync -a \"$BACKUP_DIR/passwd.bak\" /etc/passwd\n[ -f \"$BACKUP_DIR/group.bak\" ] && rsync -a \"$BACKUP_DIR/group.bak\" /etc/group\n[ -f \"$BACKUP_DIR/shadow.bak\" ] && rsync -a \"$BACKUP_DIR/shadow.bak\" /etc/shadow\n[ -f \"$BACKUP_DIR/gshadow.bak\" ] && rsync -a \"$BACKUP_DIR/gshadow.bak\" /etc/gshadow\n\n# Explicitly Set Permissions for Critical System Files\necho \"Fixing critical system file permissions...\"\nchmod 644 /etc/passwd   # Read-write for root, read-only for everyone else\nchmod 644 /etc/group    # Read-write for root, read-only for everyone else  \nchmod 640 /etc/shadow   # Read-write for root, read-only for shadow group\nchmod 640 /etc/gshadow  # Read-write for root, read-only for shadow group\n\n# 2. Restore SSH Configuration\necho \"Restoring SSH configuration...\"\n[ -d \"$BACKUP_DIR/ssh\" ] && rsync -a \"$BACKUP_DIR/ssh/\" /etc/ssh/\nchmod 600 /etc/ssh/ssh_host_*_key 2&gt;/dev/null || true\nchmod 644 /etc/ssh/ssh_host_*_key.pub 2&gt;/dev/null || true\n\n# 3. Restore UFW Configuration\necho \"Restoring UFW configuration...\"\nif [ -d \"$BACKUP_DIR/ufw\" ]; then\n    apt-get install -y ufw &gt;/dev/null\n    rsync -a \"$BACKUP_DIR/ufw/\" /etc/ufw/\nfi\n\n# 4. Restore Fail2Ban Configuration\necho \"Restoring Fail2Ban configuration...\"\nif [ -d \"$BACKUP_DIR/fail2ban\" ]; then\n    apt-get install -y fail2ban &gt;/dev/null\n    rsync -a \"$BACKUP_DIR/fail2ban/\" /etc/fail2ban/\nfi\n\n# 5. Restore Network Configuration\necho \"Restoring network configuration...\"\n[ -d \"$BACKUP_DIR/network\" ] && rsync -a \"$BACKUP_DIR/network/\" /etc/network/\n[ -d \"$BACKUP_DIR/systemd/network\" ] && rsync -a \"$BACKUP_DIR/systemd/network/\" /etc/systemd/network/\n[ -d \"$BACKUP_DIR/netplan\" ] && rsync -a \"$BACKUP_DIR/netplan/\" /etc/netplan/\n[ -f \"$BACKUP_DIR/hosts.bak\" ] && rsync -a \"$BACKUP_DIR/hosts.bak\" /etc/hosts\n[ -f \"$BACKUP_DIR/hostname.bak\" ] && rsync -a \"$BACKUP_DIR/hostname.bak\" /etc/hostname\n[ -f \"$BACKUP_DIR/resolv.conf.bak\" ] && rsync -a \"$BACKUP_DIR/resolv.conf.bak\" /etc/resolv.conf\n[ -d \"$BACKUP_DIR/wpa_supplicant\" ] && rsync -a \"$BACKUP_DIR/wpa_supplicant/\" /etc/wpa_supplicant/\n\n# 6. Restore Filesystem Table (fstab)\necho \"Restoring filesystem table (fstab)...\"\n[ -f \"$BACKUP_DIR/fstab.bak\" ] && rsync -a \"$BACKUP_DIR/fstab.bak\" /etc/fstab\n\n# 7. Restore Package List\necho \"Reinstalling packages from backup...\"\nif [ -f \"$BACKUP_DIR/package_list.txt\" ]; then\n    apt-get update && apt-get install -y dselect\n    dpkg --set-selections &lt; \"$BACKUP_DIR/package_list.txt\"\n    apt-get dselect-upgrade -y\n\n# Restart services\nsystemctl restart systemd-networkd wpa_supplicant@wlan0.service ssh ufw fail2ban \n\necho \"Configuration restoration completed. A system reboot is recommended.\"\nread -p \"Would you like to reboot now? (y/n): \" REBOOT\n[[ \"$REBOOT\" == \"y\" || \"$REBOOT\" == \"Y\" ]] && reboot\n\nexit 0\nYou probably have some questions about the script let me explain some of the decisions I made while doing some trial and error testing.\n\nOriginally, I had the backup directory as a value in the script call itself, now it just defaults to the master/ backup\n\nThis backup is one I know that works and is in the format I‚Äôm hoping\nEasier to have a standard version to reference than relying on monthly backups\n\nI had a lot of issues with incorrect permissions after restoring backups previously, so it needs to be run with sudo\nFirst, the user and group information is important, a lot of processes behind the scenes rely on these configurations\n\nPart of this, I added an explicit chmod call because after the reboot, I was getting an error with whoami\n\nThe command whoami returns which user you are/currently running commands as\nThe user and group info wasn‚Äôt exactly the same, it was leaving my user chris as UID 1000, but changing the group to 1003\nThe chmod call fixes that\nYou can use getent group | grep 'chris' to view all group IDs and assignments\n\n\nSecond, restoring the ssh configurations ensures security and remote connectivity\nThird, ufw increases your system security\nFourth, fail2ban does the same by improving security\nFifth, restoring the network configurations, originally, I had issues because networkd wasn‚Äôt included\n\nThis block ensures all of the systemd configurations are included\n\nSixth, restoring the Filesystem Table (fstab)\n\nThis is easy, since fstab is already a file, we just overwrite what‚Äôs there\n\nSeventh, package restoration\n\ndpkg --set-selections: takes the saved list and marks packages for installation or removal\napt-get dselect-upgrade: then acts on these selections to install missing packages\nThe dselect tool is installed first because it‚Äôs needed for the upgrade process\n\nThen, the specific services we modified are all explicitly started\n\nWhile developing this, some of the services wouldn‚Äôt necessarily start, so I would run into network or ssh issues post-reboot\n\nFinally, the script asks you to reboot your system so all of the changes take affect\n\n# Make the script executable\nchmod +x /scripts/config_restore.sh\n\n# Run the script\nsudo ./scripts/config_restore.sh\n\n\nAutomating Backups with Crontab\nNow that we have both backup and restore scripts in place, the next step is to automate the backup process. Manual backups are valuable but prone to human error, we might forget to run them‚Äì or run them inconsistently. Automation ensures your server configurations are backed up regularly without requiring your intervention, providing an essential safety net against data loss and configuration issues.\nCron is a time-based job scheduler in Unix-like operating systems, including Ubuntu. It enables users to schedule commands or scripts to run automatically at specified intervals. The crontab (cron table) is a configuration file that contains the schedule of cron jobs with their respective commands. Each user on the system can have their own crontab file, and there‚Äôs also a system-wide crontab that requires root privileges to modify.\n\nCreating Automated Backup Jobs\nLet‚Äôs schedule our configuration backup script to run automatically every week. First, we‚Äôll open the crontab editor for the entire system. If you‚Äôre running a more robust system with various users and groups, you‚Äôll probably want to use crontab -e to configure your user specific schedules. That being said, for the time being and because the system is configured, I‚Äôll setup my cron jobs to be system-wide as well:\n# Open the crontab file for the system\nsudo nano /etc/crontab\n\n# Run configuration backup every Sunday at 10:00 PM\n0 22 * * 0 root /home/chris/scripts/config_backup.sh\nHere‚Äôs a handy way to visualize the crontab syntax, with some explanations for each component down below. Note that we‚Äôre using the absolute path /home/chris/scripts/config_backup.sh rather than ./scripts/config_backup.sh. The absolute path starts from the root directory, making it work correctly regardless of the current working directory when cron executes the job as root. When cron runs your commands, it doesn‚Äôt necessarily use the same current or home directory you might expect, so absolute paths are more reliable.\n\n0: - At minute 0\n22: At 10 PM\n*: Every day of the month\n*: Every month\n0: Only on Sunday (day 0)\nroot: Run the command as the root user\nsudo home/chris/scripts/config_backup.sh: The command to execute\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ minute (0-59)\n‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ hour (0-23)\n‚îÇ ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ day of the month (1-31)\n‚îÇ ‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ month (1-12)\n‚îÇ ‚îÇ  ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ day of the week (0-6) (Sunday=0)\n‚îÇ ‚îÇ  ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ user to run the command as\n‚îÇ ‚îÇ  ‚îÇ ‚îÇ ‚îÇ ‚îÇ                                   \n0 22 * * 0 root /scripts/config_backup.sh\nYou may also want to create a monthly backup job that you can use for a /master backup more consistent with your most recent development.\n# Add line to create a master backup on the 1st of every month at 4:00 AM\n0 4 1 * * root /home/chris/scripts/config_backup.sh && rsync -a /mnt/backups/configs/$(date +\\%Y\\%m\\%d)/ /mnt/backups/configs/master/\nThis job runs at 4:00 AM on the first day of each month, creating a regular backup and then copying it to the /mnt/backups/configs/master/ directory. The date command is escaped with backslashes (%) because crontab interprets percent signs specially. Finally, To prevent our backup drive from filling up, let‚Äôs add a job to automatically remove backups older than 90 days, except for the master backup:\n# Add this line to the system crontab\n0 23 * * 0 root find /mnt/backups/configs/ -maxdepth 1 -type d -name \"20*\" -mtime +90 -not -path \"*/master*\" -exec rm -rf {} \\;\n\n0 5 * * 0: Run at 5:00 AM every Sunday\nfind /mnt/backups/configs/: Start searching in the configs directory\n-maxdepth 1: Only look in the immediate directory, not subdirectories\n-type d: Only look for directories\n-name \"20*\": Only match directories starting with ‚Äú20‚Äù (our date-formatted directories)\n-mtime +90: Only match items modified more than 90 days ago\n-not -path \"*/master*\": Exclude anything with ‚Äúmaster‚Äù in the path\n-exec rm -rf {} \\;: Delete each matching directory\n\n\n\nVerifying Jobs\nNow, let‚Äôs verify that everything saved and works. First, close out of nano using Ctrl+O (write-out, or save) and Ctrl+X (Exit). Then let‚Äôs print the contents with cat /etc/crontab. You can also run the following:\n# Check cron service status\nsudo systemctl status cron\n\n# View cron logs\nsudo grep CRON /var/log/syslog\n\n\nTroubleshooting\nI had no issues while making the above changes; however, after restarting cron I got the following message from the systemctl status command.\n\nAfter doing some research, I ran sudo systemctl cat cron.service to view the systemd configuration for cron. In the cron.service file there is the line ExecStart=/usr/sbin/cron -f -P $EXTRA_OPTS. The service is trying to use the $EXTRA_OPTS variable, but it‚Äôs not defined anywhere. The service is configured to look for environment variables in a file specified by this line EnvironmentFile=-/etc/default/cron. The dash before the filepath means this file is optional - if it doesn‚Äôt exist, systemd continues anyway. This explains why cron still works despite the warning, which is why the status command returned a warning, but not any errors. Here‚Äôs what the cron ini file looks like.\n# /usr/lib/systemd/system/cron.service\n[Unit]\nDescription=Regular background program processing daemon\nDocumentation=man:cron(8)\nAfter=remote-fs.target nss-user-lookup.target\n\n[Service]\nEnvironmentFile=-/etc/default/cron\nExecStart=/usr/sbin/cron -f -P $EXTRA_OPTS\nIgnoreSIGPIPE=false\nKillMode=process\nRestart=on-failure\nSyslogFacility=cron\n\n[Install]\nWantedBy=multi-user.target\nLet‚Äôs try resolving this by utilizing systemd and creating a system override. We‚Äôll add one line to the etc/systemd/system/cron.service.d/override.conf. As a note, you‚Äôve probably run into the .d/ paths on your server. Whenever you see .d added to the end of a file path, like /etc/systemd/system/cron.service.d, it just means that‚Äôs a directory which has config overrides.\n# Create a systemd override\nsudo systemctl edit cron.service\n### Editing /etc/systemd/system/cron.service.d/override.conf\n### Anything between here and the comment below will become the contents of the drop-in file\n\nEnvironment=\"EXTRA_OPTS=\"\n\n### Edits below this comment will be discarded\n\n\n### /usr/lib/systemd/system/cron.service\n# [Unit]\n# Description=Regular background program processing daemon\n# Documentation=man:cron(8)\n# After=remote-fs.target nss-user-lookup.target\n# \n# [Service]\n# EnvironmentFile=-/etc/default/cron\n# ExecStart=/usr/sbin/cron -f -P $EXTRA_OPTS\n# IgnoreSIGPIPE=false\n# KillMode=process\n# Restart=on-failure\n# SyslogFacility=cron\n# \n# [Install]\n# WantedBy=multi-user.target\nAfter you implement the solution, restart the systemctl daemon and the cron service.\nsudo systemctl daemon-reload\nsudo systemctl restart cron\nsudo systemctl status cron\nNow, you should see everything working correctly.\n\nThis should resolve the warning message. The service is already working correctly with -f (stay in foreground, which systemd needs) and -P (send messages to syslog) flags, so we‚Äôre just providing an empty value for the EXTRA_OPTS variable to stop systemd from complaining about it being undefined.\nWith these automated backup jobs in place, your Raspberry Pi server will maintain a regular backup schedule without manual intervention. The weekly backups provide recent restore points, while the monthly master backup ensures you always have a stable configuration to fall back on if needed. The cleanup job prevents your backup drive from running out of space over time.",
    "crumbs": [
      "Guides",
      "Linux Server & Raspberry Pi"
    ]
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-change_boot_media",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-change_boot_media",
    "title": "Setting Up A Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Changing Your Boot Media Device",
    "text": "Changing Your Boot Media Device\n\nKey Terms\nBoot Process Terminology:\n\nBoot Sequence: The ordered steps followed during system startup.\nBoot Media: The storage device from which a system loads its operating system.\nBoot Loader: Software responsible for loading the operating system kernel.\nFirmware: Software embedded in hardware that provides low-level control.\nUEFI (Unified Extensible Firmware Interface): A modern firmware interface that replaces BIOS.\nEFI System Partition (ESP): A FAT32-formatted partition containing boot loaders and files needed by UEFI.\nBoot Flag: A marker that identifies a partition as bootable.\nGRUB (GRand Unified Bootloader): A popular boot loader for Linux systems.\nKernel Parameters: Options passed to the Linux kernel during boot.\ninitramfs: An initial RAM filesystem loaded during boot to prepare the actual root filesystem.\n\nMedia Transition Concepts:\n\nCloning: Creating an exact copy of a storage device or partition.\nImaging: Creating a file representation of the contents of a storage device.\nBootable Media: Storage media configured to start an operating system.\nFlash: To write data to a memory device, particularly firmware or an operating system image.\nDevice ID: A unique identifier assigned to hardware components.\nUUID (Universally Unique Identifier): A standardized identifier format used to identify filesystems.\nPartition UUID: A unique identifier assigned to a specific partition.\nFilesystem UUID: A unique identifier assigned to a filesystem on a partition.\nsyslinux: A lightweight boot loader for Linux systems.\ndd: A command-line utility used for low-level copying of data between devices.\n\n\n\nBoot Configuration Transition Process\nWhen you‚Äôre moving from one boot device to another on a Raspberry Pi, you‚Äôre essentially telling the system where to find its operating system files. The Raspberry Pi‚Äôs bootloader looks for specific files on a FAT32 partition to begin the boot process, then loads the main operating system from the root partition. This transition is a fundamental server administration skill that every system administrator should understand.\nThe transition involves several critical steps.\n\nFirst, we prepare the SSD with a fresh operating system installation.\nThen we properly shut down the system to ensure no data corruption occurs.\nNext, we physically reconfigure the hardware to make the SSD the primary boot device.\nFinally, we verify that everything works correctly and remove the temporary boot media.\n\nThink of this process like moving into a new house. You‚Äôve already built the structure (partitioning), but now you need to move all your belongings (the operating system) into it, update your mailing address (boot configuration), and ensure everything works in the new location.\nThe good news, we‚Äôve already done most of the work required and what we haven‚Äôt is going to be something we did for the original boot device. We‚Äôll first run a few commands from the server, do one thing on a different computer, and then we‚Äôll be back to the server.\n1. First, you‚Äôll need to flash a fresh Ubuntu Server LTS image onto your newly partitioned SSD. This process will write the operating system files to the appropriate partitions you created. This should be done on your Raspberry Pi server.\n# Before removing the SSD, check its device identifier one more time\nsudo lsblk -o NAME,SIZE,FSTYPE,LABEL,TYPE,MOUNTPOINT\nNow you‚Äôll need to use RPi Imager on your MacBook:\n\nConnect the SSD to your MacBook\nOpen Raspberry Pi Imager\nSelect Ubuntu Server LTS (same version you used before)\nSelect your SSD as the storage device\nUse the exact same advanced settings you used when first setting up your Raspberry Pi:\n\nSame username (chris)\nSame password\nSame SSH key settings\nSame WiFi credentials\n\n\nThe reason we use identical settings is to reduce the amount of changes needed to replicate the environment we previously configured. Your SSH keys, user permissions, and network configurations will all match what we know works for an initial boot, eliminating the need to reconfigure everything from scratch.\n2. Before making any hardware changes, we need to ensure all data is written to disk and all processes are safely terminated. This prevents corruption and data loss during the transition.\n# Save any unsaved work and exit all applications\n# Ensure no important processes are running\n\n# Sync all file system buffers to disk\nsudo sync\n\n# Check for any open files on your current boot device (thumb drive)\nsudo lsof | grep -E '^[^ ]+ +[^ ]+ +[^ ]+ +[^ ]+ +[^ ]+ +[^ ]+ +FIFO'\n\n# Display active processes to ensure nothing critical is running\nps aux | grep -v '\\['\n\nThe sync command forces all pending disk writes to complete immediately. This is crucial because Linux uses write caching for performance, meaning data might still be in memory waiting to be written.\nThe lsof command lists open files, helping you identify any processes that might be accessing the current boot device.\nThe ps aux command shows all active processes, giving you a final check that nothing important is running.\n\n3. A proper shutdown sequence ensures all services stop gracefully and file systems are cleanly unmounted:\n# Perform a clean system shutdown\nsudo shutdown -h now\nThe shutdown command initiates a clean shutdown sequence. The -h flag tells the system to halt (power off) after shutdown. This is important because the system may not power off, especially on older systems‚Äîit could just bring it to single-user mode or runlevel 1, depending on configuration. While now indicates the shutdown should happen immediately. This command:\n\nSends a termination signal to all running processes\nAllows services to save their state and clean up\nUnmounts all filesystems in the correct order\nFinally powers down the system\n\n4. Now comes the physical hardware transition. With your Raspberry Pi powered off:\n\nRemove the thumb drive (current boot device)\nConnect the SSD via USB to the Raspberry Pi\nSet aside the microSD card (backup device)\nEnsure all connections are secure\n\nThis step is straightforward but crucial. The Raspberry Pi will attempt to boot from the first bootable device it finds. By removing the thumb drive and connecting the SSD, we‚Äôre ensuring the Pi finds and uses the SSD as its boot device. We do not need to remove the SD card used for backups, because it never had an OS flashed onto it. The card‚Äôs file system just provides extra memory, instead of being an extra operating system.\n5. Power on your Raspberry Pi and observe the boot process.\nMake sure to connect your monitor and keyboard before the boto begins. The boot process should proceed similarly to your initial setup. The Raspberry Pi firmware reads the configuration from the SSD‚Äôs boot partition, loads the kernel, and then mounts the root filesystem. If everything works correctly, you should see the familiar Ubuntu Server boot messages and eventually reach a login prompt.\nGood news, you won‚Äôt need to do this again (for this server). After this, you‚Äôll have your core system, memory, and configurations complete.\n6. After booting, you‚Äôll need to log in directly to the Raspberry Pi using a keyboard and monitor. This is because the SSH service may not be running automatically on the fresh installation.\n\nConnect a keyboard and monitor to your Raspberry Pi\nLog in with your username and password\nStart the SSH service manually:\n\n# Start the SSH service to enable remote connections\nsudo systemctl start ssh\n\n# Verify the service is running\nsudo systemctl status ssh\n\n# Test a remote connection from your client machine\n# Test SSH connection (initially with password authentication)\nssh chris@192.168.1.151\n\nYou‚Äôll need to specify the local IP because none of the ssh configs are updated yet\n\nYour nonstandard port, 45000 in this guide\nThe local IP may be a different one in the 192.168.0.0/16 range (which is reserved for local IPs)\n\nIf this is working and you can remotely connect, then we can move on to the next step\n\n7. With SSH access established, transfer your configuration backup and restore scripts from your computer to the Raspberry Pi:\nFirst, make sure to create the directory, because it won‚Äôt exist on a fresh boot. The, from your client computer, a MacBook in my case, you‚Äôll run the rsync commands to move the master backup and scripts. Notice that you can use either the ~ shortcut to denote your home directory, or write the path explicitly. We‚Äôll also need to move the master backup to the proper backup directory, so the restore script works properly.\n# On the Raspberry Pi, create the backup directory structure\nsudo mkdir -p /mnt/backups/configs/\n\n# From your MacBook, copy your backup files to the Pi\nrsync -avz ~/path/to/backups/configs/master chris@ubuntu-pi-server:/home/chris/\n\n# Copy your restore script to the Pi\nrsync -avz ~/path/to/scripts/config_restore.sh chris@ubuntu-pi-server:~/scripts/\n\n# Move the master backup directory to the correct location\nsudo mv /home/chris/master /mnt/backups/configs/\n8. Run the Configuration Restore Script\n# Make sure the script is executable\nchmod +x ~/scripts/config_restore.sh\n\n# Run the restore script\nsudo ./scripts/config_restore.sh\n9. Verify the System\n# Verify the root filesystem device\ndf -h /\n\n# Verify the microSD card is mounted properly\ndf -h /mnt/backups\n\n# Check that your backup files are accessible\nls -la /mnt/backups/configs/\n\n# Verify your network settings\nip addr show\n\n# Check System configurations\nsudo systemctl status ssh\nsudo systemctl status ufw\nsudo systemctl status fail2ban\nsudo systemctl status systemd-networkd\nsudo systemctl status wpa_supplicant@wlan0.service\nThe df -h / command shows disk usage statistics for the root filesystem, including which device it‚Äôs mounted from. You should see /dev/sdb2 (or similar) listed as the root device, and /dev/sdb1 as the boot device, not the thumb drive identifier you used before. You‚Äôll see a similar output, just focused on your /backups directory when running the second command. The ls -la command shows you all of the contents of a directory, as well as the permissions. The other commands you should be familiar with by now.\n\n\nFinal Thoughts\nOur Ubuntu Raspberry Pi Server is now booting from the SSD, and all your previous configurations have been restored. You may notice that on reboot, running sudo lsblk will show your SSD under a different This configuration provides a solid foundation for the more advanced server features we‚Äôll implement in the next sections. By moving to SSD boot, we‚Äôve significantly improved our server‚Äôs performance profile. SSDs offer several advantages over traditional storage media:\n\nFaster boot times: Your Raspberry Pi will start much more quickly\nImproved I/O performance: Database operations, file access, and application loading will be noticeably faster\nBetter reliability: SSDs have no moving parts, making them more resilient to physical impacts\nLower power consumption: SSDs typically use less power than traditional hard drives\n\nThe transition to SSD boot also aligns with modern server practices, where solid-state storage is becoming the standard for production environments. This configuration will serve us well as we expand into containerization with Docker and orchestration with Kubernetes.\nThe last thing to do, is give your Raspberry Pi some decency. Now, I‚Äôll attach the fan and case to the Raspberry Pi, so it‚Äôll be ready to run 24/7. This is a straightforward process, but I needed the internet‚Äôs help to figure out which cables connected to which pin.\n\nSafely shutdown your server, and unplug everything.\n\n\n\nInsert the fan into the case.\n\n\n\nInsert the board, with the SD Card inserted, into the case.\n\n\n\nConnect the Red Wire to Pin 4 (5V), Black Wire to Pin 6 (GND), and the Blue Wire to Pin 8 (GPIO14). Leave the Blue Wire unplugged if you want the fan to be always on.\n\n\n\nMake sure everything is all set, if you are passionate about wire management, you can probably make it prettier than I did.\n\n\n\nAdmire your handsome server\n\n\nNow, we‚Äôre ready to move on to some of the fun stuff. If youre fan didn‚Äôt turn on right away, it‚Äôs probably because you followed my images and plugged in the blue (control) wire. When this is connected, it turns the fan off. From what I researched online, it sounds like there is a way to keep it plugged in and then configure the fans usage parameters. That being said, I felt like diving into managing a circuit board from your OS would have been too much of a tangent/rabbit hole for this guide, so that‚Äôs probably something I‚Äôll explore in the future, separately.",
    "crumbs": [
      "Guides",
      "Linux Server & Raspberry Pi"
    ]
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-monitor_maintain",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-monitor_maintain",
    "title": "Setting Up A Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Monitoring and Maintenance",
    "text": "Monitoring and Maintenance\n\nKey Terms\nSystem Monitoring Concepts:\n\nMonitoring: The process of observing and tracking system performance and status.\nMetrics: Measurable values that indicate performance or resource usage.\nResource Utilization: The degree to which system resources are being used.\nLoad Average: A measure of CPU utilization over time.\nThreshold: A predefined value that, when exceeded, may trigger notifications or actions.\nBaseline: Normal or expected performance values for comparison.\nReal-time Monitoring: Observing system status as it happens.\nHistorical Data: Saved metrics showing performance trends over time.\nAlert: A notification triggered when monitored values exceed thresholds.\nDashboard: A visual interface displaying multiple metrics at once.\n\nMonitoring Tools and Commands:\n\ntop: A command-line utility showing real-time system resource usage.\nhtop: An enhanced version of top with a more user-friendly interface.\niotop: A utility for monitoring disk I/O usage by processes.\niostat: A command that reports CPU and disk I/O statistics.\nvmstat: A tool that displays virtual memory statistics.\nfree: A command that displays amount of free and used memory.\ndf: A utility that reports filesystem disk space usage.\ndu: A command that estimates file and directory space usage.\nnetstat: A command-line tool that displays network connections, routing tables, and interface statistics.\nss: A modern replacement for netstat for investigating sockets.\n\nMaintenance Terminology:\n\nPatch: A piece of software designed to update or fix issues in a program.\nUpdate: New versions of software that add features or fix bugs.\nUpgrade: A significant update that may involve major changes.\nPackage Manager: A system for installing, updating, and removing software packages.\napt: Advanced Package Tool, the package management system used by Debian and Ubuntu.\nRepository: A storage location from which software packages can be retrieved.\nDependency: A software package required by another package to function.\nCleanup: The process of removing unnecessary files or data.\nScheduled Maintenance: Regular, planned maintenance activities.\nPreventive Maintenance: Activities performed to prevent system failures.\n\nLog Management:\n\nLog: A record of events that occur within the system.\nsyslog: A standard for message logging.\njournald: Systemd‚Äôs logging service that collects and stores logging data.\nLog Rotation: The process of archiving and removing old log files.\nLog Level: The severity or importance assigned to a log entry.\nstdout: Standard output stream where normal process output is written.\nstderr: Standard error stream where error messages are written.\nAudit Log: A record of events relevant to security.\nlogrotate: A utility that manages automatic rotation of log files.\nCentralized Logging: Collecting logs from multiple systems in a central location.\n\n\n\nMonitoring and Maintenance Basics\nFor a 24/7 server, storage reliability is critical. A self-hosted server relies heavily on its storage device, and I/O errors can disrupt services and potentially lead to data loss. Monitoring your SSD‚Äôs health and properly configuring your system to handle power interruptions are essential practices for maintaining a reliable self-hosted server.\nContinuous uptime demands proactive management rather than reactive troubleshooting. Without proper monitoring, subtle hardware degradation can progress undetected until catastrophic failure occurs, resulting in extended downtime and potential permanent data loss. Regular maintenance prevents small issues from cascading into system-wide failures and helps maintain consistent performance over time. For Raspberry Pi servers specifically, where hardware operates in potentially suboptimal conditions (varying temperatures, consumer-grade power supplies, and external storage), monitoring becomes even more crucial. Proper maintenance routines extend hardware lifespan, optimize resource usage, and ensure security vulnerabilities are promptly addressed. The slight overhead of implementing monitoring and scheduled maintenance is substantially offset by avoiding the significant costs of emergency recovery, both in terms of lost data and service disruption. Furthermore, systematically collected performance data enables informed decisions about capacity planning and system upgrades, ensuring your server evolves to meet changing demands without overprovisioning or unexpected resource exhaustion.\nAfter chaning my boot media, I ended up with a an I/O error on my SSD, which resulted in data loss. To give a quick summary, this means the processs by which data is written to and read from memory had an issue. This section will walk through how to diagnose and resolve that issue, how and why to configure security updates on a patching schedule, finally how we can manage logs to ensure effective system information and memory usage.\n\n\nDiagnosing and Resolving SSD Issues\n1. Install the primary tool for storage health monitoring\n# Always update your current system packages before installing a new one\nsudo apt update\nsudo apt install -y smartmontools\nThe smartmontools package provides utilities for monitoring storage devices using SMART (Self-Monitoring, Analysis, and Reporting Technology) - a monitoring system included in most modern storage devices. These tools allow you to check the internal health parameters of your storage device and identify potential issues before they lead to data loss.\n2. Identify your SSD\nsudo lsblk -o NAME,SIZE,MODEL,SERIAL,MOUNTPOINT\n\nRemember, this command lists all block devices with their names, sizes, model information, serial numbers, and mount points. From the output, we can see:\n\nThe Samsung T7 SSD (PSSD T7) is /dev/sda with a capacity of 931.5GB\nIt has two partitions: sda1 (boot partition) and sda2 (root filesystem)\nAn SD card is mounted at /mnt/backups\n\nThe -o flag specifies which columns to display, giving us a comprehensive view of all storage devices connected to the Raspberry Pi.\n3. Check Basic SSD Health\nsudo smartctl -H /dev/sda\n\nThe -H flag performs a basic health check, asking the drive to evaluate its own condition. The result ‚ÄúPASSED‚Äù indicates the drive believes it‚Äôs functioning properly at a hardware level. This is the quickest way to assess if the drive has detected any internal failures.\n4. View Detailed SMART Information\nsudo smartctl -a /dev/sda\n  \nKey findings from the output:\n\nTemperature: 27¬∞C (excellent, well below the 52¬∞C warning threshold)\nPower On Hours: 2,673 (about 111 days)\nUnsafe Shutdowns: 61 (significant issue)\nMedia and Data Integrity Errors: 0 (good)\nAvailable Spare: 100% (excellent)\nPercentage Used: 0% (drive health is excellent)\n\nThe 62 Unsafe Shutdowns occur when the system loses power or crashes without properly unmounting the filesystems. When this happens:\n\nWrite operations may be interrupted: If the system is writing data to the SSD when power is lost, the write operation may be incomplete, leading to partially written files.\nJournal transactions remain unfinished: Modern filesystems like ext4 use journaling to track changes before they‚Äôre committed. Power loss can leave these journals in an inconsistent state.\nFilesystem metadata corruption: Critical filesystem structures may be left in an inconsistent state, potentially making files or directories inaccessible.\nDirty cache data loss: Data waiting in the cache to be written to disk is lost during a sudden power cut.\n\nThese unsafe shutdowns are particularly problematic for SSDs because they use complex internal mechanisms like wear leveling and garbage collection. Interrupting these processes can lead to:\n\nPartial page programming\nIncomplete block erasures\nInconsistent mapping tables\nLost block allocation information\n\nWhile modern SSDs have power loss protection mechanisms, consumer-grade external SSDs like the Samsung T7 may have limited protection compared to enterprise-grade drives. The high number (62) indicates a pattern of improper shutdowns, likely due to:\n\nPower interruptions to the Raspberry Pi\n\nThis was definitely one of the issues I had\nI was using sudo shutdown now and then unplugging, rather than adding the -h flag to halt power\nSometimes, I just unplugged the power (this is bad too)\n\nSystem crashes requiring hard resets\nUnplugging the SSD without properly unmounting it\n\nThis also happened at least once and was probably a major contributor\n\nPower saving features improperly configured\n\nThe -a flag displays all SMART information available from the drive, providing a comprehensive view of the drive‚Äôs health and history.\n5. Attempt a SMART Self-Test\nThis step doesn‚Äôt apply, if you‚Äôre using the Samsung T7, because it doesn‚Äôt support self-tests, which you can see from the smartctl output. That being said, if you‚Äôre using an SSD that allows this, the step may be helpful. The Samsung T7 portable SSD doesn‚Äôt support SMART self-tests. This is common for external/portable SSDs, as they often implement a more limited set of SMART commands. The -t short flag attempts to initiate a short self-test, which would typically take a few minutes to complete.\nsudo smartctl -t short /dev/sda\nThis is the output I received.\nsmartctl 7.4 2023-08-01 r5530 [aarch64-linux-6.8.0-1024-raspi] (local build)\nCopyright (C) 2002-23, Bruce Allen, Christian Franke, www.smartmontools.org\nSelf-tests not supported\n6. Check Filesystem Integrity\n# Check the boot partition\nsudo fsck -n /dev/sda1\n\nOutput shows two issues:\n\nDifferences between boot sector and backup\nDirty bit set - indicating improper unmounting\n\n# Check the root partition\nsudo fsck -n /dev/sda2\n\nOutput indicates the filesystem is clean but warns it‚Äôs currently mounted.\n\nThe -n flag performs a read-only check without making changes, allowing you to safely examine mounted filesystems.\nThe findings confirm that improper shutdowns have affected the filesystem integrity, particularly on the boot partition.\n\n7. Check System Logs for Errors\nsudo dmesg | grep -i 'error\\|ata\\|sda\\|failed\\|i/o'\n\nsudo journalctl -p err..emerg -o short-precise | grep -i 'sda\\|disk\\|i/o\\|error'\n\nsudo journalctl -u smartd\n\nThe output shows normal disk detection and mounting events, without indicating any current I/O errors. No errors related to the disk were found in the system journal. No entries found, indicating the SMART monitoring service isn‚Äôt running. These commands search through different system logs for any reported disk errors. The absence of current errors suggests that despite the history of unsafe shutdowns, the filesystem has remained resilient enough to recover without logging critical errors.\n8. Check the Power Supply Status\nvcgencmd get_throttled\n\nOutput: 0x0\nThis indicates no power-related issues have been detected since boot. The vcgencmd get_throttled command is specific to Raspberry Pi systems and reports if the system has experienced undervoltage, overheating, or other throttling conditions. A non-zero value would indicate power problems affecting the Raspberry Pi. The result 0x0 is good news, suggesting that your current power supply is adequate for normal operation.\n\n\nImplementing Preventive Measures\n1. Enable SMART Monitoring Service\n# Configure the SMART monitoring service\nsudo nano /etc/smartd.conf\n\nThe SMART monitoring daemon (smartd) continuously checks your drive‚Äôs health and can alert you to developing issues. Add this line to monitor your SSD (replace sda with your device if different): /dev/sda -a -o on -S on -s (S/../.././02|L/../../6/03) -m root. This should go above the line with DEVICESCAN, because every configuration after it is ignored.\nThis configuration:\n\n-a: Monitors all SMART attributes\n-o on: Enables automatic offline testing\n-S on: Saves error logs\n-s (S/../.././02|L/../../6/03): Schedules short tests at 2 AM daily and long tests on Saturdays at 3 AM\n-m root: Sends email alerts to the root user\n\nThen enable and start the service.\nsudo systemctl enable smartd\nsudo systemctl start smartd\nTo receive email alerts, you‚Äôll need to configure a mail transfer agent like postfix. Personally, I just didn‚Äôt include that functionality now, but it‚Äôs something I‚Äôll eventually configure; however, I left it in for those curious. The SMART monitoring service provides proactive protection by continuously monitoring your drive‚Äôs health metrics. It can detect deteriorating conditions before they lead to data loss and send you notifications when potential issues are identified.\n2. Configure Proper Filesystem Mount Options\n# Edit /etc/fstab\nsudo nano /etc/fstab\n\n# Add specific options for your root partition\nsudo blkid | grep sda2\nThen add this line to your /etc/fstab file: UUID=your-uuid-here / ext4 defaults,noatime,commit=60 0 1. While your SSD partitions automatically mount at boot, optimizing the mount options can significantly improve resilience against power failures.\nLet‚Äôs look at these options in detail:\n\ndefaults: This incorporates standard mount options: rw (read-write), suid (allow setuid), dev (interpret device files), exec (permit execution of binaries), auto (mountable with -a), nouser (only root can mount), and async (asynchronous I/O).\nnoatime: Disables updating access time attributes on files when they‚Äôre read. This reduces unnecessary write operations, which is especially beneficial for SSDs that have limited write cycles. Every time you read a file without this option, the system would write an update to the file‚Äôs metadata recording when it was last accessed.\ncommit=60: Changes how often filesystem changes are committed to disk (in seconds). The default is 5 seconds, meaning data may stay in RAM for up to 5 seconds before being written to disk. Increasing this to 60 seconds reduces write operations but increases the potential for data loss during a crash. However, it‚Äôs a reasonable compromise for most systems.\n0: This refers to the dump flag. A value of 0 indicates the filesystem should not be backed up by the dump utility (which is rarely used these days).\n1: This is the fsck order. A value of 1 means this filesystem should be checked first during boot if a check is needed. The root filesystem always gets 1, while other filesystems get 2 or higher, or 0 to skip checks.\n\n# Apply the changes without rebooting\nsudo mount -o remount /\nThese optimized mount options help mitigate the impact of unexpected shutdowns by reducing unnecessary writes and ensuring more efficient I/O operations. The trade-off between performance and data safety is balanced by the commit interval - 60 seconds provides reasonable protection while reducing write pressure.\nBy implementing the following measures:\n\nVerifying adequate power supply: The vcgencmd get_throttled output of 0x0 indicates your current power supply is stable.\nEnabling SMART monitoring: Setting up the smartd service provides ongoing monitoring and early warning of potential drive issues.\nOptimizing filesystem mount options: The modified fstab entries with noatime and commit=60 strike a balance between reducing unnecessary writes and maintaining data integrity.\n\nThese implementations together create a more resilient system that:\n\nContinuously monitors drive health\nReduces unnecessary write operations that contribute to SSD wear\nBalances performance with data integrity requirements\nProvides early warning of developing issues\n\nFor a 24/7 self-hosting server, these measures provide essential protection against the most common causes of data loss and system instability. While no solution can completely eliminate the risk from sudden power loss, these configurations significantly reduce the likelihood of filesystem corruption and data loss.\n\n\nLog Management\nEffective log management is crucial for maintaining system health, troubleshooting issues, and detecting security incidents on your Raspberry Pi server. Logs provide insights into system behavior, application performance, and potential security threats. This section will cover fundamental log concepts and practical strategies for managing logs efficiently.\n\nLog Basics\nLinux logs record events occurring within the system and applications, providing essential information for monitoring and troubleshooting. Understanding where logs are stored and how to interpret them allows you to efficiently diagnose problems and maintain system health.\nLet‚Äôs explore the key log locations on your Ubuntu-based Raspberry Pi server:\n# View the main system log\nsudo tail -n 50 /var/log/syslog\n\n# View authentication attempts\nsudo tail -n 50 /var/log/auth.log\n\n# View kernel messages\nsudo dmesg | tail -n 50\n\n# View systemd journal logs\nsudo journalctl -n 50\nKey log files and their purposes:\n\n/var/log/syslog: General system messages and activities\n/var/log/auth.log: Authentication and authorization events\n/var/log/kern.log: Kernel messages and warnings\n/var/log/dpkg.log: Package installation and removal logs\n/var/log/apt/: APT package management activities\n/var/log/fail2ban.log: Failed login attempt blocks\n/var/log/nginx/: Web server logs (if nginx is installed)\n\nUbuntu Server uses two primary logging systems:\n\nTraditional syslog: Text files stored in /var/log/\nSystemd journal: Binary logs accessed through journalctl\n\n# View only error and higher severity messages\nsudo journalctl -p err..emerg\nLog severity levels (from lowest to highest):\n\ndebug: Detailed debugging information\ninfo: Normal operational messages\nnotice: Normal but significant events\nwarning: Potential issues that aren‚Äôt errors\nerr: Error conditions\ncrit: Critical conditions requiring attention\nalert: Actions that must be taken immediately\nemerg: System is unusable\n\n\n\nManagement Tools and Strategy\nProper log management ensures you can efficiently access the information you need while preventing logs from consuming excessive disk space. A balanced strategy involves log rotation, centralized collection, and automated monitoring.\nFirst, let‚Äôs configure logrotate to manage log file growth. We can also create a custom logrotate config for application logs. While this doesn‚Äôt apply to anything we‚Äôve done yet, it will be useful once you start developing and deplying applications on your server. I‚Äôll include this below for anyone that wants a starting point.\n# Examine the main logrotate configuration\ncat /etc/logrotate.conf\n\n# View the service-specific configurations\nls -l /etc/logrotate.d/\n\n# Customize application log management\nsudo nano /etc/logrotate.d/myapplogs\n\n\n/home/chris/apps/*/logs/*.log {\n    weekly\n    rotate 4\n    compress\n    delaycompress\n    missingok\n    notifempty\n    create 0640 chris chris\n}\nThis configuration:\n\nweekly: Rotates logs once per week\nrotate 4: Keeps 4 rotated log files before deletion\ncompress: Compresses rotated logs with gzip\ndelaycompress: Delays compression until the next rotation cycle\nmissingok: Doesn‚Äôt generate errors if log files are missing\nnotifempty: Doesn‚Äôt rotate empty log files\ncreate 0640 chris chris: Creates new log files with specified permissions and ownership\n\nThen, let‚Äôs setup some persistent storage for our journal logs.\n# Configure persistent storage for journal logs\nsudo nano /etc/systemd/journald.conf\n[Journal]\nStorage=persistent\nCompress=yes\nSystemMaxUse=500M\nSystemMaxFileSize=50M\nMaxRetentionSec=1month\n# Apply changes\nsudo systemctl restart systemd-journald\nThis configuration:\n\nStorage=persistent: Saves logs across reboots\nCompress=yes: Compresses older journal files\nSystemMaxUse=500M: Limits total journal size to 500MB\nSystemMaxFileSize=50M: Limits individual journal files to 50MB\nMaxRetentionSec=1month: Automatically removes logs older than a month\n\nFor more detailed systemd journal queries:\n# View logs from a specific service\nsudo journalctl -u ssh\n\n# View logs from a specific time period\nsudo journalctl --since \"2024-05-10\" --until \"2024-05-12\"\n\n# View logs from the current boot\nsudo journalctl -b\nAdvanced log management strategies include:\n\nCentralized logging: Consider setting up a central log server for multiple devices using rsyslog.\nLog analysis tools: For more sophisticated analysis, tools like Logwatch provide automated log summaries.\nSecurity monitoring: Configure fail2ban to monitor logs for security threats and respond automatically.\nCustom alerting: Set up alerts for specific critical events using simple grep scripts and cron jobs.\n\nThese log management practices ensure your Raspberry Pi server maintains manageable, accessible logs while providing the information needed for effective system monitoring and troubleshooting.",
    "crumbs": [
      "Guides",
      "Linux Server & Raspberry Pi"
    ]
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-docker",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-docker",
    "title": "Setting Up A Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Docker",
    "text": "Docker\n\nKey Terms\nContainer Concepts:\n\nContainer: A lightweight, standalone, executable package that includes everything needed to run an application.\nImage: A read-only template containing instructions for creating a container.\nContainerization: The process of packaging an application with its dependencies into a container.\nIsolation: The separation of applications from each other and the underlying system.\nVirtual Machine (VM): A virtualized instance of an operating system running on hypervisor software.\nContainer Runtime: Software that executes containers and manages their lifecycle.\nNamespace: A Linux kernel feature that isolates system resources for containers.\nControl Group (cgroup): A Linux kernel feature that limits, accounts for, and isolates resource usage of process groups.\nLayer: A set of read-only files that represent filesystem differences in a Docker image.\nUnion Filesystem: A filesystem service that layers multiple directories into a single unified view.\n\nDocker Specific Terminology:\n\nDocker: A platform for developing, shipping, and running applications in containers.\nDocker Engine: The runtime that builds and runs Docker containers.\nDocker Hub: A cloud-based registry service for Docker images.\nDocker Desktop: An application for managing Docker on Windows and Mac.\nDockerfile: A text file containing instructions to build a Docker image.\nDocker Compose: A tool for defining and running multi-container Docker applications.\nDocker Swarm: A native clustering and orchestration solution for Docker.\nDocker Network: A communication system that enables containers to communicate with each other and the outside world.\nDocker Volume: A mechanism for persisting data generated by and used by Docker containers.\nDocker Registry: A storage and distribution system for Docker images.\n\nContainer Management:\n\nTag: A label attached to an image version for identification.\nRepository: A collection of related Docker images with the same name but different tags.\nPull: The action of downloading an image from a registry.\nPush: The action of uploading an image to a registry.\nBuild: The process of creating a Docker image from a Dockerfile.\nRun: The command to start a container from an image.\nExec: A command to run additional processes in a running container.\nCommit: Creating a new image from changes made to a container.\nStop/Start: Commands to halt and resume container execution.\nRemove: The action of deleting a container or image.\n\nCI/CD with Containers:\n\nCI/CD (Continuous Integration/Continuous Deployment): Practices that automate the integration and deployment of code changes.\nPipeline: A series of automated steps that code changes go through from development to production.\nBuild Automation: The process of automating the creation of software builds.\nIntegration Testing: Testing the interaction between integrated units/modules.\nDeployment Strategy: A planned approach for releasing changes to production.\nArtifact: A byproduct of software development, such as a compiled application or container image.\nRegistry Authentication: The process of securely accessing a container registry.\nWebhook: An HTTP callback triggered by specific events in a development workflow.\nGitHub Actions: GitHub‚Äôs built-in CI/CD tool.\nJenkins: An open-source automation server often used for CI/CD.",
    "crumbs": [
      "Guides",
      "Linux Server & Raspberry Pi"
    ]
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-kubernetes",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-kubernetes",
    "title": "Setting Up A Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Kubernetes",
    "text": "Kubernetes\n\nKey Terms\nKubernetes Architecture:\n\nKubernetes (K8s): An open-source platform for automating deployment, scaling, and management of containerized applications.\nCluster: A set of worker machines (nodes) that run containerized applications managed by Kubernetes.\nControl Plane: The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle - of containers.\nNode: A worker machine in Kubernetes, which may be a virtual or physical machine.\nMaster Node: A node that controls the Kubernetes cluster.\nWorker Node: A node that runs applications and workloads.\nkubelet: An agent that runs on each node to ensure containers are running in a Pod.\nkube-proxy: A network proxy that runs on each node to maintain network rules.\netcd: A consistent and highly-available key-value store used as Kubernetes‚Äô backing store for all cluster data.\nContainer Runtime Interface (CRI): The primary protocol for communication between kubelet and container runtime.\n\nKubernetes Resources:\n\nPod: The smallest deployable unit in Kubernetes that can contain one or more containers.\nDeployment: A resource that provides declarative updates for Pods and ReplicaSets.\nReplicaSet: A resource that ensures a specified number of pod replicas are running at any given time.\nService: An abstraction which defines a logical set of Pods and a policy by which to access them.\nNamespace: A mechanism to divide cluster resources between multiple users or projects.\nConfigMap: A resource that stores non-confidential data in key-value pairs.\nSecret: A resource that stores sensitive information such as passwords and tokens.\nVolume: A directory accessible to all containers in a pod, which may be backed by various storage types.\nIngress: A resource that manages external access to services in a cluster, typically HTTP.\nStatefulSet: A resource used to manage stateful applications.\n\nKubernetes Management:\n\nkubectl: The command-line tool for interacting with a Kubernetes cluster.\nkubeadm: A tool for creating and managing Kubernetes clusters.\nHelm: A package manager for Kubernetes that helps install and manage applications.\nManifest: YAML or JSON files that describe Kubernetes resources.\nScaling: Increasing or decreasing the number of replicas of an application.\nSelf-healing: The ability of Kubernetes to automatically replace failed containers.\nRolling Update: A deployment strategy that updates pods one at a time without service interruption.\nBlue-Green Deployment: A deployment strategy that maintains two production environments.\nCanary Deployment: A deployment strategy that releases a new version to a small subset of users.\nAffinity/Anti-Affinity: Rules that influence pod scheduling based on the topology of other pods.\n\nDistributed Computing:\n\nDistributed System: A system whose components are located on different networked computers.\nPySpark: The Python API for Apache Spark, a unified analytics engine for large-scale data processing.\nData Parallelism: A computation pattern where the same operation is performed on different pieces of data simultaneously.\nTask Parallelism: A computation pattern where different operations are performed simultaneously.\nWorker: A node or process that executes assigned tasks.\nMaster: A node or process that coordinates the distribution of tasks to workers.\nJob: A complete computational task, often broken down into smaller tasks.\nTask: A unit of work assigned to a worker.\nDAG (Directed Acyclic Graph): A structure used to represent the sequence of operations in a job.\nResource Allocation: The process of assigning computational resources to jobs or tasks.",
    "crumbs": [
      "Guides",
      "Linux Server & Raspberry Pi"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html",
    "href": "pages/guides/posts/quarto.html",
    "title": "Quarto",
    "section": "",
    "text": "A non-exhaustive guide on using Quarto for project documentation and personal branding.",
    "crumbs": [
      "Guides",
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#overview",
    "href": "pages/guides/posts/quarto.html#overview",
    "title": "Quarto",
    "section": "Overview",
    "text": "Overview\nQuarto is:\n\n\nAn open-source scientific and technical publishing system\nAuthor using Jupyter notebooks or with plain text markdown in your favorite editor.\nCreate dynamic content with Python, R, Julia, and Observable.\nPublish reproducible, production quality articles, presentations, dashboards, websites, blogs, and books in HTML, PDF, MS Word ePub, and more.\nShare knowledge and insights organization-wide by publishing to Posit Connect, Confluence, or other publishing systems.\nWrite using Pandoc markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more.\n\n\nDownloading and Updating\nFor simple instructions and a download/install guide using a GUI, visit Quarto - Get Started.\nFor MacOS users, I recommend downloading and learning about Homebrew, the package manager. It drastically simplifies all phases of package management. To install, simply use brew install quarto and you‚Äôre done.",
    "crumbs": [
      "Guides",
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#projects",
    "href": "pages/guides/posts/quarto.html#projects",
    "title": "Quarto",
    "section": "Projects",
    "text": "Projects\nThis section, and the rest of the guide, assume you‚Äôre familiar with and using the uv package and project manager for Python, git for version control, and the GitHub CLI for collaboration. I‚Äôll be referencing all of these tools throughout the rest of the guide. You can read my guide to learn more about uv\n\nGeneral Workflow\nI‚Äôll be walking through the general workflow, but here‚Äôs a quick note about how I use Quarto for Data related projects. I use GitHub as my collaboration/repo hosting tool, so all of my projects have a README.md file. That way, if anyone visits the actual repo, they can view a nicely rendered markdown file, but when I‚Äôm ready to add a project to my website, I‚Äôll copy the contents into a .qmd file. Then, I can add the Quarto specific formatting.\nThis simplifies my general workflow a lot, and makes it easy to formally share and document my research.\n\n\nInitializing a Project\n\nThe create command\nI‚Äôm going to assume you‚Äôve already run the uv init command to initalize your uv project. From there, it‚Äôs easy to start a project with Quarto from the command line, and there are a few built-in project types to further simplify the startup process. Furthermore, Quarto provides a simple command for creating (or initializing) a project (or extension), quarto create project, and a handy setup guide to help you use it. The following code shows you my terminal input and outputs.\nchriskornaros@chriss-air test % quarto create project\n? Type ‚Ä∫ default\n? Directory ‚Ä∫ docs\n? Title (docs) ‚Ä∫ test_docs\nCreating project at /Users/chriskornaros/Documents/test/docs:\n  - Created _quarto.yml\n  - Created .gitignore\n  - Created test_docs.qmd\n? Open With ‚Ä∫ (don't open)\nFor a quick run through: quarto create project initializes a quarto project directory within your current working directory (the uv parent directory), type lets you choose the type of Quarto documentation (book, website, confluence, etc.), title is the title of your homepage (.qmd) file. Personally, I like to remove the docs/.gitignore file because uv creates one when you initialize a project, in the parent directory. So, having just one .gitignore file helps me keep track of things more easily.\nThe only directories I added to docs after it was created by quarto, was a pages directory for various subpages and a brand directory for .scss files, images, etc. For project, blog, or guide specific media files, I kept those within their subpage folder. Here, I keep the various landing pages and their sub directory structures. Ideally, I won‚Äôt have any files in there, but the _quarto.yml file will point to their locations in my personal GitHub repo.\n\n\n\n\n\n\nFile Context in Quarto\n\n\n\nIn my time developing this site, it seems that Quarto can only pickup on files within the context of the docs (or whatever you name your Quarto project) folder. Furthermore, it struggels with absolute context paths, and at most I could get it to work with ../../file.\n\n\n\n\n\nWorking on your Project\nNow that you‚Äôve initalized your project directory, you can begin work! Head over to Quarto Basics for documentation on the basics of .qmd files and authoring with Quarto markdown.\nJust remember, every webpage will need a .qmd file!\n\n\nRendering a Projects\nThis part is blank for now. Rendering websites have some specific components to websites and GitHub pages, that are covered later on. I will update this for other document types in the future.",
    "crumbs": [
      "Guides",
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#configurations",
    "href": "pages/guides/posts/quarto.html#configurations",
    "title": "Quarto",
    "section": "Configurations",
    "text": "Configurations\n\nThe _quarto.yml file\nThis YAML file serves as the primary configuration file for your Quarto project. Similar to other Quarto YAML files, this handles document configurations, but adds the Quarto project features to sync those across documents and for more environment control. You have the ability to define project metadata for all of the different document types. In this example, I used it to define the website configurations, but if you‚Äôre working on a book or dashboard, then it could be used to normalize chapters or visuals as well.\nYou can also specify the formatting, which connects with the _brand.yml file and enables cross referencing of variables and values. Learn more with Quarto Projects.\n\n\nThe _brand.yml File\nThis is a new feature with Quarto 1.6 that allows you to define and save your design specifications in a YAML file. While this file is specific to your Quarto project directories, you can store and share the file across projects or with others to maintain brand consistency. Luckily, there is great documentation if you want more details brand.yml. While there is a lot to cover, I‚Äôll go over some basics to get started. It‚Äôs important to remember that if you specify colors for anything within .qmd files, those will overwrite the defaults in the brand file. Furthermore, Quarto and _brand.yml both utilize the Bootstrap web development framework. For a list of its full default values, visit the repo.\n\nColor\nThis is obviously an important part of all branding. There are two main components:\n\npalette\ntheme colors\n\nPalette lets you specify hexcodes and assign those to various strings. Those string values could be generic terms, like green (if there is a specific shade you would like), or terms specific to brand.yml's theme colors. When you set your default colors in this way, you can then customize the output in the _quarto.yml file. To modify, for example, your navigation bar, just define the background and foreground properties under the navbar property.\nAnother thing to keep in mind with color, just because it‚Äôs available in _brand.yml, like tertiary, doesn‚Äôt mean it‚Äôs defined and functional in the _quarto.yml file. So, you may need to be creative with how you use protected terms, like success, danger, or warning. Doing so allows you to take advantage of the programmatic benefits of the brand file, while specifying several, possibly, similar shades that would be tricky to do just be renaming colors, such as red, blue, or yellow.\nIf you aren‚Äôt sure on what colors or palettes to choose, using an LLM based chatbot can be helpful. This allows you to describe the colors and themes you‚Äôre going for, as well as refine them over time.\n\n\nTypography\nThis section lets you control which font families are included in your Quarto project. Then, you can specify where various fonts are used and for some properties, even change their specific color. As a heads up, the _brand.yml documentation seems to be correct and updated; however, bash code blocks don‚Äôt render the monospace-background the same way. So, while in-line monospace backgrounds and monospace backgrounds for Python (at the very least) will be colored as the documentation says. Bash code blocks will have no background, just the code itself in the specified font color.\n\n\nDefaults\nThis section gives you more control over various defaults, for HTML Bootstrap, Quarto, Shiny, etc. When configuring specific design colors, using the bootstrap default section will allow you to keep your Quarto files simple, while providing a high level of control over design.\n\n\nSASS - Syntactically Awesome Style Sheets\nRemember, whatever you can‚Äôt configure simply in your _brand.yml file, you can do so in a .scss file. For example, if you want to create custom light and dark mode themes, just create .scss files with the appropriate code and place this in your docs (main Quarto project) directory. Below is an example of a dark mode theme. I set the default values for the scss bootstrap variables at the top. Then, I specified the specific rules for various parts of the page. For defined variables, blockquote, you don‚Äôt need a ., but for features specific to quarto rendered sites, add a . before. For example, to modify the look of code blocks, you must use the .sourceCode variable. For child classes, for example the .sourceCode css copy-with-code class, if you want to modify that you‚Äôll need to use .sourceCode pre.copy-with-code. To find out the name of a variable you don‚Äôt know, just inspect the specific element on the webpage, and the class name will translate 1:1 with the variable name. Additionally, for any property that you need to specifically update, you can add the !important tag, which means it will override existing rules, but be careful using this.\nFor a list of all CSS variable properties, visit CSS Web Docs.\n/*-- scss:defaults --*/\n$background: #2E4053;\n$foreground: #F7F7F7;\n$primary: #FF9900;\n$secondary: #56B3FA;\n$tertiary: #655D4B;\n$light: #F7F7F7;\n$dark: #1C1F24;\n$success: #33373D;\n$danger: #1A1D23;\n$info: #56B3FA;\n$warning: #FF7575;\n\n\n/*-- scss:rules --*/\nbody {\n  font-family: 'Open Sans', sans-serif;\n  background-color: $background;\n  color: $foreground;\n}\n\nh1, h2, h3 {\n  color: $danger;\n}\n\nh4, h5, h6 {\n  color: $danger;\n  font-weight: bold;\n}\n\nblockquote {\n  background-color: #2E6490; /* added background color */\n  border-color: $dark;\n  color: $danger !important;\n}\n\ncode {\n  background-color: $success;\n  color: $info;\n}\n\n.sourceCode {\n  background-color: $success;\n  color: $info;\n}\n\n.sourceCode pre.code-with-copy{\n  padding: 0;\n}\n\n.callout-title-container.flex-fill {\n  color: $danger;\n}\n\n\n\n\n\n\nCSS Variable Names\n\n\n\nThere are some weird naming convention differences between _brand.yml and Quarto. The big one is monotone being used to reference block quotes, code blocks, and in-line code in _brand, but in Quarto it renders the in-line code as code and the code blocks as sourceCode. Make sure to use inspect element to be sure on what you‚Äôre changing. CSS class names can get long, especially when referncing nested classes, just experiment and take your time with things.",
    "crumbs": [
      "Guides",
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#websites",
    "href": "pages/guides/posts/quarto.html#websites",
    "title": "Quarto",
    "section": "Websites",
    "text": "Websites\nWebsites really stretch and push the boundaries of what you can accomplish with Quarto. In this section, I‚Äôll walk through a few key points of developing them.\n\nBlogs\nBlogs are a special kind of Quarto website that consists of a collection of posts along with a navigational page that lists them in reverse chronological order. Pretty much all of the information you‚Äôll need about blogs is the same as the parts of this guide covering websites. Just know, it‚Äôs easy to integrate a blog as a subpage of a larger website.\nSimply add the blogs project structure as a subdirectory of pages/. To keep track of things, I made the title of the main blog page blogs.qmd, so it doesn‚Äôt conflict with the index.qmd that is the home page of my whole website. Then, I added post categories within the posts/ directory of the Quarto blogs/ directory.\nThat being said, and I‚Äôm not quite sure why, but the _metadata.yml file\n\n\nRendering Websites\nI run the following code block from my main project directory. My Quarto project directory is a folder called docs. So, I specify to Quarto that I want to render the entire Quarto project docs, but quarto render‚Äôs context is specific to the quarto project directory. Therefore, I need to use the . to specify that I want the rendered .html files put in the Quarto project folder, and sub folder.\nquarto render docs --output-dir .\nConversely, you can specify, within the output property of your _quarto.yml file that output-dir: .\nThis is also the same syntax when previewing your website, using quarto preview docs, the difference is there is no need to specify an output directory. What this does is spin up a jupyter kernel to render your .qmd files, then, it displays the output in a browser. When you hit save on your _quarto.yml, .scss, and .qmd files then the site will automatically update (it doesn‚Äôt for _brand.yml saves).\nOnce you‚Äôve rendered your website, and pushed the commit, the change is reflected in a few mintues.\n\n\n\n\n\n\nquarto preview with uv\n\n\n\nThe ease of using quarto preview is magnified when using uv as your project/package manager. Instead of having to manage various virtual environments and packages, as well as activation and deactivation, uv does it all. Even VS Code picks up on the context uv provides. The terminal will automatically realize you‚Äôre in a uv environment and display output as if you were using a virutal environment (even though you haven‚Äôt activated it).\n\n\n\n\nWebsite Navigation\n\nTop Navigation\nAfter you‚Äôve set your default color values in _brand.yml, make sure to specify the design details at the top of your navbar property. This is useful, even when using .scss files for more specific design control because you can utilize those variables in your light and dark themes.\nFor pages on your top navigation bar that just have a landing page, simple use the following syntax\nnavbar:\n  left:\n    text: \"Page name\"\n    href: path/to/page/file.qmd\n\n\n\n\n\n\nDashes and Intentation Matter in YAML\n\n\n\nNotice when I‚Äôm using a - and not. This is deliberate. In my development, I realized that where you use and specify the dash can affect functionality. Some places require it, some don‚Äôt, and it may depend on the order of various parameters.\n\n\nFor page categories that may have several landing pages, or even subcategories, you‚Äôll need to utilize hybrid navigation which combines Top and Side navigation. On the top, you‚Äôll use the following syntax:\nnavbar:\n  left:\n    text: \"Page group name\"\n    menu:\n      - path/to/page/group/landing.qmd\n      - path/to/page/group/1/landing.qmd\n      - path/to/page/group/2/landing.qmd\nThen, you‚Äôll need to handle the rest in Side Navigation; however, it isn‚Äôt perfect. You can‚Äôt have nested drop down options in your top navigation bar, so the best I came up with was having a landing page for the top level and first tier subcategories, then handled the rest on the sidebar (which only pops up on affiliated pages).\n\n\nSide Navigation\nFor some reason, Side Navigation in Quarto is much more robust and intuitive. That being said, by combining features here with the top bar, you can achieve a fairly dynamic navigation experience.\nThere are a few key differences. To start with, sidebar objects inherit properties from the first defined, so long as none are changed. Second, you‚Äôll want to use an id with the top level landing pages, because this allows you to reference those in your top navigation bar (for more advanced integrations) using the address sidebar:id, although I struggled with this functionality and didn‚Äôt end up using it.\nThe general structure for your first page group is as follows.\nsidebar:\n  - id: guides\n    title: \"Guides\"\n    style: \"docked\"\n    background: dark\n    foreground: light\n    collapse-level: 2\n    contents:\n      - section: \"Guides\"\n        href: pages/guides/guides.qmd\n        contents:\nNow, if that‚Äôs where things end, you could just list pages on and on using the text: href: syntax. That being said, you probably are going to have a few subcategories, and possibly even further nested subcategories. To enable this, don‚Äôt use the text: syntax, instead use section:. This tells Quarto that you are defining a section, rather than just one single page. As you might guess, you can further nest sections, or specific pages, depending on your use of text: and section: with href:. See an example below.\nid: projects\n      title: \"Projects\"\n      contents:\n        - pages/projects/projects.qmd\n        - section: \"Data Engineering and Architecture\"\n          href: pages/projects/data_engineering/data_engineering.qmd\n          contents:\n            - text: \"Bank Marketing ETL\"\n              href: pages/projects/data_engineering/posts/bank_etl.qmd\n            - text: \"Open Source Data and Analytics Architecture\"\n              href: pages/projects/data_engineering/posts/oss_data_arch.qmd\n            - text: \"Basic Open Source Architecture\"\n              href: pages/projects/data_engineering/posts/basic_oss.qmd\nFor subsections, the landing page‚Äôs .qmd file should be specified within an href paramter, under the section line. Additionally, for the collapsable functionality to work consistently in a sidebar, you‚Äôll need it docked. The behavior is inconsistent with floating sidebars. After you‚Äôve set your default color values in _brand.yml, make sure to specify the design details at the top of your sidebar property. Having a section provides the dropdown functionality on your sidebar.\n\n\n\nSharing Websites\nThere are two primary ways to publish your website once you‚Äôre done making edits, assuming you‚Äôre also using GitHub Pages.\n\nquarto render docs\nquarto publish docs\n\nFor simplicity, I chose to use quarto render docs (note that docs is used here because that‚Äôs the name of my main quarto project directory, not because it‚Äôs part of the command itself) because all I need to do is that and then push the changes. With quarto publish docs, it appeared to me that I would need to setup a branch for my git repository and possibly GitHub actions. I will probably do this in the future, for learning purposes, but didn‚Äôt want to for the sake of time.\nThat being said, the official documentation is very straightforward, and regardless of what you choose, there are two common steps:\n\ntouch .nojekyll\n\nThis tells GitHub pages not to do any additional processing of your website, include this in your docs directory\n\nIn a browser go to GitHubPagesRepo &gt; Settings &gt; Code and automation &gt; Pages\n\nThen, make sure Source is set to Deploy from a branch\nSet your branch to the quarto project directory, in your main project folder, docs in my case\n\n\nThen the classic:\n\ngit add docs\ngit commit -m \"Website updates.\"\ngit push\n\n\n\nWebsite Tools\nQuarto offers several out of the box tools to enhance websites. Some of these are incredibly common for marketing or sharing your content, but all add value in their own way: Google Analytics, Twitter Cards, Open Graph, and RSS Feeds to name a few. The nice thing is they are incredibly easy to setup, and begin working immediately. Google Analytics for example tracked me when I was testing changes in the preview mode.\nThat being said, I tried to implement an RSS feed for the website and it broke Quarto. I was still able to render the output, but I was receiving the ‚ÄúSource cannot be Target‚Äù error. To be able to use quarto preview and quarto render (and get a successfull STDOUT) again I had to remove the robot.txt file, the sitemap.xml file, and the feed: true property from the blog landing page files (my guides.qmd for example).",
    "crumbs": [
      "Guides",
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#other-notes",
    "href": "pages/guides/posts/quarto.html#other-notes",
    "title": "Quarto",
    "section": "Other Notes",
    "text": "Other Notes\nI‚Äôll update this section with more notes and tips that come to mind as I finish building out the site, version 1.0. Then, I‚Äôll reorganize what goes here into the proper places on the document.\n\nIf you want to use past .ipynb files as documentation, or add longer write ups to those files, there is a jupyter command\n\njupyter nbconvert file.ipynb --to markdown --output file.md\nmv file.md &gt; file.qmd\nDone! Just make any quarto specific modifications that you need",
    "crumbs": [
      "Guides",
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#conclusion",
    "href": "pages/guides/posts/quarto.html#conclusion",
    "title": "Quarto",
    "section": "Conclusion",
    "text": "Conclusion\nNow, you‚Äôre all done with this guide, thank you for reading!\nCurrently, this is only updated to include my notes and thoughts from when I built my personal website. As I use Quarto to create a variety of document types, I will update this Guide with more. Follow me on Bluesky to stay connected with me and up to date with my work.",
    "crumbs": [
      "Guides",
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/test/test1.html",
    "href": "pages/guides/posts/test/test1.html",
    "title": "Test",
    "section": "",
    "text": "This is a test of the second page.",
    "crumbs": [
      "Guides",
      "Test",
      "Test"
    ]
  },
  {
    "objectID": "pages/guides/posts/test/test.html",
    "href": "pages/guides/posts/test/test.html",
    "title": "Test",
    "section": "",
    "text": "This is a test of the main page.",
    "crumbs": [
      "Guides",
      "Test"
    ]
  },
  {
    "objectID": "pages/guides/posts/docker.html#containerization",
    "href": "pages/guides/posts/docker.html#containerization",
    "title": "Docker: Novice to Journeyman",
    "section": "Containerization",
    "text": "Containerization\n\nIntroduction\nContainerization has revolutionized how we develop, deploy, and run software. At its core, containerization is a method of packaging an application along with all its dependencies‚Äîlibraries, configuration files, and everything else it needs to run‚Äîinto a standardized unit called a container. This container can then be reliably transported and run on any computing environment that supports containers.\nThink of containers like standardized shipping containers used in global logistics. Before standardized shipping containers existed, loading and unloading cargo ships was inefficient and unpredictable‚Äîdifferent-sized crates and packages made storage challenging and transportation slow. The introduction of uniform shipping containers revolutionized global trade by creating a standard unit that could be easily loaded, stacked, transported, and unloaded regardless of what was inside.\nSoftware containers work on the same principle. Instead of shipping physical goods, we‚Äôre packaging software in a way that eliminates the traditional challenge of ‚Äúit works on my machine but not in production.‚Äù Containers encapsulate the application and its environment, ensuring consistent behavior across different computing infrastructures‚Äîfrom a developer‚Äôs laptop to testing environments to production servers.\nWhat makes containers particularly powerful for developers and system administrators is their combination of isolation and efficiency. Unlike traditional virtual machines that virtualize an entire operating system, containers share the host system‚Äôs kernel while maintaining strict isolation between applications. This makes them significantly more lightweight and faster to start than VMs, while still providing the necessary separation between applications.\nFor servers, containerization offers several benefits:\n\nResource Efficiency: Containers have minimal overhead, making them perfect for the limited resources of a Raspberry Pi\nIsolation: Applications in containers won‚Äôt interfere with each other or the host system\nReproducibility: Container definitions are code, making your server setup reproducible and version-controlled\nPortability: The same container can run on your Pi, your MacBook, or any other compatible system\nSimplified Deployment: Containers bundle all dependencies, eliminating complex installation procedures\nEasy Updates: Containers can be replaced rather than updated in-place, simplifying maintenance\n\nIn this guide, we‚Äôll first explore the foundational concepts of containerization to build a solid understanding of the technology. Then, we‚Äôll dive into Docker‚Äîthe most popular containerization platform‚Äîand create a practical setup for a persistent Jupyter notebook server running on your Raspberry Pi. This container will allow you to use your MacBook as a client for development while leveraging the compute resources and persistent storage of your Pi server.\nBy the end of this guide, you‚Äôll have both a theoretical understanding of containerization and practical experience implementing it, setting the stage for more advanced container-based projects like PostgreSQL databases, development environments, and even multi-container applications. It will help to checkout my starter guide on setting up Linux Server LTS on a Raspberry Pi, if you want to know exactly what kind of system I‚Äôm using. Otherwise, most of the examples will focus on Docker, not my specific environment.\n\n\nBasic Concepts\n\nKey Terms\n\nContainer: A lightweight, standalone, executable package that includes everything needed to run a piece of software: code, runtime, system tools, libraries, and settings.\nImage: A read-only template used to create containers. Images contain the application code, libraries, dependencies, tools, and other files needed for an application to run.\nContainer Engine: Software that accepts user requests, including command line options, pulls images, and uses the operating system‚Äôs functionality to create and manage containers.\nNamespace: A Linux kernel feature that partitions system resources so that one set of processes sees one set of resources while another set of processes sees a different set of resources.\nControl Group (cgroup): A Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, etc.) of process groups.\nHost: The physical or virtual machine on which containers run.\nRegistry: A service that stores and distributes container images, similar to a Git repository for code.\nLayer: Part of an image that represents a set of filesystem changes. Images are built from a series of layers, making them efficient to store and transfer.\nVolume: A designated directory in the container that exists outside the default Union File System, used for persisting data or sharing data between containers.\nPort Binding: Mapping a container‚Äôs port to a port on the host machine, allowing external access to services running inside the container.\n\n\n\nHow Containers Work\nContainers achieve their isolation and efficiency through several key Linux kernel features, primarily namespaces and control groups (cgroups). Namespaces create isolation by providing processes with their own view of system resources. Linux implements various namespace types:\n\nPID Namespace: Isolates process IDs\nNetwork Namespace: Isolates network interfaces\nMount Namespace: Isolates filesystem mount points\nUTS Namespace: Isolates hostname and domain name\nIPC Namespace: Isolates interprocess communication resources\nUser Namespace: Isolates user and group IDs\n\nWhen a container starts, it gets its own set of these namespaces, making it appear to the application inside that it has its own isolated instance of the operating system.\n\nControl Groups (cgroups): Provide resource limitation and accounting. They ensure containers can only use allocated amounts of system resources like CPU, memory, and I/O. This prevents a single container from consuming all available resources and affecting other containers or the host system.\nUnion File Systems: Another key technology behind containers. They create layers of file system changes, enabling efficient storage and quick creation of containers. When a container is built from an image, each instruction in the image definition typically creates a new layer. These layers are cached, meaning unchanged layers can be reused between different images, saving both disk space and build time.\n\n\n\nContainers vs.¬†Virtual Machines\nA common point of confusion for newcomers is how containers differ from virtual machines (VMs). Both provide isolation, but they work in fundamentally different ways.\nVirtual Machines:\n\nRun a complete operating system with its own kernel\nVirtualize hardware resources through a hypervisor\nRequire more storage space and memory\nTake minutes to start up\nProvide strong isolation at the hardware level\n\n\nContainers:\n\nShare the host operating system‚Äôs kernel\nVirtualize at the operating system level, not hardware\nRequire minimal storage space and memory\nStart in seconds or milliseconds\nProvide process-level isolation\n\n\nThis architectural difference explains why containers are so much more lightweight than VMs. While a typical VM might be gigabytes in size and take minutes to start, a container can be megabytes in size and start in seconds.\n\n\n\nNetworking for Containers\n\nBasic Networking\nUnderstanding container networking is essential for building practical container-based applications. Container networking fundamentally relies on Linux network namespaces, which provide each container with its own isolated network stack including:\n\nNetwork interfaces\nIP addresses\nRouting tables\nFirewall rules\nSocket port numbers\n\nMost container engines support several networking modes:\n\nBridge Networking: The default mode where containers connect to a software bridge on the host, giving them their own IP addresses on an isolated network. Port mappings allow external access.\nHost Networking: Containers share the host‚Äôs network namespace with no network isolation, seeing the same network interfaces as the host. This offers the best performance but reduces isolation.\nNone Networking: Containers have no external network connectivity, useful for processing-only workloads that don‚Äôt need network access.\nOverlay Networking: Creates a distributed network among multiple container hosts, allowing containers on different hosts to communicate as if on the same local network.\nMacvlan Networking: Gives containers their own MAC address, making them appear as physical devices on the network.\n\n\n\nAdvanced Networking\nIn bridge networking, containers communicate freely on their isolated network but need port mapping to be accessible from outside. For example, mapping port 8888 on your Raspberry Pi to port 8888 in a container would let you access a Jupyter server by connecting to your Pi‚Äôs IP address on port 8888.\nContainers typically use DNS for service discovery:\n\nContainer engines often provide built-in DNS resolution between containers\nContainers can usually resolve external domain names using the host‚Äôs DNS configuration\nIn multi-container applications, service discovery systems help containers find each other automatically\n\n\n\nExisting Server Configuration\nMy Raspberry Pi server setup already includes:\n\nNon-standard SSH port (45000)\nUFW (Uncomplicated Firewall) configuration\nFail2ban for protection against brute-force attacks\n\nAny containerized services will need to work with these existing configurations. Later, when we set up our Jupyter container, we‚Äôll need to:\n\nChoose a port that doesn‚Äôt conflict with existing services\nConfigure UFW to allow traffic to this port\nEnsure the container‚Äôs networking integrates with your existing security measures\n\n\n\n\nUsing Containers\nContainerization‚Äôs versatility makes it valuable across various computing scenarios. Here are some common use cases that demonstrate why containers have become so foundational in modern computing.\n\nApplication Development and Testing\nFor developers, containers solve the ‚Äúit works on my machine‚Äù problem by ensuring consistency across development, testing, and production environments. Benefits include:\n\nConsistent Development Environments: Every developer works with identical dependencies and configurations\nFaster Onboarding: New team members can start with a working environment immediately\nParallel Version Testing: Run applications with different dependency versions simultaneously\nContinuous Integration: Test code in clean, reproducible environments\n\nFor example, a development team working on a web application can define their entire stack‚Äîfrom database to web server‚Äîas containers. New developers simply pull the container definitions and start working immediately, rather than spending days configuring their local environment.\n\n\nMicroservices Architecture\nContainers are ideal for microservices, where applications are composed of many small, independent services:\n\nService Isolation: Each microservice runs in its own container\nIndependent Scaling: Scale containers individually based on demand\nTechnology Flexibility: Use different programming languages and frameworks for different services\nSimplified Updates: Update individual services without affecting others\n\nNetflix, for instance, uses containers to manage thousands of microservices that power their streaming platform, allowing them to update and scale individual components without disrupting the entire service.\n\n\nEdge Computing and IoT\nContainers are increasingly used in edge computing and Internet of Things (IoT) scenarios:\n\nResource Efficiency: Containers‚Äô low overhead works well on limited-resource devices\nRemote Management: Deploy and update container workloads remotely\nStandardization: Same container can run in the cloud and at the edge\nIsolation: Run multiple applications on a single edge device securely\n\nMy Raspberry Pi is an example of an edge device that can benefit from containerization, allowing you to run multiple services efficiently on limited hardware.\n\n\nPersonal Projects and Self-Hosting\nFor personal projects and self-hosting, containers offer significant advantages:\n\nApplication Isolation: Run multiple applications without conflicts\nEasy Backups: Back up container volumes or entire container states\nSimple Updates: Update applications by pulling new container images\nResource Management: Limit resource usage for each application\n\n\n\nSpecific Raspberry Pi Use Cases\nFor your Raspberry Pi server specifically, containerization enables:\n\nJupyter Notebooks: Run a persistent Jupyter server for data analysis\nDatabase Servers: Host PostgreSQL, MySQL, or MongoDB without complex setup\nWeb Applications: Deploy web services with proper isolation\nDevelopment Tools: Run Git servers, CI/CD pipelines, or code quality tools\nMedia Services: Host Plex, Jellyfin, or other media servers\nHome Automation: Run Home Assistant, Node-RED, or other automation tools\n\n\n\n\nAdjacent and Complementary Topics\nWhile containers themselves are powerful, they‚Äôre part of a broader ecosystem of technologies and practices. Understanding these adjacent areas will help you get the most from containerization.\n\nContainer Orchestration\nFor managing multiple containers across multiple hosts:\n\nKubernetes: The industry-standard container orchestration platform\nDocker Swarm: A simpler orchestration solution integrated with Docker\nK3s/K3d: Lightweight Kubernetes distributions suitable for Raspberry Pi\nNomad: HashiCorp‚Äôs workload orchestrator supporting containers and other applications\n\nContainer orchestration becomes important when you need high availability, automated scaling, or management of complex multi-container applications.\n\n\nCI/CD (Continuous Integration/Continuous Deployment)\nContainers integrate naturally with modern software development practices:\n\nAutomated Testing: Run tests in clean container environments\nBuild Pipelines: Automatically build container images when code changes\nDeployment Automation: Automatically deploy new container versions\nInfrastructure as Code: Define your entire infrastructure declaratively\n\nTools like GitHub Actions, GitLab CI, Jenkins, and CircleCI all support container-based workflows.\n\n\nInfrastructure as Code\nManaging container environments declaratively:\n\nDocker Compose: Define multi-container applications\nTerraform: Provision and manage infrastructure including container hosts\nAnsible: Automate container deployment and configuration\nHelm: Package and deploy applications to Kubernetes\n\nInfrastructure as Code makes your container setups reproducible, version-controlled, and easier to maintain.\n\n\nMonitoring and Observability\nWith containers, traditional monitoring approaches need adaptation:\n\nContainer Metrics: CPU, memory, network, and disk usage per container\nLogging Solutions: Collecting and centralizing logs from ephemeral containers\nApplication Performance Monitoring: Tracing requests across container boundaries\nService Meshes: Advanced networking with observability features\n\nTools like Prometheus, Grafana, ELK Stack (Elasticsearch, Logstash, Kibana), and Jaeger help monitor containerized environments.\n\n\nSecurity Considerations\nContainer security requires specific attention:\n\nImage Scanning: Detecting vulnerabilities in container images\nRuntime Security: Monitoring container behavior for anomalies\nSecure Supply Chains: Ensuring the integrity of images from source to runtime\nPrivilege Management: Running containers with minimum necessary privileges\n\nSolutions like Trivy, Falco, Notary, and proper security practices help keep containerized environments secure.\n\n\n\nConclusion\nContainerization represents one of the most significant shifts in how we develop, deploy, and run software in recent years. By packaging applications with their complete runtime environment, containers solve the long-standing problem of environment inconsistency while providing resource efficiency and isolation.\nIn this first section, we‚Äôve explored the fundamental concepts behind containers‚Äîhow they use Linux kernel features like namespaces and cgroups to provide lightweight isolation, how they differ from virtual machines, and why they‚Äôve become essential in modern computing. We‚Äôve also looked at various use cases and adjacent technologies that complement containerization.\nWith this foundational understanding in place, we‚Äôre now ready to move from theory to practice. In the next section, we‚Äôll dive into Docker‚Äîthe most popular containerization platform‚Äîand learn how to create, manage, and use containers on your Raspberry Pi server. We‚Äôll build a persistent Jupyter notebook environment that lets you combine the convenience of developing on your laptop with the persistent computing resources of your Raspberry Pi.\nThis practical implementation will make these container concepts concrete while giving you a valuable tool for data analysis, coding, and experimentation‚Äîall within a properly isolated environment that won‚Äôt affect the rest of your server setup.",
    "crumbs": [
      "Guides",
      "Docker: Novice to Journeyman"
    ]
  },
  {
    "objectID": "pages/guides/posts/docker.html#docker",
    "href": "pages/guides/posts/docker.html#docker",
    "title": "Docker: Novice to Journeyman",
    "section": "Docker",
    "text": "Docker\n\nIntroduction\nDocker has revolutionized application development and deployment by providing a standardized way to package, distribute, and run applications in isolated environments called containers. In this comprehensive guide, we‚Äôll explore Docker from basic concepts to advanced implementations, using a Jupyter notebook server as our primary example.\nDocker enables you to package an application with all its dependencies into a standardized unit called a container. These containers can run consistently across different environments, from development machines to production servers, eliminating the classic ‚Äúit works on my machine‚Äù problem. This consistency is particularly valuable when working with complex data science environments like Jupyter notebooks, which often have numerous interdependent libraries and packages.\n\nWhy Jupyter?\nFor our Ubuntu Raspberry Pi server, Jupyter provides an ideal example of Docker‚Äôs capabilities for several important reasons:\n\nComputational offloading: Jupyter allows you to write and execute code on a Raspberry Pi‚Äôs resources while using your laptop as a client interface. This distributes the workload efficiently, leveraging each device‚Äôs strengths.\nPersistence and accessibility: A containerized Jupyter server runs 24/7 on your Raspberry Pi, maintaining long-running calculations and persisting data files while being accessible from anywhere.\nEnvironment isolation: Data engineering workflows often require specific package versions that might conflict with other applications. Docker containers provide perfect isolation for these environments.\nSecurity and controlled access: Jupyter‚Äôs token-based authentication integrates well with Docker‚Äôs networking capabilities, allowing secure remote access.\nScalability pathway: Starting with a single Jupyter container creates a foundation for more complex setups later, such as adding PostgreSQL databases or distributed computing with PySpark.\nCross-platform compatibility: Docker abstracts away many of the ARM-specific considerations of running on Raspberry Pi, though we‚Äôll address key compatibility points.\n\nBy focusing on Jupyter, we‚Äôll cover the most important Docker concepts‚Äîimages, containers, volumes, networking, and orchestration‚Äîin a practical context that directly supports data engineering work. As we progress through each section, you‚Äôll build a functional containerized Jupyter environment that serves as both a learning tool and a practical development platform. Let‚Äôs start by exploring the fundamental concepts of Docker and setting up the necessary tools on your Ubuntu Pi server.\n\n\n\nBasic Concepts\nBefore diving into implementation, let‚Äôs establish a clear understanding of Docker‚Äôs core concepts and terminology. These fundamentals will provide the foundation for everything we build throughout this guide.\n\nKey Docker Components\n\nDocker Engine: The runtime that builds and runs containers. It consists of:\n\nA server (daemon) that manages containers\nREST API that programs can use to communicate with the daemon\nCommand-line interface (CLI) for user interaction\n\nDocker Image: A read-only template containing application code, libraries, dependencies, tools, and other files needed to run an application. Think of an image as a snapshot or blueprint of an application and its environment.\nDocker Container: A runnable instance of an image‚Äîwhat the image becomes in memory when executed. A container runs completely isolated from the host environment, accessing only kernel capabilities and resources explicitly allowed.\nDockerfile: A text file containing instructions for building a Docker image. It specifies the base image, additional components, configurations, and commands to be included.\nDocker Registry: A repository for Docker images. Docker Hub is the default public registry, but private registries can also be used. Images are stored with tags to identify different versions.\nDocker Compose: A tool for defining and running multi-container Docker applications using a YAML file to configure application services, networks, and volumes.\n\n\n\nDocker Architecture\nDocker uses a client-server architecture where the Docker client communicates with the Docker daemon. The daemon handles building, running, and distributing Docker containers. The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon‚Äîmaking it especially suitable for our Raspberry Pi server setup.\nThe Raspberry Pi uses ARM architecture, which differs from the x86/x64 architecture used in most desktop and server computers. This creates some important considerations when working with Docker:\n\nImage compatibility: Docker images are architecture-specific. Many popular images offer ARM variants (often tagged with arm32v7 or arm64v8), but not all do. Always check if images have ARM support before attempting to use them.\nPerformance considerations: Some Docker images may run slower on ARM processors depending on the workload. Computationally intensive operations in containers might experience more significant performance differences compared to x86/x64 architectures.\nBuilding images locally: Building Docker images directly on your Raspberry Pi ensures architecture compatibility but may take longer due to limited resources. For complex builds, consider using Docker‚Äôs BuildKit with multi-architecture support.\nImage size awareness: ARM devices like Raspberry Pi often have storage limitations. Be particularly mindful of image sizes and use lightweight base images where possible.\n\nFor our Jupyter implementation, we‚Äôll address these considerations by selecting ARM-compatible base images and optimizing for the Raspberry Pi‚Äôs resources.\n\n\n\nConfiguring VS Code and Docker\nBefore we install Docker on the server, let‚Äôs set up the development environment on your client machine (MacBook Air for me). VS Code offers excellent Docker and remote development support through extensions, creating a seamless workflow between your local machine and the Raspberry Pi server.\nI recommend installing Docker locally, on your client (laptop), even though the Docker containers will run on your server. You can find client specific installation instructions using a GUI here. Once that‚Äôs done, and you‚Äôve setup your account, continue with the rest of the guide.\n\nEssential VS Code Extensions\nInstall the following extensions in VS Code to enhance your Docker development experience:\n\nContainer Tools extension: Microsoft‚Äôs official extension for building, managing, and deploying containerized applications. This extension replaces the older Docker extension and provides a visual interface for managing containers, images, networks, and volumes. It also offers syntax highlighting and linting for Dockerfiles and docker-compose files.\nDocker DX extension: This extension works alongside Container Tools to deliver a best-in-class authoring experience specifically for Dockerfiles, Compose files, and Bake files. Key features include:\n\nDockerfile linting with warnings and best-practice suggestions from BuildKit and Buildx\nImage vulnerability remediation that flags references to container images with known vulnerabilities\nBake file support with code completion and variable navigation\nCompose file outline view for easier navigation of complex Compose files\n\nRemote - SSH extension: Enables you to use VS Code to connect to your Raspberry Pi over SSH and work with files and terminals directly on the remote machine.\nJupyter extension: Supports working with Jupyter notebooks within VS Code, allowing you to connect to remote Jupyter kernels.\nPython extension: Provides rich support for Python language, including IntelliSense, debugging, and code navigation.\nRuff extension: A fast Python linter that helps maintain code quality by identifying errors, style issues, and potential bugs in your code.\n\nTo install these extensions:\n\nOpen VS Code\nPress Cmd+Shift+X to open the Extensions view\nSearch for each extension by name and click ‚ÄúInstall‚Äù\n\nThe combination of Container Tools and Docker DX creates a comprehensive Docker development environment, with Container Tools handling the runtime aspects (building, running, managing containers) and Docker DX focusing on improving the authoring experience for Docker-related files.\n\n\nConfiguring Docker Integration\nOnce connected to your Raspberry Pi through Remote-SSH, the Container Tools extension will automatically detect the Docker daemon running on the remote machine (after we install it in the next section). This integration provides a seamless Docker management experience:\n\nWith the remote connection active, click on the Container Tools icon in the activity bar (resembling a stack of containers)\nThe Container Tools view will display remote containers, images, volumes, and networks\nThe Docker DX extension will provide enhanced editing capabilities when working with Docker files:\n\nDockerfile editing with real-time linting from BuildKit\nCompose file navigation through the outline view\nContextual suggestions and completions based on your Docker environment\n\n\nThis setup creates a powerful development workflow where you:\n\nEdit Docker configuration files on your MacBook with syntax highlighting, linting, and intelligent suggestions\nBuild and run Docker containers on your Raspberry Pi with visual management\nAccess containerized services (like Jupyter) through either VS Code or a web browser\nMaintain high code quality with Python linting through Ruff\n\nWhen you first connect to your Raspberry Pi, VS Code might prompt you to install some server components. Allow this installation to ensure all extensions work properly in the remote environment. The Docker DX and Container Tools extensions will work together to provide both authoring and runtime capabilities for your Docker workflow.\n\n\n\nConfiguring Docker on the Server\nNow that we‚Äôve prepared our client-side development environment, let‚Äôs install and configure Docker on your Ubuntu Raspberry Pi server. We‚Äôll make sure it integrates properly with your existing server setup, including security configurations.",
    "crumbs": [
      "Guides",
      "Docker: Novice to Journeyman"
    ]
  }
]
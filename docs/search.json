[
  {
    "objectID": "pages/guides/guides.html",
    "href": "pages/guides/guides.html",
    "title": "Guides",
    "section": "",
    "text": "Welcome to the Guides section of the website! This is the landing page for step-by-step guides and instructions for various tools and workflows that I‚Äôve used in the past or am currently exploring.",
    "crumbs": [
      "Guides"
    ]
  },
  {
    "objectID": "pages/guides/guides.html#future-guides-planned",
    "href": "pages/guides/guides.html#future-guides-planned",
    "title": "Guides",
    "section": "Future Guides (Planned)",
    "text": "Future Guides (Planned)\nHere are some topics I plan to cover in the future:\n\nDuckDB\ndbt\nPostgreSQL\n\nStay tuned for updates!",
    "crumbs": [
      "Guides"
    ]
  },
  {
    "objectID": "pages/guides/guides.html#current-guides",
    "href": "pages/guides/guides.html#current-guides",
    "title": "Guides",
    "section": "Current Guides",
    "text": "Current Guides",
    "crumbs": [
      "Guides"
    ]
  },
  {
    "objectID": "pages/guides/posts/uv.html",
    "href": "pages/guides/posts/uv.html",
    "title": "uv, the Python Project and Package Manager",
    "section": "",
    "text": "A basic guide on using uv the package and projects manager for Python developers.",
    "crumbs": [
      "Guides",
      "uv"
    ]
  },
  {
    "objectID": "pages/guides/posts/uv.html#introduction",
    "href": "pages/guides/posts/uv.html#introduction",
    "title": "uv, the Python Project and Package Manager",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\nVS Code Shortcuts\n\n\n\nIf you‚Äôre using VS Code, here are some useful shortcuts. - Note, use CMD-K CMD-S to open the keyboard shortcuts. - SHFT-CMD-i inserts a code block\n\n\nuv is an Open Source project by Astral, the makers of ruff, that is self described (and worthy of the title) as an extremely fast Python package and project manager, written in Rust.\n\nüöÄ A single tool to replace pip, pip-tools, pipx, poetry, pyenv, twine, virtualenv, and more.\n‚ö°Ô∏è 10-100x faster than pip.\nüêç Installs and manages Python versions.\nüõ†Ô∏è Runs and installs Python applications.\n‚ùáÔ∏è Runs scripts, with support for inline dependency metadata.\nüóÇÔ∏è Provides comprehensive project management, with a universal lockfile.\nüî© Includes a pip-compatible interface for a performance boost with a familiar CLI.\nüè¢ Supports Cargo-style workspaces for scalable projects.\nüíæ Disk-space efficient, with a global cache for dependency deduplication.\n‚è¨ Installable without Rust or Python via curl or pip.\nüñ•Ô∏è Supports macOS, Linux, and Windows.\n\nI‚Äôm only just beginning to learn and use the tool in my own projects (including converting my existing project environments to uv) and from what I‚Äôve seen it‚Äôs going to make life much easier. That being said, while you overwrite the muscle memory developed for years with pip and venv, there will be some growing pains; however, for those who are less familiar with what I‚Äôm talking about, I‚Äôll still explain some basic concepts and snags that I both run and ran into.",
    "crumbs": [
      "Guides",
      "uv"
    ]
  },
  {
    "objectID": "pages/guides/posts/uv.html#basic-workflow-and-guide",
    "href": "pages/guides/posts/uv.html#basic-workflow-and-guide",
    "title": "uv, the Python Project and Package Manager",
    "section": "Basic workflow and guide",
    "text": "Basic workflow and guide\n\nConcepts to Know Before Getting Started\n\nBasic knowledge of directories, bash (zsh in the case of MacOS), and using the CLI bash\nBasic knowledge of Python, common project structures, and simple workflows Python\nBasic knowledge of git (for local version control) and GitHub (for collaboration) git and GitHub basics\n\n\n\nInitializing a Project\n\nLocal Repository\nThe nice thing about uv is that it‚Äôs designed to make Python development easier, so there aren‚Äôt any head-scratching gotchas.\nFor the sake of this example and entire template, let‚Äôs assume I‚Äôm currently sitting in my main directory. For some that might be home, others app, for MacOS the default is /usr/yourusername, or maybe you prefer to put all projects in a Documents or Projects folder. Anyways, to start up a project you can do one of two things:\n\nHave uv do everything, and then change directories\n\nuv init uv_basic\ncd uv_basic\n\nCreate the directory, change directories, and then have uv do everything\n\nmkdir uv_basic\ncd uv_basic\nuv init\n\n\nThis will create 4 files and initalize a local git repository:\n\n.python-version\n.pyproject.toml\nhello.py\nREADME.md\n.git\n.gitignore\n\n\n\n\n\n\n\nuv and the .gitignore file\n\n\n\nThe nice thing about uv is that it autopopulates your .gitignore file with a few files and patterns, not to mention, it provides some basic tagging for what it puts in there. Just open the file (it‚Äôs plain text) to see. Since I‚Äôm saving my progress with this repo using git, I want to keep the overall file size down. So, I also included the .html and .ipynb file that Quarto generates because they can get large fast. Additionally, when you initialize your GitHub repo with the CLI‚Äôs repo creation process, I don‚Äôt include a README or .gitignore, because those are included in uv init.\n\n\n\n\nRemote Repository\nFor anyone familiar with software development you‚Äôve probably heard of GitHub or GitLab. I‚Äôm more familiar, professionally and personally, with GitHub (which is what I‚Äôll be using in this example); however, there are a large amount of people that prefer GitLab because it is better for some enterprise and personal use cases‚Äì GitHub vs.¬†GitLab. For this, you‚Äôll want to install the GitHub CLI. Then, you can follow along.\n\nVerify the installations and make sure to get your credentials setup, in git\n\nwhich gh and which git\n\nAdd your name and email\n\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your-email@example.com\"\n\nAuthenticate access to GitHub\n\ngh auth login\nUsing the CLI option, follow the instructions\nSelect HTTPS for the easier connection option\n\nVerify you have proper access to your GitHub\n\ngh auth status\n\n\ngithub.com\n  ‚úì Logged in to github.com account itsmeChis (keyring)\n  - Active account: true\n  - Git operations protocol: https\n  - Token: gho_************************************\n  - Token scopes: 'delete_repo', 'gist', 'read:org', 'repo', 'workflow'\n\ngh repo list\nAssuming you haven‚Äôt, create your project repo from the CLI (you can also do so using the GitHub.com GUI, but I prefer this way to reinforce my learning)\n\ngh repo create\nCreate a new repository from scratch\nuv basic\noptional description\nPublic\nGNU Affero General Public License v3.0 Which license do you need? \n\nSet the newly created repo as the local git repo‚Äôs upstream\n\nThis will result in an error (git pull)\nSet the global config to merge git pull\ngit pull with a commit message\ngit status to verify\ngit push\n\n\n\n\n\nAdding and managing dependencies\nThus far, the workflow with uv isn‚Äôt too dissimilar from using pip and venv, but managing dependencies and testing scripts is where uv shines. As you‚Äôll see below, with pip and venv, you have to manually create the virtual environment, activate it, install dependencies, manage requirements files, and then run your script. With uv, however, almost all of that is done automatically and things like uv pip list or uv venv are only there for backwards compatibility. A lot of the tedious pieces of the DevOps workflow are now obsolete or handled in the background.\n\nUsing pip and venv\nWhen using a combination of pip and venv, your typical workflow is straightforward, but becomes complicated if you need to uninstall certain packages or make quick, iterative tests of code.\nmkdir uv_basic\ncd uv_basic\npython -m venv .venv\nsource .venv/bin/activate\npip install duckdb\npip install numpy\npip freeze &gt; requirements.txt\npython script.py\n\n# Realize you don't need numpy, so you want to uninstall it and keep your environment cleaner\ndeactivate\nrm -r .venv\npython -m venv .venv\nsource .venv/bin/activate\n# Two options here, delete numpy from requirements.txt, not scalable with many packages, or reinstall just duckdb, also not scaleable\npip install duckdb\npip freeze &gt; requirements.txt\npython script.py\nAs you can see, the initial workflow isn‚Äôt horrible, but if you need to make a change to the environment or just want to test something small, the number of steps quickly multiplies.\n\n\nUsing uv\nCompare that with the streamlined uv workflow.\nuv init uv_basic\ncd uv_basic\nuv add duckdb\nuv add numpy\nuv run script.py\nuv remove numpy\nuv run script.py\nThe workflow improvements and efficiency should be obvious. The nice thing is that uv functions as your standalone virtual environment, without the need for activation or deactivation. Using uv add will add a dependency to both your pyproject.toml file and your uv.lock file. Additionally, if you are more familiar with verifying using pip, running uv pip list will show that the package is there (although the pip functionality is obsolete and only for backwards compatibility at this point). If you want to remove a package, simply use uv remove and that will also remove it from the .toml and .lock files. The last feature you‚Äôll need to understand (to use uv at a basic level) is uv sync. Simply put, it syncs your environment with the project‚Äôs dependencies/lock file. This ensures that the exact versions specified in your lockfile are used in your environment‚Äì dependencies may be added, removed, or updated if there are updates to the declared dependencies.\nTo cap this off, here are some common use cases for uv sync: - Run uv sync (without ‚Äìfrozen) to keep dependencies up-to-date and to resolve changes. - Use uv sync ‚Äìfrozen to validate dependencies without altering them\n\n\n\nConverting your Legacy Projects to uv\nNow that you‚Äôve seen the benefits of uv, as well as the workflow differences, you probably want to give it a try or even convert entire projects to uv. The good news is that this is simple and only requires a few modifications to get things up and running. The general workflow is the same as I outlined above, you‚Äôll just be cleaning up your local environment and reinstalling things along the way. The project I converted to use uv for this example utilizes DuckDB and dbt for the database and data modeling/ETL. I‚Äôll include some dbt specific information, for example if you move your database file from a subdirectory to the main one, remember to update your dbt profiles in your global dbt location.\n\nChange directories to your specific project directory\nRun uv init, it will create any file or folder that isn‚Äôt currently in the main folder\n\nIf you already have a .git folder and commit history, uv will not delete or overwrite the original folder.\n\nAdd all of the dependencies you need, then remove your requirements file (it‚Äôs no longer needed)\n\nAs of writing this, I wasn‚Äôt sure how to use uv add with the legacy requirements file, uv pip install -r kind of worked, but didn‚Äôt actually add the dependencies to the .toml or .lock files\nThere must be an easier way to bulk add dependencies, but I manually did it\nIn my case, I had to remember to add both dbt and dbt-duckdb, so the adapter would work\n\nInstall all of the CLI tools that you need, and don‚Äôt want or use globally\n\nIn my case, I need jupyter, quarto, and dbt, but I also have the latter two installed globally\n\nVerify that uv can run things correctly\n\nI first used uv run hello.py to verify that the basic functionality is there\nThen, I ran a more complex script, that imports and uses duckdb, to ensure the packages are installing and running as intended\nThen, I used uv tool list to verify which CLI tools are installed\nFinally, I verified that the CLI tools work, by using uv run dbt run --select transform to test dbt model functionality in uv\n\n\n\n\nFinal Thoughts\nSo that‚Äôs it! Overall, uv is incredibly easy to setup and configure because it builds on the classic workflows, while simplifying or abstracting some of the process. You also saw how easy it is to start using uv with older projects that use the legacy workflow. At the time of writing this, I‚Äôve only been using uv for a few days, so I‚Äôm sure there are things I got wrong or missed, please comment to let me know!\nI‚Äôm happy to chat and love learning about data, as well as what folks in this space are working on. Connect with me on Bluesky @chriskornaros.bsky.social to follow along with what I‚Äôm working on, learning, or just to say hi! Below are some other notes and thoughts I had while working on this write up.\n\nGeneral Notes\n\nIt seems that while tools are specific to a uv project instance (i.e.¬†uv_basic returns the .venv dir when asking which jupyter, but test before intalling anything say it can‚Äôt be found), when you use uv tool install it installs it to the system wide uv\nuv pip list defaults to the global (non-uv or non-pip) python environment (in my case it‚Äôs pip and wheel), but once you install something (using add, pip install, etc.) it switches the context to the current parent uv dir (i.e.¬†test, instead of uv_basic)\n\nTools are still listed even after this\n\nuv tool install only works when installing python package specific tools, but DuckDB for Python (for example) doesn‚Äôt come packaged with the DuckDB CLI tools, so uv tool install duckdb won‚Äôt install the DuckDB CLI features\nIt seems that saving variable with duckdb.sql(‚Ä¶).show() and then printing the type of that, just prints the query output, insteaed of the type\nBased on tests, the workflow changes are as follows",
    "crumbs": [
      "Guides",
      "uv"
    ]
  },
  {
    "objectID": "pages/projects/projects.html",
    "href": "pages/projects/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Landing page for the project portfolio portion of this website. Contains all of my public repositories and projects (for now, may include future consulting or paid side work, but I don‚Äôt do that at the moment), including both the code in repositories and write ups (where applicable).\nCurrently, there are two categories of projects I‚Äôm working on:\n\nData Engineering and Architecture\nData Science and Machine Learning",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/bank_etl.html",
    "href": "pages/projects/data_engineering/posts/bank_etl.html",
    "title": "Bank Marketing ETL Project",
    "section": "",
    "text": "Piggy bank\nPersonal loans are a lucrative revenue stream for banks. The typical interest rate of a two-year loan in the United Kingdom is around 10%. This might not sound like a lot, but in September 2022 alone UK consumers borrowed around ¬£1.5 billion, which would mean approximately ¬£300 million in interest generated by banks over two years!\nYou have been asked to work with a bank to clean the data they collected as part of a recent marketing campaign, which aimed to get customers to take out a personal loan. They plan to conduct more marketing campaigns going forward so would like you to ensure it conforms to the specific structure and data types that they specify so that they can then use the cleaned data you provide to set up a PostgreSQL database, which will store this campaign‚Äôs data and allow data from future campaigns to be easily imported.\nThey have supplied you with a csv file called \"bank_marketing.csv\", which you will need to clean, reformat, and split the data, saving three final csv files. Specifically, the three files should have the names and contents as outlined below:",
    "crumbs": [
      "Data Engineering and Architecture",
      "Bank Marketing ETL"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/bank_etl.html#client.csv",
    "href": "pages/projects/data_engineering/posts/bank_etl.html#client.csv",
    "title": "Bank Marketing ETL Project",
    "section": "client.csv",
    "text": "client.csv\n\n\n\n\n\n\n\n\n\ncolumn\ndata type\ndescription\ncleaning requirements\n\n\n\n\nclient_id\ninteger\nClient ID\nN/A\n\n\nage\ninteger\nClient‚Äôs age in years\nN/A\n\n\njob\nobject\nClient‚Äôs type of job\nChange \".\" to \"_\"\n\n\nmarital\nobject\nClient‚Äôs marital status\nN/A\n\n\neducation\nobject\nClient‚Äôs level of education\nChange \".\" to \"_\" and \"unknown\" to np.NaN\n\n\ncredit_default\nbool\nWhether the client‚Äôs credit is in default\nConvert to boolean data type: 1 if \"yes\", otherwise 0\n\n\nmortgage\nbool\nWhether the client has an existing mortgage (housing loan)\nConvert to boolean data type: 1 if \"yes\", otherwise 0",
    "crumbs": [
      "Data Engineering and Architecture",
      "Bank Marketing ETL"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/bank_etl.html#campaign.csv",
    "href": "pages/projects/data_engineering/posts/bank_etl.html#campaign.csv",
    "title": "Bank Marketing ETL Project",
    "section": "campaign.csv",
    "text": "campaign.csv\n\n\n\n\n\n\n\n\n\ncolumn\ndata type\ndescription\ncleaning requirements\n\n\n\n\nclient_id\ninteger\nClient ID\nN/A\n\n\nnumber_contacts\ninteger\nNumber of contact attempts to the client in the current campaign\nN/A\n\n\ncontact_duration\ninteger\nLast contact duration in seconds\nN/A\n\n\nprevious_campaign_contacts\ninteger\nNumber of contact attempts to the client in the previous campaign\nN/A\n\n\nprevious_outcome\nbool\nOutcome of the previous campaign\nConvert to boolean data type: 1 if \"success\", otherwise 0.\n\n\ncampaign_outcome\nbool\nOutcome of the current campaign\nConvert to boolean data type: 1 if \"yes\", otherwise 0.\n\n\nlast_contact_date\ndatetime\nLast date the client was contacted\nCreate from a combination of day, month, and a newly created year column (which should have a value of 2022);  Format = \"YYYY-MM-DD\"",
    "crumbs": [
      "Data Engineering and Architecture",
      "Bank Marketing ETL"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/bank_etl.html#economics.csv",
    "href": "pages/projects/data_engineering/posts/bank_etl.html#economics.csv",
    "title": "Bank Marketing ETL Project",
    "section": "economics.csv",
    "text": "economics.csv\n\n\n\n\n\n\n\n\n\ncolumn\ndata type\ndescription\ncleaning requirements\n\n\n\n\nclient_id\ninteger\nClient ID\nN/A\n\n\ncons_price_idx\nfloat\nConsumer price index (monthly indicator)\nN/A\n\n\neuribor_three_months\nfloat\nEuro Interbank Offered Rate (euribor) three-month rate (daily indicator)\nN/A\n\n\n\nimport pandas as pd\nimport numpy as np\n\n# Start coding here...\ndf = pd.read_csv(\"bank_marketing.csv\")\n\nfor col in [\"credit_default\", \"mortgage\", \"previous_outcome\", \"campaign_outcome\"]:\n    print(col)\n    print(\"--------------\")\n    print(df[col].value_counts())\ncredit_default\n--------------\nno         32588\nunknown     8597\nyes            3\nName: credit_default, dtype: int64\nmortgage\n--------------\nyes        21576\nno         18622\nunknown      990\nName: mortgage, dtype: int64\nprevious_outcome\n--------------\nnonexistent    35563\nfailure         4252\nsuccess         1373\nName: previous_outcome, dtype: int64\ncampaign_outcome\n--------------\nno     36548\nyes     4640\nName: campaign_outcome, dtype: int64\nclient = df[['client_id', 'age', 'job', 'marital', 'education', 'credit_default', 'mortgage']]\ncampaign = df[['client_id', 'number_contacts', 'contact_duration', 'previous_campaign_contacts', 'previous_outcome', 'campaign_outcome', 'day', 'month']]\neconomics = df[['client_id', 'cons_price_idx', 'euribor_three_months']]\n\nprint(client.head())\nprint(campaign.head())\nprint(economics.head())\n   client_id  age        job  marital    education credit_default mortgage\n0          0   56  housemaid  married     basic.4y             no       no\n1          1   57   services  married  high.school        unknown       no\n2          2   37   services  married  high.school             no      yes\n3          3   40     admin.  married     basic.6y             no       no\n4          4   56   services  married  high.school             no       no\n   client_id  number_contacts  contact_duration  ...  campaign_outcome day month\n0          0                1               261  ...                no  13   may\n1          1                1               149  ...                no  19   may\n2          2                1               226  ...                no  23   may\n3          3                1               151  ...                no  27   may\n4          4                1               307  ...                no   3   may\n\n[5 rows x 8 columns]\n   client_id  cons_price_idx  euribor_three_months\n0          0          93.994                 4.857\n1          1          93.994                 4.857\n2          2          93.994                 4.857\n3          3          93.994                 4.857\n4          4          93.994                 4.857\nimport numpy as np\n\nclient_c = client.copy()\nclient_c['job'] = client_c['job'].replace('.', '_')\nclient_c['education'] = client_c['education'].str.replace('.', '_')\nclient_c['education'].replace('unknown', np.NaN, inplace=True)\nclient_c['credit_default'] = client_c['credit_default'].apply(lambda x: 1 if x == 'yes' else 0)\nclient_c['credit_default'] = client_c['credit_default'].astype('bool')\nclient_c['mortgage'] = client_c['mortgage'].apply(lambda x: 1 if x == 'yes' else 0)\nclient_c['mortgage'] = client_c['mortgage'].astype('bool')\n\nprint(client_c.head())\n   client_id  age        job  marital    education  credit_default  mortgage\n0          0   56  housemaid  married     basic_4y           False     False\n1          1   57   services  married  high_school           False     False\n2          2   37   services  married  high_school           False      True\n3          3   40     admin.  married     basic_6y           False     False\n4          4   56   services  married  high_school           False     False\ncampaign_c = campaign.copy()\n\ncampaign_c['previous_outcome'] = campaign_c['previous_outcome'].apply(lambda x: 1 if x == 'success' else 0).astype('bool')\ncampaign_c['campaign_outcome'] = campaign_c['campaign_outcome'].apply(lambda x: 1 if x == 'yes' else 0).astype('bool')\ncampaign_c['year'] = 2022\ncampaign_c['last_contact_date'] = pd.to_datetime(campaign_c['day'].astype(str) + campaign_c['month'] + campaign_c['year'].astype(str), format='%d%b%Y')\n\ncampaign_c.head()\n\n\n\n\n\n\n\n\nclient_id\n\n\nnumber_contacts\n\n\ncontact_duration\n\n\nprevious_campaign_contacts\n\n\nprevious_outcome\n\n\ncampaign_outcome\n\n\nday\n\n\nmonth\n\n\nyear\n\n\nlast_contact_date\n\n\n\n\n\n\n0\n\n\n0\n\n\n1\n\n\n261\n\n\n0\n\n\nFalse\n\n\nFalse\n\n\n13\n\n\nmay\n\n\n2022\n\n\n2022-05-13\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n149\n\n\n0\n\n\nFalse\n\n\nFalse\n\n\n19\n\n\nmay\n\n\n2022\n\n\n2022-05-19\n\n\n\n\n2\n\n\n2\n\n\n1\n\n\n226\n\n\n0\n\n\nFalse\n\n\nFalse\n\n\n23\n\n\nmay\n\n\n2022\n\n\n2022-05-23\n\n\n\n\n3\n\n\n3\n\n\n1\n\n\n151\n\n\n0\n\n\nFalse\n\n\nFalse\n\n\n27\n\n\nmay\n\n\n2022\n\n\n2022-05-27\n\n\n\n\n4\n\n\n4\n\n\n1\n\n\n307\n\n\n0\n\n\nFalse\n\n\nFalse\n\n\n3\n\n\nmay\n\n\n2022\n\n\n2022-05-03\n\n\n\n\n\ncampaign_c['previous_outcome'].value_counts()\nFalse    39815\nTrue      1373\nName: previous_outcome, dtype: int64\nclient = client_c\ncampaign = campaign_c.drop(['month', 'day', 'year'], axis=1)\n\nprint(client.head())\nprint(campaign.head())\nprint(economics.head())\n   client_id  age        job  marital    education  credit_default  mortgage\n0          0   56  housemaid  married     basic_4y           False     False\n1          1   57   services  married  high_school           False     False\n2          2   37   services  married  high_school           False      True\n3          3   40     admin.  married     basic_6y           False     False\n4          4   56   services  married  high_school           False     False\n   client_id  number_contacts  ...  campaign_outcome  last_contact_date\n0          0                1  ...             False         2022-05-13\n1          1                1  ...             False         2022-05-19\n2          2                1  ...             False         2022-05-23\n3          3                1  ...             False         2022-05-27\n4          4                1  ...             False         2022-05-03\n\n[5 rows x 7 columns]\n   client_id  cons_price_idx  euribor_three_months\n0          0          93.994                 4.857\n1          1          93.994                 4.857\n2          2          93.994                 4.857\n3          3          93.994                 4.857\n4          4          93.994                 4.857\nclient.to_csv('client.csv', index=False)\ncampaign.to_csv('campaign.csv', index=False)\neconomics.to_csv('economics.csv', index=False)",
    "crumbs": [
      "Data Engineering and Architecture",
      "Bank Marketing ETL"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/oss_data_arch.html#introduction",
    "href": "pages/projects/data_engineering/posts/oss_data_arch.html#introduction",
    "title": "Open Source Data and Analytics Architecture",
    "section": "Introduction",
    "text": "Introduction\nI will update this when I begin the project. The goal here is to explore and create a tech stack to support modern data and analytical workloads, using entirely open source software. Ideally, I‚Äôll be able to scale it to terabytes and then share that template and the guide as a public resource.\nCurrently, I‚Äôm thinking of the following tools, as part of a non-exhaustive list of the stack:\n\n\nOS/Environment: zsh/bash\nProject and Package Management: uv\nCollaboration and Source Control: Github\nDocumentation: Quarto\nData Modeling: dbt\nContainerization: Docker\nContainer Orchestration: Kubernetes\nOLTP Database: PostgreSQL\nOLAP Database: DuckDB\nBatch Ingestion: Python\nETL: dbt\nTesting: pytest\nData Quality: Great Expectations\nMetadata: Unity Catalog\nETL Orchestration: Airflow and/or Dagster\nStreaming Ingestion: Kafka\n\n\nGeneral workflow I‚Äôm envisioning:\n\nInitialize project with uv, add basic dependencies for the environment\nCreate the repo with the GitHub CLI\nSet the remote as the upstream and do the initial commit\nInitialize the quarto and dbt projects as subdirectories of the main, uv project directory\nCreate the postgres container with docker, use this to initialize the postgres database (Prod)\nIn your uv envionrment, initialize the duckdb (Dev/Test) persistent database\n\nSimpler to work quickly with duckdb, postgres has more configurations/overhead, but is better for long term persistent\n\nUse python and duckdb to ingest the initial batch of raw data\nUse dbt to define the data model, pytest to define the basic tests, and great expectations to define data quality\nInitialize the unity catalog instance, add the connection information (Dev/Test/Prod)\nGenerate metadata and lineage\nStart scheduling and orchestrating jobs\nPotentially scale system up to handle stremaing data",
    "crumbs": [
      "Data Engineering and Architecture",
      "Open Source Data and Analytics Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_science/data_science.html",
    "href": "pages/projects/data_science/data_science.html",
    "title": "Data Science and Machine Learning Projects",
    "section": "",
    "text": "Landing page for all my project posts related to Data Science and Machine Learning.",
    "crumbs": [
      "Data Science and Machine Learning"
    ]
  },
  {
    "objectID": "pages/projects/data_science/data_science.html#projects",
    "href": "pages/projects/data_science/data_science.html#projects",
    "title": "Data Science and Machine Learning Projects",
    "section": "Projects",
    "text": "Projects",
    "crumbs": [
      "Data Science and Machine Learning"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html",
    "title": "NFL Big Data Bowl 2025",
    "section": "",
    "text": "I will add to this later. Currently, this is a rough combination of my early notebooks.",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#introduction",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#introduction",
    "title": "NFL Big Data Bowl 2025",
    "section": "",
    "text": "I will add to this later. Currently, this is a rough combination of my early notebooks.",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#ongoing-thoughts",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#ongoing-thoughts",
    "title": "NFL Big Data Bowl 2025",
    "section": "Ongoing Thoughts",
    "text": "Ongoing Thoughts\nThis is the first time I‚Äôm adding to a unified document, it‚Äôs December 13th, or about 1 month into my project. As of now, Random Forest definitely seems like the best path forward; however, the intial version certainly overfit. I believe the model overfit because some of the plays columns are the pre/post snap home/away team win probability values. In my next iteration, I‚Äôm going to remove those values, and in the future I might even try to recreate them. That being said, there‚Äôs a little under one month to go, so I‚Äôm going to focus on putting together some kind of deliverable/submission, before I go off the deep end. That said, this page and the website in general are going to be sloppy as I figure things out and slowly improve the organization and UI.",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#exploratory-data-analysis-and-initial-thoughts",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#exploratory-data-analysis-and-initial-thoughts",
    "title": "NFL Big Data Bowl 2025",
    "section": "Exploratory Data Analysis and Initial Thoughts",
    "text": "Exploratory Data Analysis and Initial Thoughts\n\n\n\n\n\n\nNote\n\n\n\nThis was written on November 26th, 2024. It was later added to this site on December 13th, 2024. This is a general write up on the project, you can see the full notebooks below. The full repo is available here.\n\n\nCurrently, I‚Äôve made solid progress with my initial exploratory data analysis and project configuration. Here are some quick notes about the setup of my project environment (from IDE to tools/versions). - Using VS Code with the Jupyter, Jinja, YAML, Quarto (for notes/project submissions), and dbt extensions. - DuckDB is my primary database tool (for now), with dbt for the data modeling - Then, I‚Äôm using Python and Jupyter Notebooks for the analysis/ML component\nThe reason I may switch to PostgreSQL for the primary Database is to just gain experience with DuckDB as a DEV environement and Postgres for PROD. Realistically, however, for the scope of this project DuckDB accomplishes everything I need it to.\nFor the forseeable future, the only side project I‚Äôll be working on is this, so my next few posts will only look at the project progress and my thoughts about the Big Data Bowl, feel free to checkout the GitHub repository where I‚Äôm saving my work.\nSome notes about my current project progress: - The project folder has a few subdirectories, including nfl_dbt which is the dbt project folder - The raw data came in the form of 13 CSVs from Kaggle. 4 of which are 50mb or less, 9 of which are ~1gb. - I‚Äôm using Databricks‚Äô ‚ÄúMedallion Architecture‚Äù to guide my data modeling workflow. - I built the initial dbt models, using DuckDB as the DEV target (enabling 4 threads) and loaded the ‚Äúbronze‚Äù schema which contains the 13 raw tables - I aggregated the data into the ‚Äúsilver‚Äù schema, which contains an aggregated play data table - I further aggregated the data into the ‚Äúgold‚Äù schema, which provides basic analytic tables - Currently, I completed an initial analysis using an EDA notebook where I looked at using a LinearRegression and KNN to compare pre-snap play data with play outcomes. - I settled on a KNN model, but I‚Äôm only seeing about a 61.1% accuracy rate (confusion matrix and explanation below).\nSo, I‚Äôm at a bit of a crossroads, with a few ways forward. It may be simpler (for the initial project/submission) to build a linear regression model that takes pre-snap play data as features, and then looks at yards gained (or loss) for the output. Conversely, if I stick with the KNN model I‚Äôll need to make some changes. The majority of the outputs are either Gain or Completed, which refer to a positive rushing play and a completed pass, respectively. The issue here, the model overwhelmingly predicts those values, but fails to accurately predict things like Touchdowns, Sacks, or Interceptions.\nSo, I may need to limit possible play outcomes, or at least combine some categories (i.e.¬†Turnover for Fumble + Interception). Or, add some more presnap data, such as down and distance (I currently only use starting yard line, along with categorical data). If you made it this far, thank you! Below is the confusion matrix output from my current KNN model. I‚Äôll add some hashtags at the end as an experiment too, because I‚Äôm not sure if that will help with post discoverability and/or integrate with Bluesky feeds.",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#knn-classifier-notebook-first-model",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#knn-classifier-notebook-first-model",
    "title": "NFL Big Data Bowl 2025",
    "section": "KNN Classifier Notebook (First Model)",
    "text": "KNN Classifier Notebook (First Model)\n# Import dependencies\nimport duckdb\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n# Open the connection to the persistent database\ncon = duckdb.connect(\".config/nfl.duckdb\")\ncon.close()\n# Create the initial dataframe object with DuckDB\ndf = con.sql(\"\"\"\n    SELECT *\n    FROM gold.plays_numeric             \n\"\"\").df()\n\ndf.head()\n\n\n\n\n\n\n\n\ngameId\n\n\nplayId\n\n\npossessionTeam\n\n\nyardlineNumber\n\n\noffenseFormation\n\n\nreceiverAlignment\n\n\nplayType\n\n\ndefensiveFormation\n\n\npff_manZone\n\n\nyardsGained\n\n\nplayOutcome\n\n\n\n\n\n\n0\n\n\n2022102302\n\n\n2655\n\n\nCIN\n\n\n21\n\n\n3\n\n\n8\n\n\n2\n\n\n6\n\n\n2\n\n\n9\n\n\n3\n\n\n\n\n1\n\n\n2022091809\n\n\n3698\n\n\nCIN\n\n\n8\n\n\n3\n\n\n8\n\n\n2\n\n\n13\n\n\n2\n\n\n4\n\n\n3\n\n\n\n\n2\n\n\n2022103004\n\n\n3146\n\n\nHOU\n\n\n20\n\n\n6\n\n\n5\n\n\n2\n\n\n13\n\n\n2\n\n\n6\n\n\n3\n\n\n\n\n3\n\n\n2022110610\n\n\n348\n\n\nKC\n\n\n23\n\n\n6\n\n\n5\n\n\n2\n\n\n13\n\n\n2\n\n\n4\n\n\n3\n\n\n\n\n4\n\n\n2022102700\n\n\n2799\n\n\nBAL\n\n\n27\n\n\n4\n\n\n7\n\n\n1\n\n\n3\n\n\n1\n\n\n-1\n\n\n2\n\n\n\n\n\n# Split the table into features and target\nX = con.sql(\"\"\"\n    SELECT yardlineNumber, offenseFormation, receiverAlignment, playType, defensiveFormation, pff_manZone\n    FROM gold.plays_numeric\n\"\"\").df()\n\ny = np.array(con.sql(\"\"\"\n    SELECT playOutcome\n    FROM gold.plays_numeric\n\"\"\").df()).ravel()\n\nprint(X.shape, y.shape)\n(16124, 6) (16124,)\n# Instantiate the model and split the datasets into training/testing\nknn = KNeighborsClassifier(n_neighbors=7)\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.7, random_state=123)\n# Fit the model\nknn.fit(X_train, y_train)\n\n\n\nKNeighborsClassifier(n_neighbors=7)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n¬†¬†KNeighborsClassifier?Documentation for KNeighborsClassifieriFitted\n\nKNeighborsClassifier(n_neighbors=7)\n\n\n\n\n\n# Basic KNN Performance Metrics\ny_pred = knn.predict(X_val)\n\nprint(knn.score(X_val, y_val))\n0.6114096734187681\n# Datacamp Model performance Loop\n# Create neighbors\nneighbors = np.arange(1, 13)\ntrain_accuracies = {}\ntest_accuracies = {}\n\nfor neighbor in neighbors:\n  \n    # Set up a KNN Classifier\n    knn = KNeighborsClassifier(n_neighbors=neighbor)\n  \n    #¬†Fit the model\n    knn.fit(X_train, y_train)\n  \n    # Compute accuracy\n    train_accuracies[neighbor] = knn.score(X_train, y_train)\n    test_accuracies[neighbor] = knn.score(X_val, y_val)\nprint(neighbors, '\\n', train_accuracies, '\\n', test_accuracies)\n# Visualize model accuracy with various neighbors\n# Add a title\nplt.title(\"KNN: Varying Number of Neighbors\")\n\n#¬†Plot training accuracies\nplt.plot(neighbors, train_accuracies.values(), label=\"Training Accuracy\")\n\n# Plot test accuracies\nplt.plot(neighbors, test_accuracies.values(), label=\"Testing Accuracy\")\n\nplt.legend()\nplt.xlabel(\"Number of Neighbors\")\nplt.ylabel(\"Accuracy\")\n\n# Display the plot\nplt.show()\n# Map the original target variables to the KNN outputs\nplay_outcome_map = con.sql(\"\"\"\n    SELECT\n    CASE\n        WHEN playOutcome = 1 THEN 'Gain'\n        WHEN playOutcome = 2 THEN 'Loss'\n        WHEN playOutcome = 3 THEN 'Completed'\n        WHEN playOutcome = 4 THEN 'Incomplete'\n        WHEN playOutcome = 5 THEN 'Scrambled'\n        WHEN playOutcome = 6 THEN 'Touchdown'\n        WHEN playOutcome = 7 THEN 'Intercepted'\n        WHEN playOutcome = 8 THEN 'Fumbled'\n        WHEN playOutcome = 9 THEN 'Sacked'\n        WHEN playOutcome = 0 THEN 'Penalty'\n        ELSE 'Unknown'  -- Optional, in case there are values not matching any condition\n    END AS playOutcome\nFROM gold.plays_numeric\n\"\"\").df()['playOutcome'].tolist()\n\nplay_outcome_map = np.unique(play_outcome_map).tolist()\n# Create a dictionary to map playOutcome values to corresponding labels\nplay_outcome_dict = {i: play_outcome_map[i] for i in range(len(play_outcome_map))}\n\n# Generate a colormap for the string labels (use 'viridis' colormap)\ncolors = plt.cm.viridis(np.linspace(0, 1, len(play_outcome_map)))\nplay_colors = dict(zip(range(len(play_outcome_map)), colors))\n\n# Create legend patches for each class label\nlegend_patches = [mpatches.Patch(color=play_colors[i], label=play_outcome_map[i]) for i in range(len(play_outcome_map))]\n\n# Assuming `y_pred` is a list of predictions, map numeric predictions to string labels\npred_labels = [play_outcome_dict[val] for val in y_pred]\n# Attempting to conduct sensitivity analysis for feature importance\nfor feature in range(6):\n    plt.figure(figsize=(10, 6))\n    plt.scatter(X_val.iloc[:, feature], y_pred, c=[play_colors[val] for val in y_pred], cmap='viridis', edgecolor='k')\n    plt.xlabel(f\"Feature {feature + 1}\")\n    plt.ylabel(\"Predicted Class\")\n    plt.yticks(range(len(play_outcome_map)), play_outcome_map)\n    plt.title(f\"Predictions by Feature {feature + 1}\")\n    plt.legend(handles = legend_patches, title=\"Actual Class\", bbox_to_anchor=(1.05, 1), loc = 'upper left')\n    plt.tight_layout\n    plt.show()\n# Your play_outcome_dict with correct mapping\nplay_outcome_dict = {\n    1: 'Gain',\n    2: 'Loss',\n    3: 'Completed',\n    4: 'Incomplete',\n    5: 'Scrambled',\n    6: 'Touchdown',\n    7: 'Intercepted',\n    8: 'Fumbled',\n    9: 'Sacked',\n    0: 'Penalty'\n}\n\n# Map the y_pred values to the corresponding labels\npred_labels = [play_outcome_dict[val] for val in y_pred]\n\n# Define the colormap based on the labels\nplay_colors = plt.cm.viridis(np.linspace(0, 1, len(play_outcome_dict)))\n\n# Combine your features (X_val) and the predictions (y_pred) into a single DataFrame\ndf_features = X_val.copy()\ndf_features['Predicted Class'] = [play_outcome_dict[key] for key in y_pred]\n\n# Create a pairplot to visualize pairwise relationships between all features\nsns.pairplot(df_features, hue='Predicted Class', palette=dict(zip(play_outcome_dict.values(), play_colors)), markers='o')\n\n# Customize the plot\nplt.suptitle('Pairplot of Features Colored by Predicted Class', y=1.02)\nplt.legend(handles = legend_patches, title=\"Actual Class\", bbox_to_anchor=(1.05, 1), loc = 'upper left')\nplt.tight_layout()\nplt.show()\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Assuming y_true contains the true labels and y_pred contains the predicted labels\n# Map numerical values to their respective class labels\ny_true_labels = [play_outcome_dict[val] for val in y_val]  # Replace y_true with your actual true labels\ny_pred_labels = [play_outcome_dict[val] for val in y_pred]\n\n# Generate the confusion matrix\ncm = confusion_matrix(y_true_labels, y_pred_labels, labels=list(play_outcome_dict.values()))\n\n# Visualize the confusion matrix\nfig, ax = plt.subplots(figsize=(10, 8))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(play_outcome_dict.values()))\ndisp.plot(cmap='viridis', ax=ax, xticks_rotation=45)\n\n# Customize the plot\nplt.title(\"Confusion Matrix of KNN Model\")\nplt.show()\n\n\n\nKNN Classifier Confusion Matrix",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#linear-regression-notebook-second-model",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#linear-regression-notebook-second-model",
    "title": "NFL Big Data Bowl 2025",
    "section": "Linear Regression Notebook (Second Model)",
    "text": "Linear Regression Notebook (Second Model)\nimport duckdb\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n# Open the DuckDB connection, to the persistent database\ncon = duckdb.connect(\".config/nfl.duckdb\")\ncon.close()\n# Test converting the play outcomes to just yards gained or lost\ncon.sql(\"\"\"\n    SELECT *\n    FROM gold.plays_numeric   \n\"\"\")\n# Can still utilize plays_numeric, just won't use the categorical outcomes as the target\nX = con.sql(\"\"\"\n    SELECT yardlineNumber, offenseFormation, receiverAlignment, playType, defensiveFormation, pff_manZone\n    FROM gold.plays_numeric   \n\"\"\").df()\ny = con.sql(\"\"\"\n    SELECT yardsGained\n    FROM gold.plays_numeric   \n\"\"\").df()\n# Train test split\n# May need to come back and apply a Standard Scaler later\n\nlinreg = LinearRegression()\nscaler = StandardScaler()\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.7, random_state = 123)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\n# Fit the model\nlinreg.fit(X_train_scaled, y_train)\n\n\n\nLinearRegression()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n¬†¬†LinearRegression?Documentation for LinearRegressioniFitted\n\nLinearRegression()\n\n\n\n\n\n# Begin testing and scoring\ny_pred = linreg.predict(X_val_scaled)\n\nmse = mean_squared_error(y_val, y_pred)\nr2 = r2_score(y_val, y_pred)\n\nprint(f\"MSE: {mse}\")\nprint(f\"R2 Score: {r2}\")\nMSE: 80.63310622289697\nR2 Score: 0.02277208083008464\nplt.scatter(y_val, y_pred)\nplt.xlabel(\"Actual Values\")\nplt.ylabel(\"Predicted Values\")\nplt.title(\"Linear Regression: Actual vs Predicted\")\nplt.show()\n\ncoefficients = linreg.coef_\nprint(f\"Coefficients: {coefficients}\")\n\n\n\nLinear Regression Scatter Plot\n\n\nCoefficients: [[ 0.05041481  0.32550574  0.04088819  1.77381568 -0.0198335  -0.16104852]]\nridge = Ridge(alpha=1.0)\nridge.fit(X_train_scaled, y_train)\ny_pred_ridge = ridge.predict(X_val_scaled)\nprint(f\"Ridge MSE: {mean_squared_error(y_val, y_pred_ridge)}\")\nRidge MSE: 80.63311884052399",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#random-forest-notebook-third-model",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#random-forest-notebook-third-model",
    "title": "NFL Big Data Bowl 2025",
    "section": "Random Forest Notebook (Third Model)",
    "text": "Random Forest Notebook (Third Model)\nimport duckdb\nimport tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n# Create the database connection\ncon = duckdb.connect(\"nfl.duckdb\")\n#con.close()\n# Creating dataframes with DuckDB, plays and player_play both have 50 columns, more ideal for a broad random forest\nX = con.sql(\"\"\"\n    SELECT quarter, down, yardsToGo, yardlineNumber, preSnapHomeScore, preSnapVisitorScore,\n    playNullifiedByPenalty, absoluteYardlineNumber, preSnapHomeTeamWinProbability, preSnapVisitorTeamWinProbability, expectedPoints,\n    passResult_complete, passResult_incomplete, passResult_sack, passResult_interception, passResult_scramble, passLength, targetX, targetY,\n    playAction, passTippedAtLine, unblockedPressure, qbSpike, qbKneel, qbSneak, penaltyYards, prePenaltyYardsGained, \n    homeTeamWinProbabilityAdded, visitorTeamWinProbilityAdded, expectedPointsAdded, isDropback, timeToThrow, timeInTackleBox, timeToSack,\n    dropbackDistance, pff_runPassOption, playClockAtSnap, pff_manZone, pff_runConceptPrimary_num, pff_passCoverage_num, pff_runConceptSecondary_num\nFROM silver.plays_rf\n\"\"\").df()\ny = np.array(con.sql(\"\"\"\n    SELECT yardsGained\n    FROM silver.plays_rf\n\"\"\").df()).ravel()\n# Having issues with NA values, the below code does a simple count using pandas, will then go back and change the query\n# As of writing this, the issue is solved; however, the dbt model for this is far from efficient\nna_counts = (X == 'NA').sum()\n\n# Optionally, filter only columns with 'NA' values for easier review\nna_counts_filtered = na_counts[na_counts &gt; 0]\nprint(na_counts_filtered, \"\\n\", X.shape, \"\\n\", y.shape) # playClockAtSnap has only 1 NA value, will just drop that row\nSeries([], dtype: int64) \n (16124, 41) \n (16124,)\n# Instantiate the model and split the data\nrf = RandomForestRegressor(warm_start=True)\n\nselector = RFE(rf, n_features_to_select=10, step=1)\nX_selected = selector.fit_transform(X, y)\n# Begin Interpretation, first with feature importance\nselected_features = X.columns[selector.support_]\nprint(selected_features)\nIndex(['yardlineNumber', 'absoluteYardlineNumber',\n       'preSnapHomeTeamWinProbability', 'expectedPoints',\n       'passResult_scramble', 'penaltyYards', 'prePenaltyYardsGained',\n       'homeTeamWinProbabilityAdded', 'visitorTeamWinProbilityAdded',\n       'expectedPointsAdded'],\n      dtype='object')\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n\n# Train the model\nrf.fit(X_train, y_train)\n\n# Make predictions\ny_pred = rf.predict(X_test)\n\n# Calculate scores\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"R^2 Score: {r2}\")\nMean Squared Error: 1.7769936744186046\nR^2 Score: 0.9766614590863065\n# Continue with the GridSearch\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n}\n\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=4)\ngrid_search.fit(X_train, y_train)\nbest_rf = grid_search.best_estimator_\n\n# Wrap a progress bar for longer Grid Searches\n\"\"\"with tqdm(total=len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf']), desc=\"GridSearch Progress\") as pbar:\n    def callback(*args, **kwargs):\n        pbar.update(1)\n\n    # Add the callback to the grid search\n    grid_search.fit(X, y, callback=callback)\"\"\"\n\nprint(grid_search.best_params_)\n{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n# Continue with the Cross Validation Score\ncv_scores = cross_val_score(rf, X_selected, y, cv=5, scoring='neg_mean_squared_error')\nprint(f\"Cross-validated MSE: {-cv_scores.mean()}\")\nCross-validated MSE: 1.9303851017196607",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/gym_market.html",
    "href": "pages/projects/data_science/posts/gym_market.html",
    "title": "Gym Market Analysis",
    "section": "",
    "text": "gym\n\n\nYou are a product manager for a fitness studio and are interested in understanding the current demand for digital fitness classes. You plan to conduct a market analysis in Python to gauge demand and identify potential areas for growth of digital products and services.\n\nThe Data\nYou are provided with a number of CSV files in the ‚ÄúFiles/data‚Äù folder, which offer international and national-level data on Google Trends keyword searches related to fitness and related products.\n\n\nworkout.csv\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\n'month'\nMonth when the data was measured.\n\n\n'workout_worldwide'\nIndex representing the popularity of the keyword ‚Äòworkout‚Äô, on a scale of 0 to 100.\n\n\n\n\n\nthree_keywords.csv\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\n'month'\nMonth when the data was measured.\n\n\n'home_workout_worldwide'\nIndex representing the popularity of the keyword ‚Äòhome workout‚Äô, on a scale of 0 to 100.\n\n\n'gym_workout_worldwide'\nIndex representing the popularity of the keyword ‚Äògym workout‚Äô, on a scale of 0 to 100.\n\n\n'home_gym_worldwide'\nIndex representing the popularity of the keyword ‚Äòhome gym‚Äô, on a scale of 0 to 100.\n\n\n\n\n\nworkout_geo.csv\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\n'country'\nCountry where the data was measured.\n\n\n'workout_2018_2023'\nIndex representing the popularity of the keyword ‚Äòworkout‚Äô during the 5 year period.\n\n\n\n\n\nthree_keywords_geo.csv\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\n'country'\nCountry where the data was measured.\n\n\n'home_workout_2018_2023'\nIndex representing the popularity of the keyword ‚Äòhome workout‚Äô during the 5 year period.\n\n\n'gym_workout_2018_2023'\nIndex representing the popularity of the keyword ‚Äògym workout‚Äô during the 5 year period.\n\n\n'home_gym_2018_2023'\nIndex representing the popularity of the keyword ‚Äòhome gym‚Äô during the 5 year period.\n\n\n\n# Import the necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# Start coding here\nimport pandas as pd\n\nworkout = pd.read_csv('data/workout.csv')\nthree_kw = pd.read_csv('data/three_keywords.csv')\nworkout_geo = pd.read_csv('data/workout_geo.csv')\nkw_geo = pd.read_csv('data/three_keywords_geo.csv')\nworkout.head()\n\n\n\n\n\n\n\n\nmonth\n\n\nworkout_worldwide\n\n\n\n\n\n\n0\n\n\n2018-03\n\n\n59\n\n\n\n\n1\n\n\n2018-04\n\n\n61\n\n\n\n\n2\n\n\n2018-05\n\n\n57\n\n\n\n\n3\n\n\n2018-06\n\n\n56\n\n\n\n\n4\n\n\n2018-07\n\n\n51\n\n\n\n\n\npeak = workout.loc[workout['workout_worldwide'].idxmax()]\nyear_str = peak.str.split('-')[0][0]\nyear_str\n'2020'\nworkout.dtypes\nmonth                object\nworkout_worldwide     int64\ndtype: object\ncovid = workout.loc[(workout['month'] &gt; '2019-12') & (workout['month'] &lt;= '2022-12')]\npost_covid = workout.loc[workout['month'] &gt; '2022-12']\npeak_covid = three_kw.loc[(workout['month'] &gt; '2019-12') & (workout['month'] &lt;= '2022-12')][['home_workout_worldwide', 'gym_workout_worldwide', 'home_gym_worldwide']].max().idxmax()\ncurrent = three_kw.loc[workout['month'] &gt; '2022-12'][['home_workout_worldwide', 'gym_workout_worldwide', 'home_gym_worldwide']].max().idxmax()\npeak_covid\ncurrent\n'gym_workout_worldwide'\ntop_country = workout_geo.loc[workout_geo['workout_2018_2023'].idxmax()]['country']\ntop_country\n'United States'\nkw_geo1 = kw_geo.loc[(kw_geo['Country']=='Philippines') | (kw_geo['Country']=='Malaysia')]\nkw_geo1\n\n\n\n\n\n\n\n\nCountry\n\n\nhome_workout_2018_2023\n\n\ngym_workout_2018_2023\n\n\nhome_gym_2018_2023\n\n\n\n\n\n\n23\n\n\nPhilippines\n\n\n52.0\n\n\n38.0\n\n\n10.0\n\n\n\n\n61\n\n\nMalaysia\n\n\n47.0\n\n\n38.0\n\n\n15.0\n\n\n\n\n\nhome_workout_geo = kw_geo1.loc[kw_geo1['home_workout_2018_2023'] == kw_geo1['home_workout_2018_2023'].max(), 'Country'].values[0]\nhome_workout_geo\n'Philippines'",
    "crumbs": [
      "Data Science and Machine Learning",
      "Gym Market Analysis"
    ]
  },
  {
    "objectID": "pages/blogs/blogs.html",
    "href": "pages/blogs/blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "A landing page for various blogs, journals, or random thoughts. Some of these will be focused on specific tools or technology, others will be random thoughts or research notes.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 14, 2024\n\n\n1. Experimenting with GitHub Pages and WhiteWind\n\n\nChris Kornaros\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Blogs"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chris Kornaros",
    "section": "",
    "text": "This site is the personal portfolio and homepage for Chris Kornaros. I‚Äôm a Tulane Graduate and professional Data Engineer. On here, you‚Äôll find projects and code samples that I can share publicly, guides for various tools or workflows, journal or blog posts, and any independent research or professional updates (including my resume).\nThis website is a work in progress, so things are going to move around and break. Additionally, there is a lot that I have to still add: past guides, projects, etc. Follow me on social media (links above) to stay up to date with what I‚Äôm working on. To learn more about my background visit About, for guides on various open source tools visit Guides, to see my current and past (public) work visit Projects, and to see any blog posts (available on WhiteWind) visit Blogs."
  },
  {
    "objectID": "pages/blogs/posts/first_blog.html",
    "href": "pages/blogs/posts/first_blog.html",
    "title": "1. Experimenting with GitHub Pages and WhiteWind",
    "section": "",
    "text": "This is the first part of a blog post I originally wrote over a month ago and posted to WhiteWind.\nWhiteWind is a blogging application built on Bluesky‚Äôs foundational, decentralied network. Half of that post is now the beginning of the NFL Big Data Bowl 2025 project write up.\nEventually, I‚Äôd like to find a way to write a post on WhiteWind and have it automatically update my personal site. That way I can do quick, shower thoughts style blog posts there, without having to do the longer Quarto workflow.",
    "crumbs": [
      "1. Experimenting with GitHub Pages and WhiteWind"
    ]
  },
  {
    "objectID": "pages/blogs/posts/first_blog.html#original-post",
    "href": "pages/blogs/posts/first_blog.html#original-post",
    "title": "1. Experimenting with GitHub Pages and WhiteWind",
    "section": "Original Post",
    "text": "Original Post\nIn March of 2024, I took a break from social media (Twitter/Instagram/TikTok/Etc.) because the polarizing algorithms and toxicity were driving me nuts. I took to LinkedIn, in search of a community of professional data nerds, that develop and learn in their freetime. Luckily, I found some great people on there, but my feed was quickly inundated with LinkedInfluencers reposting the same content whether recycling their own, other‚Äôs, or generating posts with AI. Then, in October, I heard that data people were all jumping over to Bluesky, so I thought I‚Äôd give it a shot. Needless to say, I love the data and developer community here. Furthermore, the fundamental design and decentralization of the platform make this that much cooler. Then, I learned about WhiteWind and some other integrations, and decided this is a great way to post updates and keep myself accountable.\nA little about me, I‚Äôm a Tulane Unviersity graduate (BSM ‚Äô20, MS ‚Äô21) and currently a Data (Analytics) Engineer at GM. In my current role I wear many hats! In my 3.5 years at General Motors I‚Äôve gained experience desinging database architecture, building ETL pipelines, being an IT admin, owning products, managing projects, automating tests, and providing training/mentorship for Power BI/Databricks; however, most of my experience is in BI reporting and legacy application upgrades.\nSo, this past April I decided I wanted to really learn the ins and outs of modern data engineering and data science. So, I bought a MacBook Air and started coding! From my Masters degree at Tulane, I had experience with R, Python, SQL for coding, as well as the statistical knwoledge needed for machine learning. Instead of those areas, I began with zsh/bash scripting (using the Terminal to do everything I could), then I learned the basics of git for version control, then I jumped into dbt for data modeling, and finally Docker so I could understand containers. Following that, I began Harvard‚Äôs CS50p course in Python programming. I wanted to understand Python, the programming language, because I had learned Python using Jupyter notebooks instead of scripts, packages, testing, etc.\nIn the few month or so since then, I‚Äôve worked on a few public projects which you can view on my GitHub. These projects are pretty varied, and narrow in scope.\n\nDataCamp projects based on simple ETL, analytics, or programming.\nMy CS50p lecture notes and problem set submissions.\nA simple Introduction to DuckDB using the NFL Big Data Bowl CSV files.\nA private repository with bash scripts and configuration files for starting up my AWS EC2 instance, connecting via ssh, Docker containers, quarto, and the GitHub CLI.",
    "crumbs": [
      "1. Experimenting with GitHub Pages and WhiteWind"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/titanic.html#general-overview",
    "href": "pages/projects/data_science/posts/titanic.html#general-overview",
    "title": "Titanic Disaster",
    "section": "General Overview",
    "text": "General Overview\nThe Kaggle Titanic dataset and ML competition is one that many people are familiar with, and if they‚Äôre like me, it was also their first ML project. I redid this after 3 years to get familiar with my current workflow of using git, notebooks, venvs, etc. Below I included some notes to myself. While it was overkill, I used a Dockerized environment for this project, just to increase my familiarity with containers and the docker toolset.",
    "crumbs": [
      "Data Science and Machine Learning",
      "Titanic Disaster"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/titanic.html#notes-on-the-titanic-model-and-the-process-for-using-the-jupyter-kernel-on-the-ec2-server",
    "href": "pages/projects/data_science/posts/titanic.html#notes-on-the-titanic-model-and-the-process-for-using-the-jupyter-kernel-on-the-ec2-server",
    "title": "Titanic Disaster",
    "section": "Notes on the Titanic model and the process for using the Jupyter Kernel on the EC2 Server",
    "text": "Notes on the Titanic model and the process for using the Jupyter Kernel on the EC2 Server\n\n\nFirst, navigate to Local-Scripts/.AWS/.EC2_Scripts and run the ec2_start.sh script zsh ec2_start.sh\nNext, execute the unix_test_dns.sh script to store the EC2 public DNS in the /etc/hosts file ./unix_test_dns.sh\nUse the ssh -i command to connect to the EC2 server, then run the Jupyter kernel image docker run -p 8888:8888 titanic-env Look into adding a volume mount command here to persist model/file changes in the EC2\nNow that it‚Äôs running. Use a different terminal window (or the VS Code IDE) and test the DNS name. ping unix_test.local Need to make this DNS dynamic\nIn VS Code, open the .ipynb file in the model folder and continue work.\n\nIf you need to reconnect to a kernel, use the Titanic preset.\nVS Code connects to the EC2 IPv4 address, even though the Kernel tells you 127.0.0.1\n\nThe format for connecting to the Public IPv4 is http://IPv4:8888/\nTo pull the file out of the container and store it on the EC2 server\n\ndocker cp 786853360d97:/home/files/titanic_submission.csv files/titanic_submission.csv\ndocker copy instance-id:/path/to/file local/path\n\n\n\n\nIf you need to modify the container\n\nDo so locally, or anywhere, and then push the change to the GitHub repoistory\nThen, pull the changes into the EC2 server\nClear the Docker library/cache, and then rebuild the image from scratch, use the following docker build -t titanic-env -f .config/Dockerfile .\nEnsure this is done from the main project folder and uses those flags\n\nFor Titanic, this is in the admin/Kaggle/Titanic folder in the EC2 instance\n--no-cache ensures it‚Äôs a fresh build (This will take a while, not worth it in the smaller environment. Rebuild with cache)\n-t sets the name of the image\n-f lets you specify the Dockerfile location\n. lets Docker know that your current working directory is where the build context should take place",
    "crumbs": [
      "Data Science and Machine Learning",
      "Titanic Disaster"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/titanic.html#the-code-portion-of-this-notebook",
    "href": "pages/projects/data_science/posts/titanic.html#the-code-portion-of-this-notebook",
    "title": "Titanic Disaster",
    "section": "The Code Portion of this notebook",
    "text": "The Code Portion of this notebook\n\n\n\n\n\n\nCaution\n\n\n\nWhen I most recently completed this competition, I didn‚Äôt do it with the goal in mind of doing a nice write-up. This is really just an amalgamation of the notes I made to myself on how to use/modify the Docker container I ran my model on and the actual notebook + code + notes.\n\n\nimport numpy as np\nimport pandas as pd\nimport duckdb\nimport sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n\n# Initialize a connection and create a persistent database\n# Worth noting that due to the workflow I'm using, the database was/should be created externally, and then built into the Docker container\n# This way, the raw database files are saved locally, but file size won't grow exponentially\ncon = duckdb.connect(\"files/titanic.duckdb\")\n# Using this to DROP and Recreate train_raw, ensuring a fresh process\ncon.sql(\"DROP TABLE train_raw;\")\ncon.sql(\"DROP TABLE test_raw;\")\ncon.sql(\"CREATE TABLE train_raw AS SELECT * FROM 'files/train.csv'\")\ncon.sql(\"CREATE TABLE test_raw AS SELECT * FROM 'files/test.csv'\")\n# Create working tables\n#con.sql(\"DROP TABLE train;\")\n#con.sql(\"DROP TABLE test;\")\ncon.sql(\"CREATE TABLE train AS SELECT * FROM train_raw\")\ncon.sql(\"CREATE TABLE test AS SELECT * FROM test_raw\")\n# Verify the proper tables are loaded\ncon.sql(\"SELECT * FROM duckdb_tables()\")\n# Generate summary statistics\ncon.sql(\"SUMMARIZE train\")\n#con.sql(\"SUMMARIZE test\")\n# Examine Nulls for the Age, Cabin, and Embarked columns (do this for test as well)\ncon.sql(\"SELECT * FROM train WHERE Age IS NULL\") # Seems to make the most sense to use the average age here\ncon.sql(\"SELECT * FROM train WHERE Cabin IS NULL\") # Seems likely Cabins not as strictly recorded for lower class guests, probably unnecessary for model\ncon.sql(\"SELECT * FROM train WHERE Embarked IS NULL\") # This only comprises 2 records and it's unclear if they made it on in the first place, not a high enough percentage of 1st class survivors to consider keeping\n# Update the Age column, replace NULL values with the average Age\ncon.sql(\"\"\"UPDATE train AS train_clean\n        SET Age = (\n            SELECT\n                avg(raw.Age) AS cleanAge\n            FROM train as raw\n            WHERE raw.Age IS NOT NULL\n        )\n        WHERE Age IS NULL\"\"\")\n# Update the Sex column, change the VARCHAR type to BOOLEAN\ncon.sql(\"\"\"ALTER TABLE train ALTER Sex \n        SET DATA TYPE BOOLEAN USING CASE\n            WHEN Sex = 'female' THEN 1 ELSE 0 END\n        \"\"\")\n# Update the Age column in the test dataset\ncon.sql(\"\"\"UPDATE test AS test_clean\n        SET Age = (\n            SELECT\n                avg(raw.Age) AS cleanAge\n            FROM test as raw\n            WHERE raw.Age IS NOT NULL\n        )\n        WHERE Age IS NULL\"\"\")\n# Update the Sex column, change the VARCHAR type to BOOLEAN\ncon.sql(\"\"\"ALTER TABLE test ALTER Sex \n        SET DATA TYPE BOOLEAN USING CASE\n            WHEN Sex = 'female' THEN 1 ELSE 0 END\n        \"\"\")\n# Remove the PassengerId, Name, Cabin, Embarked, Fare, and Ticket columns\ncon.sql(\"ALTER TABLE train DROP PassengerId\") # Has no bearing on the outcome of the model\ncon.sql(\"ALTER TABLE train DROP Name\") # Has to be numeric data\ncon.sql(\"ALTER TABLE train DROP Cabin\")\ncon.sql(\"ALTER TABLE train DROP Embarked\")\ncon.sql(\"ALTER TABLE train DROP Fare\") # Dropping because there are nulls in the test file\ncon.sql(\"ALTER TABLE train DROP Ticket\") # Dropping because of inconsistent values\n\n# Remove the PassengerId, Name, Cabin, Embarked, Fare, and Ticket columns\ncon.sql(\"ALTER TABLE test DROP Name\") # Has to be numeric data\ncon.sql(\"ALTER TABLE test DROP Cabin\")\ncon.sql(\"ALTER TABLE test DROP Embarked\")\ncon.sql(\"ALTER TABLE test DROP Fare\") # Dropping because there are nulls in the test file\ncon.sql(\"ALTER TABLE test DROP Ticket\") # Dropping because of inconsistent values\n\n# Creating dataframes for testing/training, I'll be using sklearn here, which needs both\ntrain = con.sql(\"SELECT * FROM train\").df()\ntest = con.sql(\"SELECT * FROM test\").df()\n# Create features and target\nX = train.drop(\"Survived\", axis = 1).values\ny = train[\"Survived\"].values\n\nX_test = test.drop(\"PassengerId\", axis = 1).values\n# Initialize Regression object and split data\nlogreg = LogisticRegression(penalty = 'l2', tol = np.float64(0.083425), C = np.float64(0.43061224489795924), class_weight = 'balanced')\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.7, random_state = 123)\n# Fit and predict\nlogreg.fit(X_train, y_train)\n\n\n\nLogisticRegression(C=np.float64(0.43061224489795924), class_weight='balanced',\n                   tol=np.float64(0.083425))\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n¬†¬†LogisticRegression?Documentation for LogisticRegressioniFitted\n\nLogisticRegression(C=np.float64(0.43061224489795924), class_weight='balanced',\n                   tol=np.float64(0.083425))\n\n\n\n\n\n# Predict and measure output\ny_pred = logreg.predict(X_val)\ny_pred_probs = logreg.predict_proba(X_val)[:, 1]\nprint(roc_auc_score(y_val, y_pred_probs))\n0.8149262043998886\n# Create Parameter Dictionary for Model Tuning\nkf = KFold(n_splits = 5, shuffle = True, random_state = 123)\nparams = {\n    \"penalty\": [\"l1\", \"l2\"],\n    \"tol\": np.linspace(0.0001, 1.0, 25),\n    \"C\": np.linspace(0.1, 1.0, 50),\n    \"class_weight\": [\"balanced\", {0:0.8, 1:0.2}]\n}\nlogreg_cv = RandomizedSearchCV(logreg, params, cv=kf)\n# Run the parameter search, fit the object, print the output\nlogreg_cv = RandomizedSearchCV(logreg, params, cv=kf)\nlogreg_cv.fit(X_train, y_train)\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\nprint(\"Tuned Logistic Regression Best Accuracy Score: {}\".format(logreg_cv.best_score_))\n# Apply the model to the test set\npredictions = logreg.predict(X_test)\n\nsubmission = pd.DataFrame({\n    'PassengerId': test['PassengerId'],\n    'Survived': predictions\n})\n\nsubmission.head()\n\n\n\n\n\n\n\n\nPassengerId\n\n\nSurvived\n\n\n\n\n\n\n0\n\n\n892\n\n\n0\n\n\n\n\n1\n\n\n893\n\n\n1\n\n\n\n\n2\n\n\n894\n\n\n0\n\n\n\n\n3\n\n\n895\n\n\n0\n\n\n\n\n4\n\n\n896\n\n\n1\n\n\n\n\n\n# Write the file to .csv and submit\ncon.sql(\"SELECT * FROM submission\").write_csv(\"files/titanic_submission.csv\")",
    "crumbs": [
      "Data Science and Machine Learning",
      "Titanic Disaster"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/netflix.html",
    "href": "pages/projects/data_science/posts/netflix.html",
    "title": "Netflix Movies",
    "section": "",
    "text": "Movie popcorn on red background\nNetflix! What started in 1997 as a DVD rental service has since exploded into one of the largest entertainment and media companies.\nGiven the large number of movies and series available on the platform, it is a perfect opportunity to flex your exploratory data analysis skills and dive into the entertainment industry. Our friend has also been brushing up on their Python skills and has taken a first crack at a CSV file containing Netflix data. They believe that the average duration of movies has been declining. Using your friends initial research, you‚Äôll delve into the Netflix data to see if you can determine whether movie lengths are actually getting shorter and explain some of the contributing factors, if any.\nYou have been supplied with the dataset netflix_data.csv , along with the following table detailing the column names and descriptions. This data does contain null values and some outliers, but handling these is out of scope for the project. Feel free to experiment after submitting!",
    "crumbs": [
      "Data Science and Machine Learning",
      "Netflix Movies"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/netflix.html#the-data",
    "href": "pages/projects/data_science/posts/netflix.html#the-data",
    "title": "Netflix Movies",
    "section": "The data",
    "text": "The data\n\nnetflix_data.csv\n\n\n\nColumn\nDescription\n\n\n\n\nshow_id\nThe ID of the show\n\n\ntype\nType of show\n\n\ntitle\nTitle of the show\n\n\ndirector\nDirector of the show\n\n\ncast\nCast of the show\n\n\ncountry\nCountry of origin\n\n\ndate_added\nDate added to Netflix\n\n\nrelease_year\nYear of Netflix release\n\n\nduration\nDuration of the show in minutes\n\n\ndescription\nDescription of the show\n\n\ngenre\nShow genre\n\n\n\n# Importing pandas and matplotlib\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Start coding!\nnetflix_df = pd.read_csv('netflix_data.csv')\nnetflix_subset = netflix_df[netflix_df[\"type\"] == \"Movie\"]\nnetflix_movies = netflix_subset[[\"title\", \"country\", \"genre\", \"release_year\", \"duration\"]]\nnetflix_movies.head()\n\n\n\n\n\n\n\n\ntitle\n\n\ncountry\n\n\ngenre\n\n\nrelease_year\n\n\nduration\n\n\n\n\n\n\n1\n\n\n7:19\n\n\nMexico\n\n\nDramas\n\n\n2016\n\n\n93\n\n\n\n\n2\n\n\n23:59\n\n\nSingapore\n\n\nHorror Movies\n\n\n2011\n\n\n78\n\n\n\n\n3\n\n\n9\n\n\nUnited States\n\n\nAction\n\n\n2009\n\n\n80\n\n\n\n\n4\n\n\n21\n\n\nUnited States\n\n\nDramas\n\n\n2008\n\n\n123\n\n\n\n\n6\n\n\n122\n\n\nEgypt\n\n\nHorror Movies\n\n\n2019\n\n\n95\n\n\n\n\n\nshort_movies = netflix_movies[netflix_movies[\"duration\"]&lt;60]\nshort_movies.head(20)\n\n\n\n\n\n\n\n\ntitle\n\n\ncountry\n\n\ngenre\n\n\nrelease_year\n\n\nduration\n\n\n\n\n\n\n35\n\n\n#Rucker50\n\n\nUnited States\n\n\nDocumentaries\n\n\n2016\n\n\n56\n\n\n\n\n55\n\n\n100 Things to do Before High School\n\n\nUnited States\n\n\nUncategorized\n\n\n2014\n\n\n44\n\n\n\n\n67\n\n\n13TH: A Conversation with Oprah Winfrey & Ava ‚Ä¶\n\n\nNaN\n\n\nUncategorized\n\n\n2017\n\n\n37\n\n\n\n\n101\n\n\n3 Seconds Divorce\n\n\nCanada\n\n\nDocumentaries\n\n\n2018\n\n\n53\n\n\n\n\n146\n\n\nA 3 Minute Hug\n\n\nMexico\n\n\nDocumentaries\n\n\n2019\n\n\n28\n\n\n\n\n162\n\n\nA Christmas Special: Miraculous: Tales of Lady‚Ä¶\n\n\nFrance\n\n\nUncategorized\n\n\n2016\n\n\n22\n\n\n\n\n171\n\n\nA Family Reunion Christmas\n\n\nUnited States\n\n\nUncategorized\n\n\n2019\n\n\n29\n\n\n\n\n177\n\n\nA Go! Go! Cory Carson Christmas\n\n\nUnited States\n\n\nChildren\n\n\n2020\n\n\n22\n\n\n\n\n178\n\n\nA Go! Go! Cory Carson Halloween\n\n\nNaN\n\n\nChildren\n\n\n2020\n\n\n22\n\n\n\n\n179\n\n\nA Go! Go! Cory Carson Summer Camp\n\n\nNaN\n\n\nChildren\n\n\n2020\n\n\n21\n\n\n\n\n181\n\n\nA Grand Night In: The Story of Aardman\n\n\nUnited Kingdom\n\n\nDocumentaries\n\n\n2015\n\n\n59\n\n\n\n\n200\n\n\nA Love Song for Latasha\n\n\nUnited States\n\n\nDocumentaries\n\n\n2020\n\n\n20\n\n\n\n\n220\n\n\nA Russell Peters Christmas\n\n\nCanada\n\n\nStand-Up\n\n\n2011\n\n\n44\n\n\n\n\n233\n\n\nA StoryBots Christmas\n\n\nUnited States\n\n\nChildren\n\n\n2017\n\n\n26\n\n\n\n\n237\n\n\nA Tale of Two Kitchens\n\n\nUnited States\n\n\nDocumentaries\n\n\n2019\n\n\n30\n\n\n\n\n242\n\n\nA Trash Truck Christmas\n\n\nNaN\n\n\nChildren\n\n\n2020\n\n\n28\n\n\n\n\n247\n\n\nA Very Murray Christmas\n\n\nUnited States\n\n\nComedies\n\n\n2015\n\n\n57\n\n\n\n\n285\n\n\nAbominable Christmas\n\n\nUnited States\n\n\nChildren\n\n\n2012\n\n\n44\n\n\n\n\n295\n\n\nAcross Grace Alley\n\n\nUnited States\n\n\nDramas\n\n\n2013\n\n\n24\n\n\n\n\n305\n\n\nAdam Devine: Best Time of Our Lives\n\n\nUnited States\n\n\nStand-Up\n\n\n2019\n\n\n59\n\n\n\n\n\ncolors = []\nfor lab, row in netflix_movies.iterrows():\n    if row['genre'] == \"Children\":\n        colors.append(\"Blue\")\n    elif row['genre'] == \"Documentaries\":\n        colors.append(\"Red\")\n    elif row['genre'] == \"Stand-Up\":\n        colors.append(\"Green\")\n    else:\n        colors.append(\"Black\")\ncolors[:10]\n['Black',\n 'Black',\n 'Black',\n 'Black',\n 'Black',\n 'Black',\n 'Black',\n 'Black',\n 'Black',\n 'Red']\nfig = plt.figure(figsize=(12, 8))\n&lt;Figure size 1200x800 with 0 Axes&gt;\nplt.scatter(netflix_movies['release_year'], netflix_movies['duration'], c=colors)\n&lt;matplotlib.collections.PathCollection at 0x7f49c4f3c430&gt;\n\n\n\npng\n\n\nplt.title(\"Movie Duration by Year of Release\")\nplt.xlabel(\"Release year\")\nplt.ylabel(\"Duration (min)\")\nText(0, 0.5, 'Duration (min)')\n\n\n\npng\n\n\nplt.show()\nanswer = \"no\"\n# Importing pandas and matplotlib\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Read in the Netflix CSV as a DataFrame\nnetflix_df = pd.read_csv(\"netflix_data.csv\")\n\n# Subset the DataFrame for type \"Movie\"\nnetflix_subset = netflix_df[netflix_df[\"type\"] == \"Movie\"]\n\n# Select only the columns of interest\nnetflix_movies = netflix_subset[[\"title\", \"country\", \"genre\", \"release_year\", \"duration\"]]\n\n# Filter for durations shorter than 60 minutes\nshort_movies = netflix_movies[netflix_movies.duration &lt; 60]\n\n# Define an empty list\ncolors = []\n\n# Iterate over rows of netflix_movies\nfor label, row in netflix_movies.iterrows() :\n    if row[\"genre\"] == \"Children\" :\n        colors.append(\"red\")\n    elif row[\"genre\"] == \"Documentaries\" :\n        colors.append(\"blue\")\n    elif row[\"genre\"] == \"Stand-Up\":\n        colors.append(\"green\")\n    else:\n        colors.append(\"black\")\n        \n# Inspect the first 10 values in your list        \ncolors[:10]\n\n# Set the figure style and initalize a new figure\nfig = plt.figure(figsize=(12,8))\n\n# Create a scatter plot of duration versus release_year\nplt.scatter(netflix_movies.release_year, netflix_movies.duration, c=colors)\n\n# Create a title and axis labels\nplt.title(\"Movie Duration by Year of Release\")\nplt.xlabel(\"Release year\")\nplt.ylabel(\"Duration (min)\")\n\n# Show the plot\nplt.show()\n\n# Are we certain that movies are getting shorter?\nanswer = \"no\"\n\n\n\npng",
    "crumbs": [
      "Data Science and Machine Learning",
      "Netflix Movies"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/login_validation.html",
    "href": "pages/projects/data_science/posts/login_validation.html",
    "title": "User Login Validation",
    "section": "",
    "text": "login_img\n\n\nYou recently joined a small startup as a junior developer. The product managers have come to you for help improving new user sign-ups for the company‚Äôs flagship mobile app.\nThere are lots of invalid and incomplete sign-up attempts crashing the app. Before creating new accounts, you suggest standardizing validation checks by writing reusable Python functions to validate names, emails, passwords, etc. The managers love this idea and task you with coding core validation functions for improving sign-ups. It‚Äôs your job to write these custom functions to check all user inputs to ensure they meet minimum criteria before account creation to reduce crashes.\n# Re-run this cell\n# Preloaded data for validating email domain.\ntop_level_domains = [\n    \".org\",\n    \".net\",\n    \".edu\",\n    \".ac\",\n    \".gov\",\n    \".com\",\n    \".io\"\n]\n# Start coding here. Use as many cells as you need.\ndef validate_name(name):\n    if type(name) != str:\n            return False\n    elif len(name) &lt;= 2: \n        return False\n    else:\n        return True\n\ndef validate_email(email):\n    if '@' not in email:\n        return False\n    for domain in top_level_domains:\n        if domain in email:\n            return True\n    return False",
    "crumbs": [
      "Data Science and Machine Learning",
      "Login Validation"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/data_engineering.html",
    "href": "pages/projects/data_engineering/data_engineering.html",
    "title": "Data Engineering and Architecture Projects",
    "section": "",
    "text": "Landing page for all my project posts related to Data Engineering and Architecture.",
    "crumbs": [
      "Data Engineering and Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/data_engineering.html#projects",
    "href": "pages/projects/data_engineering/data_engineering.html#projects",
    "title": "Data Engineering and Architecture Projects",
    "section": "Projects",
    "text": "Projects",
    "crumbs": [
      "Data Engineering and Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "",
    "text": "This guide provides step-by-step instructions and explanations for configuring a Raspberry Pi 4 to learn about hardware, servers, containerization, and self-hosting principles. To be clear, this guide is not-exhaustive and I‚Äôm sure there were areas I made mistakes or misunderstood a topic. I‚Äôm inviting (encouraging) you to let me know! You can submit an issue via GitHub on the guide on my website. That being said, the primary purpose of this guide is so that I can go back and reference what I previously did, as well as understand the thought process, when I need to troubleshoot or recreate something. The secondary purpose is to provide a helpful resource for others in a similar situation to me, because I struggled to find the sort of comprehensive document I‚Äôm aiming to create.\nBack to this guide, eventually I‚Äôd like to setup an actual server cluster and self-host some interesting, more resource-intensive applications. Before I make that kind of commitment, I wanted to learn the basics and see if this is something I would enjoy‚Äì the good news, I learned I do. The great news, Raspberry Pi makes their hardware very affordable and easy to purchase. Here‚Äôs the official webpage for the exact computer I bought.\nIt‚Äôs worth adding, I bought the 8GB Raspberry Pi 4. The price difference isn‚Äôt that great, but the performance is, between the lesser 2GB and 4GB models. Additionally, because I‚Äôm planning to host and experiment with CI/CD, I also bought a case and cooling fan to help with longevity. All in, the base price for that (with the power supply and HDMI cable) is $107.30 before taxes, shipping, and other fees.\nFinally, you‚Äôll see an outline below, you can gloss over it to get a general idea of what we‚Äôll be doing and in what order. At the start of each section I‚Äôll include a key terms list that has all of the fundamental terms which are important for a given topic.\n\n\n\nIntroduction\n\nPurpose and scope of the guide\nWhat you‚Äôll learn and build\nPrerequisites and assumptions\n\nInitial Setup\n\nHardware Requirements\n\nRaspberry Pi 4 specifications\nStorage devices (Thumbdrive, SSD, microSD cards)\nAccessories and peripherals (Keyboard, monitor, etc.)\n\nImage Requirements\n\nSelecting and downloading Ubuntu Server LTS\nUsing Raspberry Pi Imager\nInitial configuration options\n\nGet Started\n\nThe physical setup of the Raspberry Pi, what to plug in, where, etc.\nWhat to expect as things turn on\n\n\nLinux Server Basics\n\nFirst Boot Process\n\nConnection and startup\nUnderstanding initialization\n\nService Management with systemd\n\nUnderstanding systemd units and targets\nBasic service commands\n\nUnderstanding Your Home Directory\n\nShell configuration files\nHidden application directories\n\nThe Root Filesystem\n\nFilesystem Hierarchy Standard (FHS)\nKey directories and their purposes\n\nUser and Group Permissions\n\nBasic permission concepts\nchmod and chown usage\nUnderstanding advanced permissions\n\n\nNetworking Basics\n\nComputer Networking Fundamentals\n\nOSI and TCP/IP models\nKey networking protocols\n\nNetwork Connections\n\nWired vs wireless configurations\nUnderstanding IP addressing\n\nUbuntu Server Networking Tools\n\nTesting connectivity\nViewing network statistics\n\nsystemd-networkd\n\nConfiguration file structure\nWired and wireless setup\n\nConverting Netplan to networkd\n\nWhy and how to transition\nTroubleshooting network issues\n\nAdvanced Networking\n\nSubnets and routing\nSecurity considerations\n\n\nSSH (Secure Shell)\n\nUnderstanding SSH Configuration\n\nClient vs server setup\nKey-based authentication\n\nKey-Based Authentication\n\nTypes of SSH key encryption\nGenerating keys\nInstalling the public key\n\nServer-Side SSH Configuration\n\nHost keys and security options\nOptimizing for security\n\nClient-Side Configuration\n\nSetting up SSH config\nManaging known hosts\n\nAdditional Security Measures\n\nFirewall configuration with UFW\nIntrusion prevention with Fail2Ban\n\nSecure File Transfers\n\nUsing SCP (Secure Copy Protocol)\nUsing rsync for efficient transfers\n\n\nPartitions\n\nPartitioning Basics\n\nUnderstanding partition tables and types\nFilesystem options and considerations\n\nPartitioning Tools\n\nUsing parted and other utilities\n\nPartitioning for Backups\n\nSetting up microSD cards\nMount points and fstab configuration\n\nPartitioning your SSD\n\nBoot and root partitions\nFormatting and preparation\n\nAdvanced Partitioning\n\nMonitoring usage\nResizing partitions\n\n\nBackups and Basic Automation\n\nSetting Up the Backup Directory\n\nDirectory structure and permissions\n\nConfiguration Files Backup\n\nUsing rsync for system configurations\nRemote transfers of backups\n\nRestoring from Backup\n\nCreating restoration scripts\nTesting recovery procedures\n\n\nChanging Your Boot Media Device\n\nBoot Configuration Transition\n\nFlashing OS to new media\nProper shutdown procedures\nPhysically changing boot devices\nTesting the new boot configuration\nRestoring configurations\n\n\nRemote Development with VS Code\n\nSetting Up VS Code Remote SSH\nManaging Remote Projects\nDebugging and Terminal Integration\n\nMonitoring and Maintenance\n\nSystem Monitoring Tools\nLog Management\nPerformance Optimization\nSecurity Updates and Patching\n\nContainerization with Docker\n\nContainerization and Virtualization Basics\nDocker Installation and Setup\nCreating Images with Dockerfile\nManaging Images and Containers\nDocker Compose for Multi-container Applications\nCI/CD Integration\n\nContainer Orchestration with Kubernetes\n\nOrchestration Basics\nKubernetes Concepts, Installation, and Setup\nSetting Up a Kubernetes Cluster\nDeployment Strategies\nManaging Resources\nScaling Applications"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#initial-setup",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#initial-setup",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Initial Setup",
    "text": "Initial Setup\n\nHardware Requirements\nThis section will provide basic setup instructions, so you‚Äôll have the same tools I do and can follow along with this guide, step-by-step.\n\nRaspberry Pi 4 8GB\n\nMicro HDMI to HDMI cord (for direct access)\nProtective case\nCooling fan\nAppropriate Power Supply\n\nKeyboard (connected via USB for direct access)\nMonitor (for direct access)\n1TB Samsung T7 SSD (connected via USB for boot media/core memory)\n64GB Generic Flash Drive (used as the boot media when partitioning the SSD)\nAmazon Basics 128GB microSD card (or other microSD cards for backups media)\nSSH-capable devices for headless access\n\nI‚Äôm using a MacBook Air\nI prefer the MacOS and Terminal for personal development, because I use Windows at my day job\n\n\n\n\nImage requirements\nOnce you have your hardware ready to go, you can being setting up the software. I‚Äôm using Linux Server LTS because it‚Äôs a stable version of Linux intended for headless, server environments. LTS means long-term support, so unlike the more frequently updated versions, these OS versions are supported for 5 years. Additionally, you‚Äôll want to use public-key authentication for better security purposes, but more on that in the SSH Section.\n\nHave your Thumb Drive ready and able to connect to either a laptop or desktop (whichever you plan to use with SSH)\nDownload Raspberry Pi Imager from the official website\nRun the Imager and configure your installation of the most recent Linux Server LTS image\n\nSelect your Raspberry Pi device\n\n\n\nSelect the OS Image you want to flash\n\n\n\nSelect the media storage device for the image\n\n\n\nConfigure settings\n\n Here, you‚Äôll configure your primary user ID and password; network connection; locale and timezone; and your hostname (the nickname your computer remembers the IP address as).\n Here, you‚Äôll configure your SSH settings. You should probably use public-key authentication only when dealing with SSH in your leave, but for learning purposes, you don‚Äôt need to at this time. Later on in this guide, I‚Äôll walk you through the steps to manually configure SSH, if you are unfamiliar.\n These are more preference based, but it‚Äôs nice to have the storage device automatically eject once the flashing is complete. Then, you just need to unplug it and plug it into your Raspberry Pi to get going.\n\n\n\nGet Started\nIt‚Äôs time to get the actual Raspberry Pi device setup. For most of this guide, I recommend leaving the Pi outside of the case, because it‚Äôll be easier to plug and unplug some of the devices‚Äì the microSD slot is not accessible while the case is on. Later, once we‚Äôve got everything configured and setup as we like, we will attach the fan and case, so it‚Äôs a bit safer and able to run in an always-on state. I‚Äôll share a picture of what my server looks like during the early development, and then later I‚Äôll show what it looks like with everything connected and setup.\n\nNow you‚Äôre ready to plug your boot media device (the Thumb Drive) into your Raspberry Pi. You should also connect a keyboard, monitor, and power supply. Once all of this is connected, your Raspberry Pi will boot up. Connecting a monitor and keyboard will allow you to directly interact with the system‚Äôs terminal. Ideally, you‚Äôll use SSH, but it may be helpful to have direct access in case there are any network issues. Eventually, the SSD will serve as the boot media and primary storage device for the server; however, we can‚Äôt modify its partitions while it‚Äôs serving as the boot device. So, we‚Äôll use a thumb drive as the boot media device, until we complete the partitions.\nWhen first connecting from the wired keyboard and monitor, let all of the start up processes finish running (these will hopefully have brackets with the word Success in green). Then, type in the name of the User ID you wish to login with, in my case it‚Äôs chris. Then, enter the password (no characters will show up as you type it in) and hit enter. You‚Äôll see a plaintext message telling you the OS version, some system information (memory usage, temperature, etc.), and you‚Äôll see a line where you can enter commands (the CLI). In my case, it looks like this: chris@ubuntu-pi-server:~$\nNow you can run some basic commands to see where you are and what you have available to you. Spoiler alert, you‚Äôre in your home directory and have no files. In my case it‚Äôs /home/chris, where the /home directory is owned by root and /chris is owned by my user‚Äì UID 1000 (the default for new users on a fresh system/image). Right now your directory will be empty, outside of some hidden folders like .ssh. More on this later.\nNext we‚Äôll cover what happened during the boot process, the basic structure of the Linux Server OS, and some important information related to permissions, before we move on to basic networking concepts and configurations."
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#networking-basics",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#networking-basics",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Networking Basics",
    "text": "Networking Basics\nThis section provides a brief example of how to connect your server to WiFi. It assumes you are already connected using the wireless network settings you configured in the Requirements Section. That being said, I‚Äôll also go over some basic networking concepts and background information. As a result, some of the decisiions and terminology in this guide will make more sense (it also helps me remember what I‚Äôm doing and why).\n\nComputer Networking\nSimply put, a computer network is a collection of interconnected devices that can communicate with each other using a set of rules called protocols. Networking allows devices to share resources, exchange data, and collaborate on tasks. On a deeper level, it helps to understand the conceptual models that describes how data moves through a network. Before we dive in, let‚Äôs go over some basic terminology.\n\nKey Terms\nBasic Networking Concepts:\n\nProtocol: A set of rules that determine how data is transmitted between devices on a network. Examples include TCP, UDP, and HTTP.\nMAC Address: Media Access Control address; a unique hardware identifier assigned to network interfaces. It‚Äôs a 48-bit address (e.g., 00:1A:2B:3C:4D:5E) permanently assigned to a network adapter.\nIP Address: A numerical label assigned to each device on a network that uses the Internet Protocol. Functions like a postal address for devices.\nPacket: A unit of data transmitted over a network. Includes both the data payload and header information for routing.\nSubnet: A logical subdivision of an IP network that allows for more efficient routing and security segmentation.\nGateway: A network node that serves as an access point to another network, typically connecting a local network to the wider internet.\nDNS: Domain Name System; translates human-readable domain names (like google.com) into IP addresses computers can understand.\nDHCP: Dynamic Host Configuration Protocol; automatically assigns IP addresses and other network configuration parameters to devices.\n\nNetwork Types and Components:\n\nLAN: Local Area Network; a network confined to a small geographic area, like a home or office.\nWAN: Wide Area Network; connects multiple LANs across large geographic distances.\nRouter: A device that forwards data packets between computer networks, determining the best path for data transmission.\nSwitch: A networking device that connects devices within a single network and uses MAC addresses to forward data to the correct destination.\nBandwidth: The maximum data transfer rate of a network connection, measured in bits per second (bps).\nLatency: The delay between sending and receiving data, typically measured in milliseconds.\n\nLinux Networking Terminology:\n\nInterface: A connection between a device and a network. In Linux, these have names like eth0 (Ethernet) or wlan0 (wireless).\nNetplan: Ubuntu‚Äôs default network configuration tool that uses YAML files to define network settings.\nsystemd-networkd: A system daemon that manages network configurations in modern Linux distributions.\nNetworkManager: An alternative network management daemon that provides detection and configuration for automatic network connectivity.\nSocket: An endpoint for sending or receiving data across a network, defined by an IP address and port number.\n\nSecurity Concepts:\n\nFirewall: Software or hardware that monitors and filters incoming and outgoing network traffic based on predetermined security rules.\nSSH: Secure Shell; a cryptographic network protocol for secure data communication and remote command execution.\nEncryption: The process of encoding information to prevent unauthorized access.\nPort: A virtual point where network connections start and end. Ports are identified by numbers (0-65535).\nNAT: Network Address Translation; allows multiple devices on a local network to share a single public IP address.\nVPN: Virtual Private Network; extends a private network across a public network, enabling secure data transmission.\n\n\n\nThe OSI Model\nNow that you understand some common terms and concepts, we can dive into the conceptual models. The Open Systems Interconnection (OSI) Model divides networking into seven layers, each handling specific aspects of network communication.\n\nPhysical Layer: Physical medium, electrical signals, cables, and hardware\nData Link Layer: Physical addressing (MAC addresses), error detection\nNetwork Layer: Logical addressing (IP addresses), routing\nTransport Layer: End-to-end connections, reliability (TCP/UDP)\nSession Layer: Session establishment, management, and termination\nPresentation Layer: Data translation, encryption, compression\nApplication Layer: User interfaces and services (HTTP, SMTP, etc.)\n\n\n\nThe TCP/IP Model\nThe OSI Model is conceptual, but the TCP/IP Model is more practical and has four layers.\n\nNetwork Access Layer:Combines OSI‚Äôs Physical and Data Link layers\nInternet Layer: Similar to OSI‚Äôs Network layer (IP)\nTransport Layer: Same as OSI‚Äôs Transport layer (TCP/UDP)\nApplication Layer: Combines OSI‚Äôs Session, Presentation, and Application layers\n\n\n\nNetwork Protocols\nRemember, a protocol is a set of rules that determine how data is transmitted between devices on a network. You can think of protocols in one of two camps, Connection-Oriented and Connectionless. Within these camps, two protocols stand out as the backbone of the internet‚Äôs data transfers: TCP and UDP.\nTCP (Transmission Control Protocol) is a connection-oriented protocol that establishes a dedicated end-to-end connection before transmitting data. TCP is used when reliability is more important than speed (e.g., web browsing, email, file transfers). It has four defining traits:\n\nReliability: Guarantees delivery of packets in the correct order\nFlow Control: Prevents overwhelming receivers with too much data\nError Detection: Identifies and retransmits lost or corrupted packets\nHandshake Process: Three-way handshake establishes connections\n\nUDP (User Datagram Protocol) is a connectionless protocol that sends data without establishing a dedicated connection. UDP is used for real-time applications (e.g., video streaming, VoIP, online gaming). It also has four defining traits:\n\nSimplicity: No connection setup or maintenance overhead\nSpeed: Faster than TCP due to fewer checks and guarantees\nLower Reliability: No guarantee of delivery or correct ordering\nEfficiency: Better for real-time applications where occasional data loss is acceptable\n\nBeyond those, there are some other important protocols to know, because they provide the foundation for most of the user friendly features we are used to today.\n\nIP (Internet Protocol)\n\nIP handles addressing and routing of packets across networks. There are two versions in common use:\nIPv4: 32-bit addresses (e.g., 192.168.1.1)\nIPv6: 128-bit addresses (e.g., 2001:0db8:85a3:0000:0000:8a2e:0370:7334)\n\nICMP (Internet Control Message Protocol)\n\nICMP helps diagnose network issues by sending error messages and operational information. The ping command uses ICMP to test connectivity.\n\nHTTP/HTTPS (Hypertext Transfer Protocol)\n\nHTTP and its secure variant HTTPS are application-layer protocols used for web browsing.\n\nDNS (Domain Name System)\n\nDNS translates human-readable domain names (like google.com) into IP addresses.\n\n\n\n\n\nNetwork Connections\nThere are two ways for systems to connect to the internet: wired and wireless.\n\nWired Connections\nEthernet is the most common wired networking technology. Its name comes from the term ether referring to a theoretical medium that was believed to carry light waves through space. It was developed by Robert Metcalf and David Boggs at Xerox‚Äôs PARC facility in the 1970s. The goal was to provide a more stable LAN which could facilitate high speed transfers between computers and laser printers. They succeeded, and had improved on a precursor‚Äôs, ALOHAnet, design by creating a system that could detect collisions‚Äì when two devices try to transmit at the same time. Here are some key traits:\n\nReliability: Less susceptible to interference\nSpeed: Typically faster and more stable than wireless\nSecurity: Harder to intercept without physical access\nConnectors: RJ45 connectors on Ethernet cables\nStandards: 10/100/1000 Mbps (Gigabit) are common speeds\n\n\n\nWireless Connections\nWi-Fi allows devices to connect to networks without physical cables. Its name is not short for Wireless Fidelity, but actually a marketing choice by the brand-consulting firm Interbrand. They chose the name because it sounded similar to Hi-Fi. Wi-Fi was developed by numerous researchers and engineers, but the key breakthrough was by Dr.¬†John O‚ÄôSullivan from CSIRO in Australia. His work focused on a wireless LAN, which would eventually become the IEEE (Institute of Electrical and Electronics Engineers) 802.11 standard in 1997. Eventually, Apple would help with widespread adoption by including the AirPort feature on its laptops, enabling W-Fi connectivity out of the box. Here are some key traits:\n\nConvenience: No cables required, more flexible placement\nStandards: 802.11a/b/g/n/ac/ax (Wi-Fi 6) with varying speeds and ranges\nSecurity: WEP, WPA, WPA2, and WPA3 encryption standards (WPA2/WPA3 recommended)\n\n\n\nNetwork Interface Names in Linux\nIn Ubuntu Server, network interfaces follow a predictable naming convention:\n\neth0, eth1: Traditional Ethernet interface names\nwlan0, wlan1: Traditional wireless interface names\nenp2s0, wlp3s0: Modern predictable interface names (based on device location)\n\n\n\nIP Addressing\nAn IP (Internet Protocol) Address, is a unique identifier for a device on the internet, or a LAN. There are two different kinds of addresses: IPv4 and IPv6.\nIPv4 uses 32-bit addresses, providing approximately 4.3 billion unique addresses (now largely exhausted):\n\nFormat: Four octets (numbers 0-255) separated by dots (e.g., 192.168.1.1)\nClasses: Traditionally divided into classes A, B, C, D, and E\nPrivate Ranges:\n\n10.0.0.0 to 10.255.255.255 (10.0.0.0/8)\n172.16.0.0 to 172.31.255.255 (172.16.0.0/12)\n192.168.0.0 to 192.168.255.255 (192.168.0.0/16)\n\nSubnet Masks: Used to divide networks (e.g., 255.255.255.0 or /24)\nIssues: IPv4 address exhaustion due to limited capacity\n\nIPv6 uses 128-bit addresses, providing approximately 3.4√ó10^38 unique addresses:\n\nFormat: Eight groups of four hexadecimal digits separated by colons (e.g., 2001:0db8:85a3:0000:0000:8a2e:0370:7334)\nShorthand: Leading zeros in a group can be omitted, and consecutive groups of zeros can be replaced with :: (only once)\n\nExample: 2001:db8:85a3::8a2e:370:7334\n\nAddress Types:\n\nUnicast: Single interface\nAnycast: Multiple interfaces (closest responds)\nMulticast: Multiple interfaces (all respond)\n\nBenefits: More addresses, improved security, simplified headers, no need for NAT\n\nOne final note, CIDR (Classless Inter-Domain Routing) notation represents IP addresses and their associated routing prefix:\n\nFormat: IP address followed by ‚Äú/‚Äù and prefix length (e.g., 192.168.1.0/24)\nCalculation: A prefix of /24 means the first 24 bits are the network portion, leaving 8 bits for hosts (allowing 2^8 = 256 addresses)\n\n\n\n\nUbuntu Server Networking Tools\nNow that we‚Äôve covered the basic concepts, it‚Äôs time to dive into the actual commands and tools that will let you configure and manage your server‚Äôs network. To start, you can view network interfaces and their statuses using the command ip link show, or ip addr show for your IP Address configuration. You can view only the IPv4 or IPv6 addresses using ip -4 addr or ip -6 addr, respectively.\n\nTesting Connectivity\nAlthough it seems redundant if you already viewed your IP addresses, you can also test connectivity using the ping and traceroute commands. These will be more useful for checking your servers network status from your desktop or laptop.\nTest basic connectivity to a host:\nping -c 4 google.com\n\nTrace the route to a destination:\n# First update and install your packages\nsudo apt update && sudo apt upgrade -y\n\n# Install traceroute\nsudo apt install traceroute -y\n\n# Run traceroute\ntraceroute google.com\n\nCheck the DNS resolution:\nnslookup google.com\n# dig google.com\n\n\n\nViewing Network Statistics\nYou can view more specific network information with the ss command. This command‚Äôs name is an acronym for socket statistics and is used as a replacement for the older netstat plan because it offers faster performance and a more detailed output. Additionally, you can filter by specific protocol.\nss -tuln\nThe tuln flag is made up of four separate flags:\n\n-t, displays only TCP sockets\n-u, displays only UDP sockets\n-l, displays listening sockets\n-n, displays address numerically, instead of resolving them\n\n\n\nConfiguration Files\nFinally, there are a few crucial configuration files that will handle the bulk of your networking. In Ubuntu Server, network interfaces and DNS configurations are configured and stored in the /etc/ directory.\nNetwork Interfaces:\n\n/etc/netplan/: Contains YAML configuration files for Netplan\n/etc/network/interfaces: Configuration method (if NetworkManager is used)\n\nDNS Configuration:\n\n/etc/resolv.conf: DNS resolver configuration\n/etc/hosts: Static hostname to IP mappings\n/etc/hostname: System hostname\n\n\n\n\nsystemd-networkd\nsystemd-networkd is a system daemon that manages network configurations in modern Linux distributions. It‚Äôs part of the systemd suite and provides network configuration capabilities through simple configuration files.\nIt generally works using three key components:\n\nConfiguration Files: You define network settings in .network files located in /etc/systemd/network/\nService Management: systemd-networkd runs as a system service to apply and maintain network configurations\nIntegration: Works closely with other systemd components for DNS resolution and networking\n\n\nBasic Wired Configuration\nsystemd-networkd uses configuration files with .network extension. Each file consists of sections with key-value pairs. A basic configuration for a static IP would look like this:\n# /etc/systemd/network/20-wired.network\n[Match]\nName=eth0\n\n[Network]\nAddress=192.168.1.100/24\nGateway=192.168.1.1\nDNS=8.8.8.8\nDNS=8.8.4.4\nLet‚Äôs walk through the configuration file‚Äôs structure:\n\nFile Naming Convention:\n\nThe file is named 20-wired.network.\nThe number prefix (20-) determines the processing order (lower numbers processed first), allowing you to create prioritized configurations.\nThe suffix .network tells systemd-networkd that this is a network interface configuration file.\n\n[Match] Section:\n\nThis critical section determines which network interfaces the configuration applies to.\nName=eth0: This specifies that the configuration should apply to the eth0 interface only.\nYou can use wildcards (e.g., eth* would match all Ethernet interfaces) or match by other properties such as MAC address using MACAddress=xx:xx:xx:xx:xx:xx.\nBehind the scenes:\n\nsystemd-networkd scans all available network interfaces.\nCompares their properties against those specified in the [Match] section.\nIf all properties match, the configuration is applied to that interface.\n\n\n[Network] Section:\n\nThis section defines the network configuration parameters.\nAddress=192.168.1.100/24: Sets a static IPv4 address with CIDR notation. The /24 represents the subnet mask (equivalent to 255.255.255.0) and defines the network boundary.\nGateway=192.168.1.1: Specifies the default gateway for routing traffic outside the local network. All traffic not destined for the local subnet (192.168.1.0/24) will be sent to this IP address.\nDNS=8.8.8.8 and DNS=8.8.4.4: These are Google‚Äôs public DNS servers. When specified, systemd-networkd will automatically configure /etc/resolv.conf through systemd-resolved. You can specify multiple DNS servers, and they will be tried in order.\n\nBehind the scenes:\n\nsystemd-networkd identifies the eth0 interface\nAssigns the static IP address using kernel netlink sockets\nSets up the routing table to use the specified gateway\nCommunicates with systemd-resolved to configure DNS settings\nMaintains this configuration and reapplies it if the interface goes down and back up\n\n\nThis configuration example works well for server environments where static, predictable networking is preferable. This is a declarative configuration, it describes the desired state, rather than the steps to achieve it, so repeated application produces the same result.\n\n\nDHCP with a Wired Connection\nIf you want to add DHCP, you can use the following:\n# /etc/systemd/network/20-wired.network\n[Match]\nName=eth0\n\n[Network]\nDHCP=yes\nLet‚Äôs walk through the differences between a dynamic and static host configuration file structure:\n\nDHCP=yes: This single line replaces all the static configuration parameters from the previous example.\n\nIt instructs systemd-networkd to obtain IP address, subnet mask, gateway, DNS servers, and other network parameters automatically from a DHCP server.\nYou can also use DHCP=ipv4 to enable only IPv4 DHCP, or DHCP=ipv6 for only IPv6 DHCP, or DHCP=yes for both.\n\nBehind the scenes:\n\nsystemd-networkd identifies the eth0 interface\nInitiates the DHCP client process, which follows the DHCP protocol‚Äôs Discover-Offer-Request-Acknowledge (DORA) sequence:\n\nThe client broadcasts a DISCOVER message\nAvailable DHCP servers respond with OFFER messages\nThe client selects an offer and sends a REQUEST\nThe selected server sends an ACKNOWLEDGE\n\nApplies all the received network parameters (IP, subnet, gateway, DNS)\nSets up a lease timer to manage when the configuration needs renewal\nHandles DHCP lease renewals automatically\n\nAdvantages:\n\nSimplified configuration maintenance - no need to update parameters when network details change\nWorks well in networks where IP assignments are centrally managed\nAutomatically adapts to network changes\n\n\nThis configuration works well for environments where network parameters are dynamic or managed by a network admin through DHCP.\n\n\nWireless Configurations and wpa_supplicant\nWhile wired connections are a basic part of networking, wireless connections require some extra work. More specifically, with systemd-networkd, you‚Äôll need a tool like WPA. Wi-Fi Protected Access (WPA) emerged as a response to weaknesses in the original Wired Equivalent Privacy (WEP) security protocol. As wireless networks became ubiquitous, secure authentication and encryption mechanisms became essential. The Linux ecosystem offers several powerful tools for managing these connections:\n\nwpa_supplicant: The core daemon that handles wireless connections\nwpa_cli: A command-line interface for controlling wpa_supplicant dynamically\nwpa_passphrase: A utility for generating secure password hashes\n\nOn the systemd-networkd side of things, the configuration is simple, broken down in detail below.\n# /etc/systemd/network/25-wireless.network\n[Match]\nName=wlan0\n\n[Network]\nDHCP=yes\n\nWireless Interface:\n\nThe configuration targets wlan0, which is the traditional name for the first wireless network interface in Linux.\n\nMinimal Configuration:\n\nThe file only has the information needed by systemd-networkd to manage the IP addressing aspect of the wireless connection. Note what‚Äôs missing: there‚Äôs no SSID, password, or security protocol information. This is because:\nsystemd-networkd isn‚Äôt designed to handle wireless authentication and association\nThis separation of concerns is intentional in the systemd design philosophy - specialized tools should handle specialized tasks\n\nIntegration with wpa_supplicant:\n\nwpa_supplicant is the standard Linux utility for managing wireless connections\nsystemd-networkd handles the network layer (Layer 3) configuration once wpa_supplicant establishes the data link layer (Layer 2) connection\nThis division follows the OSI model‚Äôs separation of network layers\n\nBehind the scenes:\n\nwpa_supplicant handles wireless scanning, authentication, and association\nOnce a wireless link is established, it notifies the system\nsystemd-networkd detects the active interface that matches wlan0\nIt then initiates the DHCP client process to configure the network parameters\n\nThis separation provides flexibility and security\n\nThe wireless security operations are handled by a dedicated, well-tested component\nNetworking remains under systemd-networkd‚Äôs control for consistency with other interfaces\n\n\nWhile the systemd-networkd configuration is straightforward, things get more complicated with WPA. In standard wpa_supplicant configuration files, wireless passwords are often stored in plaintext. This creates a security vulnerability - anyone with access to the configuration file can view the password.\n# /etc/wpa_supplicant/wpa_supplicant-wlan0.conf\nctrl_interface=/run/wpa_supplicant\nupdate_config=1\n\nnetwork={\n    ssid=\"YourNetworkSSID\"\n    psk=\"YourWiFiPassword\"\n}\nThe wpa_passphrase tool solves this problem by generating a pre-computed hash of the password. Running this is straightforward as the basic syntax is wpa_passphrase [SSID] [passphrase]. Then, WPA outputs a hashed version of your password.\n# Generate a hashed passphrase\nwpa_passphrase \"MyHomeNetwork\" \"MySecurePassword123\"\nTo then use the hashed password in your configuration, you can run the following command, just make sure to remove the line with the plaintext password from the config file after runtime:\n# Generate the hash and save directly to the configuration file\nwpa_passphrase \"MyHomeNetwork\" \"MySecurePassword123\" | sudo tee -a /etc/wpa_supplicant/wpa_supplicant-wlan0.conf\nWhen you use wpa_passphrase: - It combines the SSID and password using the PBKDF2 (Password-Based Key Derivation Function 2) algorithm - It applies 4096 iterations of HMAC-SHA1 for key strengthening - The result is a 256-bit (32-byte) hash represented in hexadecimal format - This hash is what‚Äôs actually used for the authentication process, not the original password\nThis approach makes it virtually impossible to reverse-engineer the original password from the hash.\n# /etc/wpa_supplicant/wpa_supplicant-wlan0.conf\nctrl_interface=/run/wpa_supplicant\nupdate_config=1\n\nnetwork={\n    ssid=\"MyHomeNetwork\"\n    #psk=\"MySecurePassword123\"\n    psk=a8e665b82929d810746c5a1208c472f9d2a25db67a6bc32a99fa4158aea02175\n}\nNow that you have an idea about the basic structure of this file, lets go over some key points:\n\nFile Naming Convention:\n\nThe file wpa_supplicant-wlan0.conf is specifically named to associate with the wlan0 interface.\nThis naming allows different wireless interfaces to have different configurations.\n\nConfiguration Directives:\n\nctrl_interface=/run/wpa_supplicant: This specifies the control interface path, which is a socket that allows programs to communicate with wpa_supplicant. This enables tools like wpa_cli to connect and control wpa_supplicant dynamically.\nupdate_config=1: Allows wpa_supplicant to update the configuration file automatically, useful when network details change or when using wpa_cli to add networks interactively.\n\nNetwork Block:\n\nThe network={} block defines a single wireless network configuration.\nssid=\"YourNetworkSSID\": The Service Set Identifier - the name of the wireless network to connect to.\npsk=\"YourWiFiPassword\": The Pre-Shared Key - the password for the wireless network in plaintext.\n\nSecurity Considerations:\n\nWhen you enter the password in plaintext as shown, wpa_supplicant will automatically convert it to a hash during processing.\nFor better security, you can pre-hash the password using: wpa_passphrase ‚ÄúYourNetworkSSID‚Äù ‚ÄúYourWiFiPassword‚Äù and use the generated hash.\nThe configuration file should have restricted permissions (600) to prevent other users from reading the passwords.\n\nBehind the scenes:\n\nwpa_supplicant reads this configuration at startup\nIt scans for available wireless networks\nWhen it finds the specified SSID, it attempts to authenticate using the provided credentials\nIt handles all the wireless protocol handshakes, including:\n\nAuthentication and association with the access point\nNegotiation of encryption parameters\nEstablishment of the encrypted channel\n\n\n\nOnce connected, it maintains the connection and handles roaming between access points with the same SSID. This configuration represents the minimum needed for a WPA/WPA2 Personal network connection. For more complex scenarios like enterprise authentication (WPA-EAP), additional parameters would be needed in the network block.\nWhile the wpa_supplicant configuration files provide static configuration that saves when you write out, wpa_cli offers interactive, dynamic control over wireless connections. First ensure wpa_supplicant is running with a control interface by using ps aux | grep wpa_supplicant. If it‚Äôs running with the -c flag pointing to a config file that contains the ctrl_interface=/run/wpa_supplicant line, you can connect to it.\n\n\n\n\n\n\nImportant\n\n\n\nAs a heads up, because we already created the wlan0 configuration file manually, the following steps are just for your knowledge. You‚Äôll probably get some messages saying FAIL if you try to run some of the commands, but I think it‚Äôs good to learn them anyways‚Äì even though they aren‚Äôt necessarilly important right now.\n\n\nFirst, start the interactive mode with sudo wpa_cli, or specify the interface with sudo wpa_cli -i wlan0. Let‚Äôs go over some essential commands:\n# Show help\nhelp\n\n# List all available commands\nhelp all\n\n# List available networks\nscan\nscan_results\n\n# Show current status\nstatus\n\n# List configured networks\nlist_networks\n\n# Add a new network\nadd_network\nStep-By-Step: Adding a Network\n&gt; add_network\n0\n&gt; set_network 0 ssid \"MyNetwork\"\nOK\n&gt; set_network 0 psk \"MyPassword\"\nOK\n&gt; set_network 0 priority 5 \nOK\n&gt; enable_network 0\nOK\n&gt; save_config\nOK\n\n# For networks with hashed passwords\n&gt; add_network\n0\n&gt; set_network 1 ssid \"MyNetwork\"\nOK\n&gt; set_network 1 psk 0a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0u1v2w3x4y5z\nOK\n&gt; set_network 1 priority 10\nOK\n&gt; enable_network 1\nOK\n&gt; save_config\nOK\nIt‚Äôs good to know that higher priority values (like 10), are preferred over lower ones (like 5). Now, you can also use wpa by running one off commands or writing scripts with non-interactive mode. Additionally, you should know that when you first boot up your Raspberry Pi, the internet will be managed by Netplan (more on that later in this section). So, if you try to use wpa_cli save_config after creating the config files, that will return FAIL. Instead, once you write the files in the proper directories, run the reconfigure command.\n# Scan for networks\nsudo wpa_cli scan\nsudo wpa_cli scan_results\n\n# Save the current configuration\nsudo wpa_cli save_config\n\n# Reconnect to the network\nsudo wpa_cli reconfigure\nFinally, you can monitor signal quality and connection status by using the signal_poll command. The RSSI (Received Signal Strength Indicator) shows connection quality in dBm. Values closer to 0 indicate stronger signals. Additionally, you can debug connection issues using status.\n&gt; signal_poll\nRSSI=-67\nLINKSPEED=65\nNOISE=9999\nFREQUENCY=5220\n\n&gt; status\nbssid=00:11:22:33:44:55\nfreq=5220\nssid=MyNetwork\nid=0\nmode=station\npairwise_cipher=CCMP\ngroup_cipher=CCMP\nkey_mgmt=WPA2-PSK\nwpa_state=COMPLETED\nip_address=192.168.1.100\nNow that we‚Äôve covered a lot of the great features available with wpa_cli, it‚Äôs time to continue configuring our server. You may remember me mentioning that the Raspberry Pi default networking tool is Netplan. Before we can enable and start the wlan0 service (meaning your primary wifi is on), we need to safely shut down Netplan.\n\n\n\nConverting Netplan to networkd\nIt‚Äôs no surprise that Raspberry Pi uses Netplan as the default network manager because it provides a consistent interface for network configuration; however, there are several reasons you might want to use systemd-networkd directly:\n\nSimplicity: Direct systemd-networkd configuration eliminates a layer of abstraction\nControl: Direct access to all of systemd-networkd‚Äôs features without Netplan‚Äôs limitations\nIntegration: Better alignment with other systemd components\nLearning: Understanding the underlying network configuration system\nPerformance: Potentially faster setup without the translation layer\n\nUbuntu Server uses a layered approach to network configuration:\n\nUser configuration layer: YAML files in /etc/netplan/\nTranslation layer: Netplan reads YAML files and generates configurations for a backend\nBackend layer: Either systemd-networkd or NetworkManager applies the actual configuration\n\nBy removing the middle layer (Netplan), we‚Äôre configuring the backend directly. As you can see from the previous parts of this Networking section in the guide, I like the learning value and longterm potential of systemd, which is why I went with it over Netplan.\n\nStep-by-step Migration\n\nBegin by creating backups of your current network configuration\n\n# Create a backup directory\nsudo mkdir -p /etc/netplan/backups\n\n# Copy all netplan config files\nsudo cp /etc/netplan/*.yaml /etc/netplan/backups/\n\n# Document the current network state\nsudo ip -c addr | sudo tee /etc/netplan/backups/current-ip-addr.txt\nsudo ip -c route | sudo tee /etc/netplan/backups/current-ip-route.txt\n\nReview your existing Netplan so you know what to recreate\n\n# View your current netplan configs\ncat /etc/netplan/*.yaml\n\nNow, create the corresponding systemd-networkd configuration files in /etc/systemd/network/.\n\nFor each interface (wired, wireless, etc.) in your Netplan configuration, create a corresponding .network file, with the appropriate configurations (i.e.¬†static vs.¬†DHCP).\nRemember: For wireless connections, you need both a systemd and a wpasupplicant configuration.\n\n\n# Create the directory if it doesn't exist\nsudo mkdir -p /etc/systemd/network/\n\n# For an Ethernet configuration\nsudo nano /etc/systemd/network/20-wired.network\n\n# For a Wireless configuration\nsudo nano /etc/systemd/network/25-wireless.network\nsudo nano /etc/wpa_supplicant/wpa_supplicant-wlan0.conf\n\nNow that your networkd configuration is in place, disable Netplan.\n\n# Ensure systemd-networkd is enabled\nsudo systemctl enable systemd-networkd\nsudo systemctl enable systemd-resolved\nsudo systemctl start systemd-networkd\n\n# Move the Netplan configurations to a disabled state\nsudo mkdir -p /etc/netplan/disabled\nsudo mv /etc/netplan/*.yaml /etc/netplan/disabled/\n\n# Create a minimal netplan configuration that defers to systemd-networkd\nsudo tee /etc/netplan/01-network-manager-all.yaml &gt; /dev/null &lt;&lt; EOF\nnetwork:\n  version: 2\n  renderer: networkd\nEOF\nLet‚Äôs take a look at the systemctl enable and start commands, because without them we will lose connectivity when turning off netplan. Before we do that, however, what the minimal netplan configuration file does is essentially tell Netplan to just use networkd. We‚Äôll remove it once we are sure everything is up and running.\n\nEnable Command:\n\nsudo systemctl enable systemd-networkd: This configures systemd-networkd to start automatically when the system boots.\nBehind the scenes:\n\nThis command creates the necessary symbolic links in systemd‚Äôs unit directories so that the network daemon will be started by systemd during the boot process.\nIt integrates the service into systemd‚Äôs dependency tree.\nWithout this step, you would need to manually start networkd after each reboot, which is impractical for a server environment.\n\n\nStart Command:\n\nsudo systemctl start systemd-networkd: This launches the systemd-networkd daemon immediately.\nBehind the scenes:\n\nsystemd spawns the networkd process, which then:\n\nReads all .network, .netdev, and .link configuration files in /etc/systemd/network/ and /usr/lib/systemd/network/\nApplies the configurations to matching interfaces\nSets up monitoring for network changes\n\n\n\nRestart Command:\n\nsudo systemctl restart systemd-networkd: This stops and then starts the daemon again, ensuring all configuration changes are applied.\nBehind the scenes:\n\nsystemd sends a termination signal to the running networkd process, waits for it to exit cleanly, and then starts a new instance.\nThe new instance repeats the initialization process, reading all configuration files again.\nThis is the command you‚Äôll use most frequently when making changes to network configurations.\n\n\nWhy Restart Is Necessary:\n\nWhile systemd-networkd does monitor for some changes, editing configuration files doesn‚Äôt automatically trigger a reconfiguration.\nThe restart ensures that:\n\nAll new or modified configuration files are re-read\nAny removed configurations are no longer applied\nAll interface configurations are freshly evaluated against the current state\n\n\nImpact on Network Connectivity:\n\nA restart will temporarily disrupt network connectivity as interfaces are reconfigured\nFor remote servers, use caution when restarting network services to avoid losing your connection\nFor critical remote systems, consider using a command pipeline, like:\n\nsudo systemctl restart systemd-networkd.service || (sleep 30 && sudo systemctl start systemd-networkd.service)\nWhich attempts to restart and then tries to start the service again after 30 seconds if connectivity is lost\n\n\n\n\nApply the systemd-networkd network configuration\n\n# Apply Netplan changes (this will do nothing as we now have a minimal config)\nsudo netplan apply\n\n# Restart systemd-networkd to apply our direct configuration\nsudo systemctl restart systemd-networkd\n\n# The next step is to enable and start the wpa_supplicant service.\n\nsudo systemctl enable wpa_supplicant@wlan0.service\nsudo systemctl start wpa_supplicant@wlan0.service\nThese commands are crucial for integrating wpa_supplicant with systemd, let‚Äôs break them down:\n\nService Template:\n\nThe wpa_supplicant@wlan0.service syntax uses systemd‚Äôs template unit feature.\nThe @ symbol indicates a template service, and wlan0 is the instance name that gets passed to the template.\nThis allows the same service definition to be used for different wireless interfaces.\n\nEnable Command:\n\nsudo systemctl enable wpa_supplicant@wlan0.service: This creates symbolic links from the system‚Äôs service definition directory to systemd‚Äôs active service directory, ensuring the service starts automatically at boot.\nBehind the scenes:\n\nThis modifies systemd‚Äôs startup configuration by adding the service to the correct target units. Typically multi-user.target.\nThe symbolic links created point to the wpa_supplicant service template file.\n\n\nStart Command:\n\nsudo systemctl start wpa_supplicant@wlan0.service: This immediately starts the service without waiting for a reboot.\nBehind the scenes:\n\nsystemd executes the wpa_supplicant binary with appropriate arguments\nDerived from the service template and the instance name (wlan0).\nThe command effectively executed is similar to: /usr/sbin/wpa_supplicant -c /etc/wpa_supplicant/wpa_supplicant-wlan0.conf -i wlan0\n\n\nIntegration with systemd-networkd:\n\nWhen wpa_supplicant successfully connects to a wireless network, it brings the interface up\nsystemd-networkd detects this state change through kernel events\nsystemd-networkd then applies the matching network configuration (our earlier 25-wireless.network file)\nIf DHCP is enabled, the DHCP client process begins\n\nBenefits of this systemd configuration:\n\nDependency management (services can start in the correct order)\nAutomatic restart if the service fails\nStandardized logging through journald\nConsistent management interface alongside other system services\nThe template approach allows for modular configuration that can be easily expanded if you add more wireless interfaces to your Raspberry Pi.\n\n\n\nVerify the new configuration by checking the systemctl status and running simple network check commands\n\n# Check systemd-networkd status\nsystemctl status systemd-networkd\n\n# Check interface status\nip addr show\n\n# Test connectivity\nping -c 4 google.com\nYou should see outputs that look like this: Note, I didn‚Äôt show the output of id addr because I don‚Äôt want to accidentally post my actual IP address online.\n\n\n\nMake the change permanent\n\n# Remove the minimal Netplan configuration\nsudo rm /etc/netplan/01-network-manager-all.yaml\n\n# Mask the Netplan service to prevent it from running\nsudo systemctl mask netplan-wpa@.service\nsudo systemctl mask netplan-ovs-cleanup.service\nsudo systemctl mask netplan-wpa-wlan0.service\nOne final note before moving on, by the time I removed the generic netplan configuration, my system did not have netplan-wpa.service or netplan-wpa-wlan0.service. I forgot to look before I tested the previous steps, so I‚Äôm not sure if I did, but I‚Äôll leave them here just in case someone needs them. That being said, I was able to mask netplan-ovs-cleanup.service successfully.\n\n\nTroubleshooting\nOnce you‚Äôve finished making changes and applying them, verify that everything is up, running, and as you expect.\n# Check systemd-networkd status\nsystemctl status systemd-networkd\n\n# View network status\nnetworkctl status\n\n# List all network links\nnetworkctl list\nThese are crucial commands for troubleshooting and confirming your network configuration, let‚Äôs break them down:\n\nsystemd-networkd Status Check:\n\nsystemctl status systemd-networkd: This displays the current status of the systemd-networkd service. The output includes:\n\nWhether the service is active, inactive, or failed\nWhen it was started and how long it‚Äôs been running\nThe process ID and memory usage\nRecent log entries directly related to the service\n\nBehind the scenes:\n\nThis queries systemd‚Äôs internal service management database and pulls relevant information from the journal logging system.\nUseful pattern: Look for ‚ÄúActive: active (running)‚Äù to confirm the service is working properly and check the logs for any warning or error messages.\n\n\nNetwork Status Overview:\n\nnetworkctl status: This command provides a comprehensive overview of your system‚Äôs network state.\n\nThe output includes:\n\nHostname and domain information\nGateway and DNS server configurations\nCurrent network interfaces and their states\nNetwork addresses (IPv4 and IPv6)\n\n\nBehind the scenes:\n\nThis tool directly communicates with systemd-networkd using its D-Bus API to retrieve the current network state.\nThis command is particularly useful because it aggregates information that would otherwise require multiple different commands to collect.\n\n\nNetwork Links Enumeration:\n\nnetworkctl list: This lists all network interfaces known to systemd-networkd.\n\nThe output shows:\n\nInterface index numbers\nInterface names\nInterface types (ether, wlan, loopback, etc.)\nOperational state (up, down, dormant, etc.)\nSetup state (configured, configuring, unmanaged)\n\n\nBehind the scenes:\n\nLike the status command, this uses systemd-networkd‚Äôs D-Bus API to enumerate all network links and their current states.\nThis provides a quick way to verify which interfaces systemd-networkd is managing and their current status.\n\n\nTroubleshooting with These Commands:\n\nStart with systemctl status systemd-networkd to ensure the service is running\nUse networkctl list to see which interfaces are detected and their states\nIf an interface shows ‚Äúconfiguring‚Äù instead of ‚Äúconfigured,‚Äù check for configuration errors\nUse networkctl status to verify DNS settings and addressing\nFor more detailed logs: journalctl -u systemd-networkd shows all logs from the networkd service\n\n\nThese commands represent the primary diagnostic tools when working with systemd-networkd. They provide a layered approach to troubleshooting - from service-level status to detailed interface information - that helps pinpoint issues in your network configuration. If you need more:\n\nNetwork Connectivity Loss:\n\nConnect directly to the device via console or keyboard/monitor\nCheck logs with journalctl -u systemd-networkd\nRestore the Netplan configuration from your backup if needed\n\nDNS Resolution Issues:\n\nEnsure systemd-resolved is running: systemctl status systemd-resolved\nCheck /etc/resolv.conf is a symlink to /run/systemd/resolve/stub-resolv.conf\nIf not, create it: sudo ln -sf /run/systemd/resolve/stub-resolv.conf /etc/resolv.conf\n\nConfiguration Errors:\n\nVerify syntax with networkctl list to see if interfaces are ‚Äúconfigured‚Äù or ‚Äúconfiguring‚Äù\nCheck for errors with journalctl -u systemd-networkd -n 50\n\n\n\n\n\nAdvanced Networking\n\nSubnets\nA subnet is a logical subdivision of an IP network. Subnetting allows network administrators to partition a large network into smaller, more manageable segments. Subnetting serves several important functions:\n\nAddress Conservation: More efficient allocation of limited IPv4 address space\nSecurity Segmentation: Isolating sensitive systems from general network traffic\nBroadcast Domain Control: Reducing broadcast traffic by limiting its scope\nHierarchical Addressing: Simplifying routing tables and network management\nTraffic Optimization: Improving network performance by segregating traffic types\n\nA subnet mask determines which portion of an IP address refers to the network and which portion refers to hosts within that network. Consider an IPv4 address: 192.168.1.10 with subnet mask 255.255.255.0 (/24)\n\nIn binary:\n\nIP: 11000000.10101000.00000001.00001010\nMask: 11111111.11111111.11111111.00000000\nThe 1s in the mask represent the network portion, while the 0s represent the host portion.\n\nIn CIDR Notation:\n\n/24 means the first 24 bits identify the network (equivalent to 255.255.255.0)\n/16 means the first 16 bits identify the network (equivalent to 255.255.0.0)\n\nSubnet Calculations: For a /24 network\n\nNetwork address: First address in range (e.g., 192.168.1.0)\nBroadcast address: Last address in range (e.g., 192.168.1.255)\nAvailable host addresses: 2^(32-prefix) - 2 = 2^8 - 2 = 254 usable addresses\n\n\nCreating subnets involves both network design and interface configuration. Here‚Äôs how to implement subnetting on a Linux server using systemd-networkd:\n\nScenario 1: Simple Subnet Isolation\n\nThis configuration matches the eth1 interface\nAssigns it the IP 10.0.1.1 within a /24 subnet (255.255.255.0)\nEnables IP forwarding to allow traffic between this subnet and others\nWhen applied, this creates a subnet with 254 usable addresses (10.0.1.1 through 10.0.1.254, excluding the network address 10.0.1.0 and broadcast address 10.0.1.255).\n\n\n# /etc/systemd/network/25-subnet.network\n[Match]\nName=eth1\n\n[Network]\nAddress=10.0.1.1/24\nIPForward=yes\n\nScenario 2: Multiple Subnets on a Single Interface\n\nThis configuration creates three separate subnets accessed through the same physical interface\n\nA /24 subnet (256 addresses) in the 192.168.1.x range\nA /24 subnet (256 addresses) in the 10.10.10.x range\nA /16 subnet (65,536 addresses) in the 172.16.x.x range\n\n\n\nThe system serves as a router/gateway for all three networks simultaneously.\n# /etc/systemd/network/30-multi-subnet.network\n[Match]\nName=eth0\n\n[Network]\nAddress=192.168.1.10/24\nAddress=10.10.10.1/24\nAddress=172.16.1.1/16\n\nDHCP Server Configuration for Subnets\n\nThis configuration, creates a subnet (10.0.1.0/24) on eth1\nEnables a DHCP server\nAllocates IPs from 10.0.1.11 (base + offset of 10) through 10.0.1.210 (for 200 addresses)\nProvides DNS server information to DHCP clients\n\n\n# /etc/systemd/network/25-dhcp-server.network\n[Match]\nName=eth1\n\n[Network]\nAddress=10.0.1.1/24\nIPForward=yes\nDHCPServer=yes\n\n[DHCPServer]\nPoolOffset=10\nPoolSize=200\nEmitDNS=yes\nDNS=8.8.8.8\nAlthough more complicated than simple networking, subnetting can enhance security when configured properly. It improves isolation by putting separate sensitive services onto different subnets, segmentation by limiting broadcast domains to reduce the potential attack surface, and access control by implementing filters between subnets at the router level. You can see an example of a security-enhanced subnet configuration below, as well as a list of commands to troubleshoot your subnet with.\n# /etc/systemd/network/25-secure-subnet.network\n[Match]\nName=eth2\n\n[Network]\nAddress=10.0.3.1/24\nIPForward=yes\nIPMasquerade=yes  # NAT for outgoing connections\nConfigureWithoutCarrier=yes\n\n[DHCPServer]\nPoolOffset=50\nPoolSize=100\nEmitDNS=yes\nDNS=1.1.1.1\n\n# Restrict routes between subnets for this segment\n[Route]\nGateway=_ipv4gateway\nDestination=0.0.0.0/0\n# Check interface configuration\nip addr show\n\n# Verify routing tables\nip route show\nip route show table 200  # For custom route tables\n\n# Test connectivity between subnets\nping 10.0.1.1  # From another subnet\n\n# View ARP table to verify proxy ARP functionality\nip neigh show\n\n# Check systemd-networkd logs for issues\njournalctl -u systemd-networkd -n 50"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#ssh",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#ssh",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "SSH",
    "text": "SSH\nThis section provides a thorough walkthrough of setting up and securing SSH (Secure Shell) on a Raspberry Pi running Ubuntu Server. SSH is a network protocol that creates an encrypted tunnel between computers, allowing secure remote management. Think of it as establishing a private, secure telephone line that only authorized parties can use to communicate.\n\nUnderstanding SSH Configuration Files\nBefore diving into the setup, it‚Äôs important to understand the key configuration files:\n\nSSH Client vs Server Configuration\nThe SSH system uses two main configuration files with distinct purposes:\n\nssh_config:\n\nLives on your client machine (like your laptop)\nControls how your system behaves when connecting to other SSH servers\nAffects outgoing SSH connections\nLocated at /etc/ssh/ssh_config (system-wide) and ~/.ssh/config (user-specific)\n\nIf your server ever moves or connects to a new IP address, simply update it in the user config file\n\n\nsshd_config:\n\nLives on your server (the Raspberry Pi)\nControls how your SSH server accepts incoming connections\nDetermines who can connect and how\nLocated at /etc/ssh/sshd_config\nRequires root privileges to modify\nChanges require restarting the SSH service\n\n\n\n\n\nKey-Based Authentication Setup\n\nUnderstanding SSH Keys and Security\nThis guide uses ECDSA-384 keys, which offer several advantages:\n\nUses the NIST P-384 curve, providing security equivalent to 192-bit symmetric encryption\nBetter resistance to potential quantum computing attacks compared to smaller key sizes\nStandardized under FIPS 186-4\nExcellent balance between security and performance\n\n\n\nGenerating Your SSH Keys\nOn your laptop, generate a new SSH key pair:\n# Generate a new SSH key pair using ECDSA-384\nssh-keygen -t ecdsa -b 384 -C \"ubuntu-pi-server\"\nThis command:\n\n-t ecdsa: Specifies the ECDSA algorithm\n-b 384: Sets the key size to 384 bits\n-C \"ubuntu-pi-server\": Adds a descriptive comment\n\nThe command generates two files:\n\n~/.ssh/id_ecdsa: Your private key (keep this secret!)\n~/.ssh/id_ecdsa.pub: Your public key (safe to share)\n\n\n\nInstalling Your Public Key on the Raspberry Pi\nTransfer your public key to the Pi:\nssh-copy-id -i ~/.ssh/id_ecdsa.pub chris@ubuntu-pi-server\nThis command:\n\nConnects to your Pi using password authentication\nCreates the .ssh directory if needed\nAdds your public key to authorized_keys\nSets appropriate permissions automatically\n\n\n\n\nServer-Side SSH Configuration\n\nUnderstanding Server Host Keys\nYour Pi‚Äôs /etc/ssh directory contains several important files: - Host key pairs (public and private) for different algorithms - Configuration files and directories - The moduli file for key exchange\n\n\nOptimizing Server Security\n\nBack up the original configuration:\n\nsudo cp /etc/ssh/sshd_config /etc/ssh/sshd_config.backup-$(date +%Y%m%d)\n\nOptimize host key settings in sshd_config:\n\n# Specify host key order (prioritize ECDSA)\nHostKey /etc/ssh/ssh_host_ecdsa_key\nHostKey /etc/ssh/ssh_host_ed25519_key\nHostKey /etc/ssh/ssh_host_rsa_key\n\nStrengthen the moduli file:\n\n# Back up the existing file\nsudo cp /etc/ssh/moduli /etc/ssh/moduli.backup\n\n# Remove moduli less than 3072 bits\nsudo awk '$5 &gt;= 3072' /etc/ssh/moduli &gt; /tmp/moduli\nsudo mv /tmp/moduli /etc/ssh/moduli\n\nApply changes:\n\n# Test the configuration\nsudo sshd -t\n\n# Restart the SSH service (on Ubuntu Server)\nsudo systemctl restart ssh\n\n# Verify the service status\nsudo systemctl status ssh\nJust note, you‚Äôll probably need to reboot (sudo reboot) your server before all of the changes fully take place. Once you‚Äôve done that, you may need to run sudo systemctl start ssh.\n\n\n\nClient-Side Configuration\n\nSetting Up Your SSH Config\nCreate or edit ~/.ssh/config on your laptop:\nHost ubuntu-pi-server\n    HostName ubuntu-pi-server\n    User chris\n    IdentityFile ~/.ssh/id_ecdsa\n    Port 22\n\n\n\n\n\n\nSSH Config: Include\n\n\n\nIf your ssh isn‚Äôt picking up on the ~/.ssh/ssh_config then you might need to specify it in the system config. Find the line in /etc/ssh/ssh_config that says Include and add the absolute file path. If you need to include more than your user specific config, such as the default /etc/ssh/ssh_config.d/* just add that absolute path separated by a space from any other path included.\n\n\n\n\nManaging Known Hosts\n\nBack up your current known_hosts file:\n\ncp ~/.ssh/known_hosts ~/.ssh/known_hosts.backup\n\nView current entries:\n\nssh-keygen -l -f ~/.ssh/known_hosts\n\nRemove old entries:\n\n# Remove specific host\nssh-keygen -R ubuntu-pi-server\n\nHash your known_hosts file for security:\n\nssh-keygen -H -f ~/.ssh/known_hosts\n\n\nSecuring the Key File\nWhen using SSH key-based authentication, adding a password to your key enhances security by requiring a passphrase to use the key. This guide explains how to add and remove a password from an existing SSH key.\nAdding a Password to an SSH Key\nIf you already have an SSH key and want to add a password to it, use the following command:\nssh-keygen -p -f ~/.ssh/id_rsa\nExplanation:\n-p : Prompts for changing the passphrase.\n-f ~/.ssh/id_rsa : Specifies the key file to modify (adjust if your key has a different name).\nYou will be asked for the current passphrase (leave blank if none) and then set a new passphrase.\nRemoving a Password from an SSH Key\nIf you want to remove the passphrase from an SSH key, run:\nssh-keygen -p -f ~/.ssh/id_rsa -N \"\"\nExplanation:\n-N \"\" : Sets an empty passphrase (removes the password).\nThe tool will ask for the current passphrase before removing it.\nVerifying the Changes\nAfter modifying the key, test the SSH connection from your CLI, or using an SSH tunnel.\nssh -i ~/.ssh/id_rsa user@your-server\nIf you added a passphrase, you‚Äôll be prompted to enter it when connecting.\nBy using a passphrase, your SSH key is protected against unauthorized use in case it gets compromised. If you frequently use your SSH key, consider using an SSH agent (ssh-agent) to cache your passphrase securely.\n\n\n\nAdditional Security Measures\n\nFirewall Configuration\n# Install UFW (if it isn't already)\nsudo apt install ufw\n\n# Allow SSH connections\nsudo ufw allow ssh\n\n# Enable the firewall\nsudo ufw enable\nNow, you‚Äôll want to add rules for example, allowing traffic on a specific port if you took the step to choose a nonstandard, one that isn‚Äôt the default Port 22.\n# Add a new rule in the port/protocol format\nsudo ufw add 6025/tcp\n\n# See a list of all rules\nsudo ufw status numbered\n\n# Remove the default rules\nsudo ufw delete 1\n\n\nFail2Ban\nFail2Ban is a security tool designed to protect servers from brute force attacks. It works by monitoring log files for specified patterns, identifying suspicious activity (like multiple failed login attempts), and banning the offending IP addresses using firewall rules for a set period. It‚Äôs especially useful for securing SSH, FTP, and web services.\nThe best part is the project is entirely open source, you can view the source code and contribute here.\n# Install Fail2Ban\nsudo apt update\nsudo apt install fail2ban\n\n# Start and enable Fail2Ban\nsudo systemctl start fail2ban\nsudo systemctl enable fail2ban\n\n\n\nSystem Updates\nKeep your system updated:\nsudo apt update && sudo apt upgrade\n\n\nMonitoring and Maintenance\n\nRegular Security Checks\n\nMonitor SSH login attempts:\n\nsudo journalctl -u ssh\n\nCheck authentication logs:\n\nsudo tail -f /var/log/auth.log\n\n\nKey Management Best Practices\n\nProtect your private key:\n\n\nUse a strong passphrase\nNever share or copy to unsecured devices\nKeep secure backups\n\n\nVerify file permissions:\n\n# On your laptop\nchmod 700 ~/.ssh\nchmod 600 ~/.ssh/id_ecdsa\nchmod 644 ~/.ssh/id_ecdsa.pub\n\n# On your Pi\nchmod 700 ~/.ssh\nchmod 600 ~/.ssh/authorized_keys\n\n\n\nTroubleshooting\nIf you can‚Äôt connect:\n\nVerify SSH service status:\n\nsudo systemctl status ssh\n\nCheck SSH connectivity:\n\n# Test SSH connection verbosely\nssh -v chris@ubuntu-pi-server\n\nVerify host key fingerprints:\n\n# On the Pi\nssh-keygen -l -f /etc/ssh/ssh_host_ecdsa_key.pub\nRemember: When you see a host key verification prompt, always verify the fingerprint matches your server‚Äôs key before accepting.\n\n\nUsing SCP to Transfer Scripts\nThis section outlines the process of securely copying Bash scripts from an Ubuntu Pi Server to a MacBook Air using SCP (Secure Copy Protocol), a file transfer tool built on top of SSH. The user should have an SSH configuration file (~/.ssh/config) set up to simplify connections to their Raspberry Pi server.\nFor a few basic scripts, SCP is fine, but for larger file transfers rsync is better suited and a more efficient solution.\n\nEnsure the SSH Configuration Works\nInitially, the ssh ubuntu-pi-server command did not use the expected user-specific SSH configuration (~/.ssh/config). Instead, it defaulted to the system-wide configuration (/etc/ssh/ssh_config).To fix this, I ran the command with the -F flag explicitly specifying the user config:\nssh -F ~/.ssh/config ubuntu-pi-server\nNote: To make sure SSH always uses the correct config, I tried the following:\n\nMade sure ~/.ssh/config exists and has the correct permissions (chmod 600 ~/.ssh/config).\nModified the /etc/ssh/ssh_config to include the user config:\n\nInclude ~/.ssh/config\nAfter fixing the issue, the command ssh ubuntu-pi-server worked as expected.\n\n\nCopying Scripts from Server to MacBook Air\nOnce SSH was working correctly, the next step was to copy two Bash scripts from the Ubuntu Pi Server to the MacBook Air using scp.\nThe scripts were stored on the Pi as:\n/home/chris/scripts/system_backup.sh\n/home/chris/scripts/config_backup.sh\nThe following scp commands were used to transfer them to the MacBook Air:\nscp ubuntu-pi-server:~/scripts/backup.sh ~/Documents/pi-scripts/\nscp ubuntu-pi-server:~/scripts/maintenance.sh ~/Documents/pi-scripts/\n\n\nCopying Multiple Files at Once\nTo copy all Bash scripts from the scripts directory in one command:\nscp chris@ubuntu-pi-server:~/mnt/backups/ ~/Documents/pi-scripts/backups\n\n\nCopying Files from MacBook Air to Server\nSimply enter the command in reverse, but notice here I did things a little differently.\n\n-r tells scp to recursively copy a directory, meaning it moves it and all of its contents\nchris@ tells scp to specify the user when connecting to the server, this can be helpful if you have connection issues\n\nscp -r ~/Documents/pi-scripts chris@ubuntu-pi-server:~/scripts\n\n\nSCP Notes\n\nSSH is now correctly configured and working using ssh ubuntu-pi-server.\nBash scripts can be securely copied from the Ubuntu Pi Server to the MacBook Air using scp.\n\nJust take note of the specific syntax used, namely server-name:path/to/files\n\nThe user can now maintain local backups of important scripts efficiently.\n\nEnables you to develop where you‚Äôd like and then easily move files to test scripts"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#conclusion",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#conclusion",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Conclusion",
    "text": "Conclusion\nThis configuration provides a robust, secure SSH setup for your Raspberry Pi. It uses modern cryptography (ECDSA-384) while maintaining compatibility with other systems. Regular monitoring and maintenance will help ensure your server remains secure.\nRemember to keep your private keys secure and regularly update your system. If you need to make changes to the SSH configuration, always test them before disconnecting from your current session to avoid being locked out."
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#partitions-and-backups",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#partitions-and-backups",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Partitions and Backups",
    "text": "Partitions and Backups\n\nPartitioning Basics\nPartitions are logical divisions of a physical storage device. Think of a storage device like a large piece of land, and partitions as fenced areas within that land dedicated to different purposes. Each partition appears to the operating system as a separate disk, even though physically they‚Äôre on the same device. Remember from the beginning of this guide, I‚Äôm using an SSD for my primary memory and a microSD card for backups.\n\nSeparation of concerns: Isolate the operating system from user data, which improves security and simplifies backups\nPerformance optimization: Different filesystems can be used for different workloads\nMulti-boot capability: Install multiple operating systems on the same physical device\nData protection: Limiting the scope of filesystem corruption to a single partition\nResource management: Setting size limits for specific system functions\n\nFor our Raspberry Pi server, proper partitioning creates a solid foundation for everything else you‚Äôll build. We‚Äôll primarily use ext4 for Linux partitions and FAT32 for the microSD card that needs broader compatibility.\n\n\n\n\n\n\n\n\nFilesystem\nBest For\nFeatures\n\n\n\n\next4\nLinux\n\nJournaling\nLarge file support\nBackwards compatible\n\n\n\nFAT32\nCross-platform compatibility\n\nWorks with virtually all operating systems\nLimited to 4GB Files\n\n\n\nexFAT\nModern cross-platform\n\nSupports large files\nNo built-in journaling\n\n\n\nNTFS\nWindows compatibility\n\nJournaling\nPermissions\nCompression\n\n\n\nBtrfs\nAdvanced Linux systems\n\nSnapshots\nChecksums\nCompression\n\n\n\n\nFinally, let‚Äôs cover some important terms:\n\nPartition Table: A data structure on a disk that describes how the disk is divided\n\nMBR (Master Boot Record): Traditional partition scheme limited to 2TB drives and 4 primary partitions\nGPT (GUID Partition Table): Modern scheme supporting larger drives and more partitions\n\nPartition Types:\n\nPrimary: Can be bootable and hold an operating system\nExtended: Acts as a container for logical partitions (MBR only)\nLogical: Created within an extended partition (MBR only)\n\nFilesystem: The method used to organize and store data within a partition\n\nCommon Linux filesystems: ext4, Btrfs\nCross-platform filesystems: FAT32, exFAT\n\n\n\n\nPartitioning Tools\nSeveral command-line tools are available for disk partitioning on Linux. Each has strengths for different scenarios:\n\n\n\n\n\n\n\n\n\nTool\nStrengths\nLimitations\nBest For\n\n\nfdisk\n\nSimple interface\nWidely available\n\n\nLimited GPT support in older versions\n\n\nBasic partitioning tasks\n\n\n\nparted\n\nFull GPT support\nHandles large drives\n\n\nMore complex syntax\n\n\nAdvanced partitioning needs\n\n\n\ngdisk\n\nGPT focused\nSimilar to fdisk\n\n\nLess common on minimal installations\n\n\nGPT-specific operations\n\n\n\nsfdisk\n\nScriptable for automation\n\n\nLess user-friendly\n\n\nAutomated deployments\n\n\n\n\nFor this project, and after doing some research, I chose parted for both the microSD card and SSD partitioning because:\n\nIt fully supports both MBR and GPT partition tables\nIt can handle drives larger than 2TB (relevant for the SSD)\nIt provides a more consistent interface across different partition table types\nIt supports both interactive and command-line usage\nIt‚Äôs included in most Ubuntu installations\n\n\n\nPartitioning a MicroSD Card for Backups\nLet‚Äôs partition our microSD card to serve as backup media. You can get great quality cards from Amazon Basics that are perfect for this use case. We‚Äôll use a simple, effective partition scheme. Let‚Äôs walk through this step-by-step:\n\nIdentify the device name of the microSD. Your microSD card will typically appear as something like /dev/mmcblk0 (what mine showed as) or /dev/sdX (where X is a letter like a, b, c). This command lists block devices with key information:\n\nsudo lsblk -o NAME,SIZE,FSTYPE,TYPE,MOUNTPOINT\n\nNAME: Device identifier\nSIZE: Storage capacity\nFSTYPE: Current filesystem type\nTYPE: Whether it‚Äôs a disk or partition\nMOUNTPOINT: Where it‚Äôs currently mounted (if applicable)\n\n\nFor a backup microSD card, we‚Äôll use a simple partition layout with a single partition using ext4 filesystem, which provides good performance and Linux compatibility.\n\n# Start parted on the microSD card (replace /dev/mmcblk0 with your device)\nsudo parted /dev/mmcblk0\n\n# Inside parted, create a new GPT partition table\n(parted) mklabel gpt\n\n# Create a single partition using the entire card\n(parted) mkpart primary ext4 0% 100%\n\n# Set a name for easy identification\n(parted) name 1 backup\n\n# Verify the partition layout\n(parted) print\n\n# Exit parted\n(parted) quit\n\nmklabel gpt: Creates a new GPT partition table (preferred over MBR for modern systems)\nmkpart primary ext4 0% 100%: Creates a primary partition using the ext4 filesystem that spans the entire device\nname 1 backup: Names the first partition ‚Äúbackup‚Äù for easy identification\nprint: Shows the current partition layout\nquit: Exits the parted utility\n\n\nAfter creating the partition, we need to format it with the ext4 filesystem:\n\n# Format the partition (adjust if your device/partition is different)\nsudo mkfs.ext4 -L backup /dev/mmcblk0p1\n-L backup: Sets the filesystem label to ‚Äúbackup‚Äù /dev/mmcblk0p1: The partition we just created (p1 indicates the first partition)\n\nNow, we need to prepare the SD card for backups. You can make those changes with the following commands:\n\n# Create a mount point\nsudo mkdir -p /mnt/backup\n\n# Add an entry to /etc/fstab for automatic mounting\necho \"UUID=$(sudo blkid -s UUID -o value /dev/mmcblk0p1) /mnt/backup ext4 defaults,noatime 0 2\" | sudo tee -a /etc/fstab\n\n# Mount all filesystems from fstab\nsudo mount -a\n\n# Create backup directories\nsudo mkdir -p /mnt/backup/{configs,system,logs}\n\n# Set ownership (replace 'chris' with your username)\nsudo chown -R chris:chris /mnt/backup\n\n# Set secure permissions\nsudo chmod -R 700 /mnt/backup\n\nmkdir -p: Creates directories and parent directories if they don‚Äôt exist\nblkid -s UUID -o value: Gets the UUID (unique identifier) of the partition\ndefaults,noatime: Mount options for good performance (noatime disables recording access times)\n0 2: The fifth field (0) disables dumping, the sixth field (2) enables filesystem checks\nmount -a: Mounts all filesystems specified in fstab\nchmod -R 700: Sets permissions so only the owner can read/write/execute\n\n\n\nPartitioning your SSD\n\nFor the Samsung T7 SSD, we‚Äôll follow a similar workflow, but we‚Äôll create a more sophisticated partition scheme to support different use cases. The Samsung T7 SSD will likely appear as /dev/sdX (where X is a letter like a, b, c), mine is /dev/sdb.\n\nsudo lsblk -o NAME,SIZE,FSTYPE,TYPE,MOUNTPOINT\n\nFor flexibility, we‚Äôll create the following partitions:\n\n\nA small EFI System Partition for UEFI boot compatibility\nA root partition for the Ubuntu OS\nA swap partition, a dedicated section of your hard drive used as virtual memory when your computer‚Äôs RAM (Random Access Memory) fills up\nA data partition for general storage\n\n# Start parted on the SSD (replace /dev/sda with your device)\nsudo parted /dev/sda\n\n# Create a new GPT partition table\n(parted) mklabel gpt\n\n# Create the EFI System Partition (ESP)\n(parted) mkpart primary fat32 1MiB 513MiB\n(parted) name 1 ESP\n(parted) set 1 esp on\n\n# Create the root partition\n(parted) mkpart primary ext4 513MiB 100GiB\n(parted) name 2 ubuntu-root\n\n# Create a swap partition (adjust size based on your RAM)\n(parted) mkpart primary linux-swap 100GiB 108GiB\n(parted) name 3 swap\n\n# Create a data partition using the remaining space\n(parted) mkpart primary ext4 108GiB 100%\n(parted) name 4 data\n\n# Verify the partition layout\n(parted) print\n\n# Exit parted\n(parted) quit\n\nmkpart primary fat32 1MiB 513MiB: Creates a 512MiB FAT32 partition starting at 1MiB\nset 1 esp on: Sets the ESP flag on the first partition\nThe sizes (100GiB, 108GiB) are specific positions on the disk, not partition sizes\nUsing MiB and GiB ensures proper alignment for better performance\n\n\nNow we need to format each partition.\n\n# Format the ESP partition\nsudo mkfs.fat -F32 -n ESP /dev/sda1\n\n# Format the root partition\nsudo mkfs.ext4 -L ubuntu-root /dev/sda2\n\n# Create the swap space\nsudo mkswap -L swap /dev/sda3\n\n# Format the data partition\nsudo mkfs.ext4 -L data /dev/sda4\n\nmkfs.fat -F32: Creates a FAT32 filesystem\n-n ESP: Sets the volume label to ‚ÄúESP‚Äù\nmkfs.ext4: Creates an ext4 filesystem\n-L ubuntu-root: Sets the filesystem label\nmkswap: Initializes a swap area\n/dev/sda1, /dev/sda2, etc.: The specific partitions we created\n\n\nNow we‚Äôll need to mount the partitions by setting up mount points and telling the system to use them.\n\n# Activate the swap\nsudo swapon /dev/sda3\n\n# Create mount points\nsudo mkdir -p /mnt/data\n\n# Add entries to /etc/fstab (using UUIDs for reliability)\necho \"UUID=$(sudo blkid -s UUID -o value /dev/sda2) / ext4 defaults,noatime 0 1\" | sudo tee -a /etc/fstab\necho \"UUID=$(sudo blkid -s UUID -o value /dev/sda1) /boot/efi vfat defaults 0 2\" | sudo tee -a /etc/fstab\necho \"UUID=$(sudo blkid -s UUID -o value /dev/sda3) none swap sw 0 0\" | sudo tee -a /etc/fstab\necho \"UUID=$(sudo blkid -s UUID -o value /dev/sda4) /mnt/data ext4 defaults,noatime 0 2\" | sudo tee -a /etc/fstab\n\n# Mount all filesystems\nsudo mount -a\n\nswapon: Activates the swap partition\ndefaults,noatime: Standard mount options with access time recording disabled for better performance\n0 1: For the root partition, enabling filesystem checks with high priority\nsw: Indicates swap space\nThe different numbers at the end indicate filesystem check priority (0 = no check, 1 = highest priority)\n\n\nAdvanced Partitioning\nAs you begin to utilize your server more, you‚Äôre bound to use up more memory. So, it‚Äôs important to monitor your partition space usage.\n# View disk usage\ndf -h\n\n# View inode usage (for number of files)\ndf -i\n\n# View detailed filesystem information\nsudo tune2fs -l /dev/sda2 | grep -E 'Block count|Block size|Inode count|Inode size'\n\ndf -h: Shows disk usage in human-readable format\ndf -i: Shows inode usage (inode = index node, representing a file)\ntune2fs -l: Lists filesystem information for ext2/3/4 filesystems\ngrep -E: Filters output for specified patterns\n\nFurthermore, you may realize that you want to reformat your SSD at some point because your storage needs changed. You can reformat the partitions using the following code.\n# For online resizing of ext4 (unmounting not required)\nsudo parted /dev/sda\n(parted) resizepart 4 100%  # Resize partition 4 to use all available space\n(parted) quit\n\n# After resizing the partition, expand the filesystem\nsudo resize2fs /dev/sda4\n\nresizepart 4 100%: Resizes partition 4 to use 100% of the remaining available space\nresize2fs: Resizes an ext2/3/4 filesystem to match the partition size"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#system-backups-and-upgrades",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#system-backups-and-upgrades",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "System Backups and Upgrades",
    "text": "System Backups and Upgrades\n\nPartitions\nBefore diving into the actual backup process for your server, it‚Äôs best to figure out your storage media and partition your drives. Partitions are just logicial divisions in your physical storage, each of which can use different filesystems. With some examples below:\n\next4: Standard Linux filesystem\nFAT32: Compatible with most systems but limited to 4GB files\nexFAT: Enhanced FAT, good for cross-platform use\nNTFS: Windows filesystem, readable by Linux\n\n\nPartitioning Tools\nBefore you can partition your hard drive or another piece of physical storage (like a microSD), you‚Äôll need to be connected to your boot media. In my case, that‚Äôs the 1TB SSD I have plugged into my server. You are not able to partition the active storage device on any server or computer.\nSo, your first step is to take a look at the block devices available to you.\nsudo lsblk\n\n\n\nComprehensive Linux Configuration Backup Guide for Raspberry Pi Server\nThis guide explains how to create a complete backup of Linux configurations and system files on a Raspberry Pi Server running Ubuntu Server LTS using rsync. We‚Äôll use rsync because it provides several important advantages over simple copy commands:\n\nIncremental backups that only transfer changed files\nPreservation of file permissions, ownership, and timestamps\nBuilt-in compression for efficient transfers\nDetailed progress information and logging\nThe ability to resume interrupted transfers\n\n\n\nPrerequisites\n\nRaspberry Pi Server running Ubuntu Server LTS\nPhysical keyboard access\nRoot or sudo privileges\nMounted backup drive at /mnt/backups/\nrsync (typically pre-installed on Ubuntu Server)\n\n\n\nSetting Up the Backup Directory\nFirst, we‚Äôll prepare the backup directory structure and set appropriate permissions:\n# Create backup directories if they don't exist\nsudo mkdir -p /mnt/backups/configs\nsudo mkdir -p /mnt/backups/system\n\n# Change ownership to your user (replace 'chris' with your username)\nsudo chown -R chris:chris /mnt/backups\n\n# Set appropriate permissions\nsudo chmod -R 700 /mnt/backups  # Only owner can read/write/execute\n\n\nConfiguration Files Backup\nWe‚Äôll use rsync to create a structured backup of essential configuration files. The following script demonstrates how to perform the backup while preserving all file attributes:\n#!/bin/bash\n# Using the {} around DATEYMD in the file path ensure it's specified as the variable's value, and the subsequent parts are not included\n\nDATEYMD=$(date +%Y%m%d)\nBACKUP_DIR=\"/mnt/backups/configs/$DATEYMD\"\nLOG_DIR=\"/mnt/backups/logs\"\nLOG_FILE=\"$LOG_DIR/${DATEYMD}_config_backup.log\"\n\n# Create backup directory\nmkdir -p \"$BACKUP_DIR\"\n\n{\n    # 1. User and Group Information\n    sudo rsync -aAXv /etc/passwd \"$BACKUP_DIR/passwd.bak\"\n    sudo rsync -aAXv /etc/group \"$BACKUP_DIR/group.bak\"\n    sudo rsync -aAXv /etc/shadow \"$BACKUP_DIR/shadow.bak\"\n    sudo rsync -aAXv /etc/gshadow \"$BACKUP_DIR/gshadow.bak\"\n\n    # 2. Crontab Configurations\n    sudo rsync -aAXv /etc/crontab \"$BACKUP_DIR/\"\n    sudo rsync -aAXv /var/spool/cron/crontabs/. \"$BACKUP_DIR/crontabs/\"\n\n    # 3. SSH Configuration\n    sudo rsync -aAXv /etc/ssh/. \"$BACKUP_DIR/ssh/\"\n    sudo rsync -aAXv ~/.ssh/. \"$BACKUP_DIR/user_ssh/\"\n\n    # 4. UFW (Uncomplicated Firewall) Configuration\n    sudo rsync -aAXv /etc/ufw/. \"$BACKUP_DIR/ufw/\"\n    sudo ufw status verbose &gt; \"$BACKUP_DIR/ufw_rules.txt\"\n\n    # 5. Fail2Ban Configuration\n    sudo rsync -aAXv /etc/fail2ban/. \"$BACKUP_DIR/fail2ban/\"\n\n    # 6. Network Configuration\n    sudo rsync -aAXv /etc/network/. \"$BACKUP_DIR/network/\"\n    sudo rsync -aAXv /etc/netplan/. \"$BACKUP_DIR/netplan/\"\n    sudo rsync -aAXv /etc/NetworkManager/. \"$BACKUP_DIR/NetworkManager/\"\n    sudo rsync -aAXv /etc/hosts \"$BACKUP_DIR/hosts.bak\"\n    sudo rsync -aAXv /etc/hostname \"$BACKUP_DIR/hostname.bak\"\n    sudo rsync -aAXv /etc/resolv.conf \"$BACKUP_DIR/resolv.conf.bak\"\n    sudo rsync -aAXv /etc/wpa_supplicant/wpa_supplicant.conf \"$BACKUP_DIR/wpa_supplicant.conf.bak\"\n\n    # 7. Package Manager Configurations (apt)\n    sudo rsync -aAXv /etc/apt/. \"$BACKUP_DIR/apt/\"\n\n    # 8. Systemd Services and Timers\n    sudo rsync -aAXv /etc/systemd/system/. \"$BACKUP_DIR/systemd/\"\n\n    # 9. Logrotate Configuration\n    sudo rsync -aAXv /etc/logrotate.conf \"$BACKUP_DIR/logrotate.conf.bak\"\n    sudo rsync -aAXv /etc/logrotate.d/. \"$BACKUP_DIR/logrotate.d/\"\n\n    # 10. Timezone and Locale\n    sudo rsync -aAXv /etc/timezone \"$BACKUP_DIR/timezone.bak\"\n    sudo rsync -aAXv /etc/localtime \"$BACKUP_DIR/localtime.bak\"\n    sudo rsync -aAXv /etc/default/locale \"$BACKUP_DIR/locale.bak\"\n\n    # 11. Keyboard Configuration\n    sudo rsync -aAXv /etc/default/keyboard \"$BACKUP_DIR/keyboard.bak\"\n\n    # 12. Package List\n    dpkg --get-selections &gt; \"$BACKUP_DIR/package_list.txt\"\n\n    # Set appropriate permissions\n    sudo chown -R chris:chris \"$BACKUP_DIR\"\n    sudo chmod -R 600 \"$BACKUP_DIR\"\n\n    echo \"Configuration backup completed at: $BACKUP_DIR\"\n\n} &gt; \"$LOG_FILE\" 2&gt;&1\n\necho \"Logs available at: $LOG_FILE\"\n# Make the script executable\nchmod +x /scripts/config_backup.sh\n\n\nSystem Files Backup\nFor system files, we‚Äôll create a separate rsync script that handles system directories efficiently:\n#!/bin/bash\n\nDATEYMD=$(date +%Y%m%d)\nBACKUP_DIR=\"/mnt/backups/system/$DATEYMD\"\nLOG_DIR=\"/mnt/backups/logs\"\nLOG_FILE=\"$LOG_DIR/${DATEYMD}_system_backup.log\"\n\n\n\n# Create backup directory\nmkdir -p \"$BACKUP_DIR\"\n\n{\n    # Starting script\n    echo \"Starting system backup at: $(date)\"\n    echo \"Backup directory: $BACKUP_DIR\"\n\n    # The --one-file-system option prevents crossing filesystem boundaries\n    # --hard-links preserves hard links\n    # --acls and --xattrs preserve extended attributes\n    sudo rsync -aAXv --one-file-system --hard-links \\\n        --exclude=\"/mnt/\" \\\n        / \"$BACKUP_DIR\"\n\n    # 2. System Information Files\n    # Partition layout\n    sudo fdisk -l &gt; \"$BACKUP_DIR/partition_layout.txt\"\n    # Disk UUIDs\n    sudo blkid &gt; \"$BACKUP_DIR/disk_uuids.txt\"\n\n    # Set appropriate permissions\n    sudo chown -R chris:chris \"$BACKUP_DIR\"\n    sudo chmod -R 600 \"$BACKUP_DIR\"\n\n    echo \"System backup completed at: $BACKUP_DIR.\"\n\n} &gt; \"$LOG_FILE\" 2&gt;&1\n\necho \"Logs available at: $LOG_FILE\"\n# Make the script executable\nchmod +x /scripts/system_backup.sh\n\n\nUnderstanding the rsync Options\nThe rsync commands use several important options:\n\n-a: Archive mode, preserves almost everything\n-A: Preserve ACLs (Access Control Lists)\n-X: Preserve extended attributes\n-v: Verbose output\n--one-file-system: Don‚Äôt cross filesystem boundaries\n--hard-links: Preserve hard links\n--exclude: Skip specified directories\n\n\n\nStoring your backups externally\nWhile it‚Äôs definitely beneficial to have a local copy of your backups to easily roll back changes, it isn‚Äôt the most secure solution to have all of your information in one place. So, how can you go about transferring your system and configuration backups to another storage system?\nFor the purpose of this guide, I‚Äôll be showing you how to use rsync for a remote transfer and how to flash the backup onto a microSD.\n\nRsync for Remote Transfers\nRsync is specifically designed for copying and transferring files, so it offers more sophisticated file synchronization capabilities than basic tools like SCP.\nrsync -avz --partial --progress --update chris@ubuntu-pi-server:/mnt/backups/system/master/ ~/Documents/raspberry_pi_server/backups/system/master\nThe flags do the following:\n\n-a: Archive mode, which preserves permissions, timestamps, symbolic links, etc.\n-v: Verbose output, showing what files are being transferred\n-z: Compress data during transfer for faster transmission\n--partial: Keep partially transferred files, allowing you to resume interrupted transfers\n--progress: Show progress during transfer\n--update: Skip files that are newer on the receiver (only transfer if source is newer)\n\n\n\nHardware Transfers\n\n\n\nNote:\nEverything up until this point has been tested and works‚Äì relatively efficiently for such a simple setup. That being said, I still haven‚Äôt tested the restore script, nor have I tried to setup simple cron jobs to automate and cleanup the backups.\n\n\nRestoring from Backup\nTo restore your system from these backups:\n\n\nImportant Notes\n\nThe --delete option during restore will remove files at the destination that don‚Äôt exist in the backup. Use with caution.\nConsider using rsync‚Äôs --dry-run option to test backups and restores without making changes.\nThe backup includes sensitive system files. Store it securely and restrict access.\nConsider encrypting the backup directory for additional security.\nTest the restore process in a safe environment before using in production.\n\n\n\nAutomating the Backup\nCreate a master backup script that runs both configuration and system backups:\n# Create master backup script (save as master-backup.sh)\ncat &lt;&lt; 'EOF' &gt; /mnt/backups/master-backup.sh\n#!/bin/bash\n\n# Set up logging\nexec 1&gt; &gt;(logger -s -t $(basename $0)) 2&gt;&1\n\n# Run configuration backup\n/mnt/backup/backup-configs.sh\n\n# Run system backup\n/mnt/backup/backup-system.sh\n\n# Remove backups older than 30 days\nfind /mnt/backups/configs/ -type d -mtime +30 -exec rm -rf {} +\nfind /mnt/backups/system/ -type d -mtime +30 -exec rm -rf {} +\nEOF\n\n# Make the script executable\nchmod +x /mnt/backup/master-backup.sh\n\n# Add to crontab (run daily at 2 AM)\n(crontab -l 2&gt;/dev/null; echo \"0 2 * * * /mnt/backup/master-backup.sh\") | crontab -\n\n\nTroubleshooting\nIf you encounter issues:\n\nCheck rsync error messages with --verbose option\nVerify sufficient disk space with df -h\nMonitor backup progress with --progress option\nCheck system logs: sudo journalctl -u cron\nVerify file permissions and ownership\nTest network connectivity for remote backups\n\nRemember to regularly verify your backups by checking the log files and occasionally testing the restore process in a safe environment."
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#configuring-microsd-cards",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#configuring-microsd-cards",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "2. Configuring MicroSD Cards",
    "text": "2. Configuring MicroSD Cards\n\nPurpose of MicroSD Cards\n\nExperiment with other OS installations (e.g., NetBSD).\nUse one card as a backup Linux bootloader.\nAllocate one card for portable environments or additional storage.\n\n\n\nSteps to Use MicroSD Cards\n\nFormat the Cards:\n\nUse gparted on Linux or similar tools to format the cards.\nChoose FAT32 for compatibility or ext4 for Linux systems.\n\nInstall Operating Systems:\n\nDownload the desired OS images (e.g., NetBSD).\nFlash the image to the card using balenaEtcher or Raspberry Pi Imager.\n\nSwitching OS:\n\nInsert the appropriate microSD card and reboot the Raspberry Pi."
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#lxc-vs.-lxd",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#lxc-vs.-lxd",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "4. LXC vs.¬†LXD",
    "text": "4. LXC vs.¬†LXD\n\nDefinitions\n\nLXC: Low-level tool for managing lightweight containers that share the host OS kernel.\nLXD: High-level manager for LXC, adding user-friendly features, API, and support for virtual machines (VMs).\n\n\n\n\n\n\n\n\n\nFeature\nLXC\nLXD\n\n\n\n\nRole\nLow-level container tool\nHigh-level container and VM manager\n\n\nEase of Use\nManual configuration\nUser-friendly CLI and API\n\n\nVM Support\nNo\nYes\n\n\nTarget Users\nAdvanced users, developers\nDevelopers, system admins"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#using-lxclxd-for-virtualization",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#using-lxclxd-for-virtualization",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "5. Using LXC/LXD for Virtualization",
    "text": "5. Using LXC/LXD for Virtualization\n\nInstalling LXD\nsudo apt update && sudo apt install -y lxd\nsudo lxd init\n\n\nCreating and Managing Containers\n\nLaunch a Container:\nlxc launch ubuntu:20.04 my-container\nList Running Containers:\nlxc list\nAccess a Container:\nlxc exec my-container -- /bin/bash\nSnapshot and Export:\n\nCreate a snapshot:\n\nlxc snapshot my-container snapshot1\n\nExport the container as an image:\n\nlxc publish my-container --alias my-image\nDeploying Multiple VMs\nCreate Instances:\nlxc launch my-image vm1  \nlxc launch my-image vm2  \nSet Up Networking:\nlxc network create my-bridge  \n\nAttach containers or VMs to this network using:  \n\nlxc network attach my-bridge vm1 eth0"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#setting-up-docker-kubernetes-and-spark",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#setting-up-docker-kubernetes-and-spark",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "6. Setting Up Docker, Kubernetes, and Spark",
    "text": "6. Setting Up Docker, Kubernetes, and Spark\n\nDocker\n\nInstall Docker:\nsudo apt update  \nsudo apt install -y docker.io\nRun a Container:\ndocker run -d -p 8080:80 nginx\nCreate a Dockerfile:\nFROM python:3.8-slim  \nCOPY app.py /app.py  \nCMD [\"python\", \"/app.py\"]  \n\nBuild and run the image:  \n\ndocker build -t my-python-app .  \ndocker run -d my-python-app\n\n\n\n\nKubernetes (K3s)\n\nInstall K3s:\ncurl -sfL https://get.k3s.io | sh -\nDeploy Applications:\nCreate a deployment file (`nginx-deployment.yaml`):  \n\napiVersion: apps/v1  \nkind: Deployment  \nmetadata:  \n  name: nginx-deployment  \nspec:  \n  replicas: 2  \n  selector:  \n    matchLabels:  \n      app: nginx  \n  template:  \n    metadata:  \n      labels:  \n        app: nginx  \n    spec:  \n      containers:  \n      - name: nginx  \n        image: nginx:latest  \n        ports:  \n        - containerPort: 80\nApply the deployment:\nkubectl apply -f nginx-deployment.yaml\n\n\n\n\nApache Spark\n\nInstall Spark:\nwget https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz  \ntar -xzf spark-3.5.0-bin-hadoop3.tgz  \nexport SPARK_HOME=~/spark-3.5.0-bin-hadoop3  \nexport PATH=$SPARK_HOME/bin:$PATH\nRun Spark: Start the master:\nstart-master.sh\nStart a worker:\nstart-worker.sh spark://&lt;master-ip&gt;:7077\nSubmit a Job:\nspark-submit --master spark://&lt;master-ip&gt;:7077 my_script.py\n\n\n\n\nDocker vs.¬†LXC/LXD\n\n\n\nUse Case\nRecommended Tool\n\n\n\n\nPackaging an app and its dependencies\nDocker\n\n\nRunning a full Linux distro in a container\nLXC or LXD\n\n\nManaging containers and VMs at scale\nLXD\n\n\nSimulating enterprise clusters\nLXD with Kubernetes\n\n\nRunning a multi-container app (microservices)\nDocker + Kubernetes"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#conclusion-1",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#conclusion-1",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Conclusion",
    "text": "Conclusion\nThis guide outlines how to configure and use your Raspberry Pi for a robust learning environment. By combining tools like Docker, LXC/LXD, Kubernetes, and Spark, you can simulate enterprise-grade distributed computing clusters and practice advanced data engineering techniques.\nIf you need further assistance, feel free to revisit specific sections or ask for clarification!"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/basic_oss.html",
    "href": "pages/projects/data_engineering/posts/basic_oss.html",
    "title": "Basic OSS Architecture",
    "section": "",
    "text": "The purpose of this project is to showcase the power of open source tools when designing a data and analytics system. I will be walking through my workflow step by step, and including both images, code, and notes.",
    "crumbs": [
      "Data Engineering and Architecture",
      "Basic Open Source Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/basic_oss.html#project-initialization",
    "href": "pages/projects/data_engineering/posts/basic_oss.html#project-initialization",
    "title": "Basic OSS Architecture",
    "section": "Project Initialization",
    "text": "Project Initialization\n\nFirst Steps\nLet‚Äôs start from scratch, a blank VS Code IDE. I‚Äôll do everything from the command line, so make sure to open that up (Ctrl-`).\n\n\n\nBlank IDE\n\n\nFirst, I begin in my home directory. Then, I change to my Documents directory, which I use for all of my projects. This is where I‚Äôll begin creating the project directory and initializing the subsequent tools. As you‚Äôll see below, I first initialize the uv repository and change into it. Then, I create the repo on GitHub (because I like generating the license then), pull (my global is set to merge), commit, and make the initial push. Then, I will initialize the Quarto project, to begin documentation as I work.\n\n\n\nProject and Repo Initialization\n\n\n You can see here that I have both jupyter and dbt already installed. That‚Äôs because uv installs tools system wide, because these are typically used from the CLI. That being said, some CLI tools (like Quarto and DuckDB) in my experience don‚Äôt work with uv because it doesn‚Äôt install their executables.\n\n\n\nGit Remote Add and Pull\n\n\n\n\n\nFirst Commit\n\n\n\nAdding Quarto\nNow, it‚Äôs time to setup some extra functionality in the project. I‚Äôm going to be using Quarto for documentation, so I‚Äôll run quarto create project. To learn more about Quarto and configuring your documentation in projects, checkout my guide. It‚Äôs a fantastic tool for building beautiful, robust documentation, even in enterprise production environments. Consider it for future papers, websites, dashboards, and reports.\nThat being said, if you are ever taking screenshots of your work and want to quickly move them into your images folder, you can do so from the CLI.\n\n\n\nQuarto Project Initialization\n\n\n\n\n\nMoving Images from the CLI\n\n\nNow that you‚Äôve done that, it‚Äôs time to start adding dependencies. As a heads up, don‚Äôt be surprised if you don‚Äôt see the uv.lock or the .venv objects in your directory right away, because uv doesn‚Äôt create those until you add dependencies. Simply run uv add to start adding them. Afterwards, the necessary requirements and lock files will update automatically. If you want to learn more, checkout my uv guide.\n\n\n\nAdd Dependencies with uv\n\n\n\n\nThe Pyproject.toml file\nOnce that‚Äôs done, uv will update the general dependencies in the pyproject.toml file and the specific versions in uv.lock (think requirements.txt on steroids). The nice thing here, it only lists the actual package you needed, not everything else that the package requires. So, when you want to remove packages you can simply use uv remove and the individual package names listed here to remove everything in your environment. There‚Äôs an example below.\n[project]\nname = \"basic-oss-architecture\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.13\"\ndependencies = [\n    \"dbt-core&gt;=1.9.1\",\n    \"dbt-duckdb&gt;=1.9.1\",\n    \"dbt-postgres&gt;=1.9.0\",\n    \"duckdb&gt;=1.1.3\",\n    \"fsspec&gt;=2024.10.0\",\n    \"great-expectations&gt;=0.18.22\",\n    \"jupyter&gt;=1.1.1\",\n    \"pandas&gt;=2.2.3\",\n    \"pytest&gt;=8.3.4\",\n    \"quarto&gt;=0.1.0\",\n    \"requests&gt;=2.32.3\",\n    \"ruff&gt;=0.8.4\",\n]\n\n\nUsing .gitignore effectively\nYou probably noticed, but when you initialize a project with uv it automatically creates a .gitignore file and populates it with basic files and directories which don‚Äôt need to be checked into source control (like .venv). I take this a step further, and add some Quarto specific files and directories too, .quarto and _files folders. Managing this file effectively can drastically reduce the file size of your commits.\nBelow is an example of my file at this early project stage.\n\n\n\n\nInitializing a Data Environment\nNow, you‚Äôll be setting up dbt. Similar to the other CLI tools, dbt uses the dbt init command to create the folder structure necessary for the program to be effective. As you can see below, the process is very easy. You‚Äôll only enter a name for your project, which will (case sensitively) become the name of the dbt directory. Next, I‚Äôll walk through the fundamental pieces of a dbt project in depth.\n\n\n\n\n\n\n\nLower Case Naming Conventions\n\n\n\nDue to some naming conventions and for simplicity, I later changed the name of the dbt projct in my environment to basic_oss. It‚Äôs easier to type when it‚Äôs all lowercase and it fits with the dbt naming conventions better. So, don‚Äôt be surprised if/when you see it later.\n\n\n\n\nThe Data Build Tool (dbt)\nAs I said, creating a dbt project is easy, but it can get confusing from here on out if you‚Äôre alone with the dbt documentation. In my experience, dbt initalizes a logs/ folder in the project root directory not the dbt root directory. So, I make sure to add that to the gitignore file, because I don‚Äôt think that needs to be checked into version control.\nSo, now that you‚Äôve initialized your folder, let‚Äôs go through the basics:\n\nProject Root: Basic_OSS\nIn the case of my project, the root folder is called Basic_OSS. Here, you‚Äôll find the 6 subdirectories, a .gitignore file, a README.md file, and the dbt_project.yml file. The .gitignore can be deleted, because you have one in the project root directory, and for the same reason, so can the README. The dbt project file is the core of your entire data environment, in the same exact way that a _quarto.yml file is the core of your website, book, or documentation project.\nThis is where you‚Äôll configure the actual structure and hierarchy of your environment, along with things like schemas or variables, or aliases.\n\n\nAnalyses\nContains the SQL files for any analysis done that are not part of the core models. Think of these as the SELECT statements for analytical queries, whereas models handle the DDL statements for database architects. Depending on your workflow, this folder could be unused.\n\n\nMacros\nThis is where you can store custom macros (functions) and reusable code written in either SQL or jinja. This is the Python package equivalent for SQL and it‚Äôs often used to ensure DRY (Don‚Äôt Repeat Yourself) principles for ETL and other database work.\n\n\n\n\n\n\nCustom Schema Macro\n\n\n\nBy default, dbt will use main for the schema name. Furthermore, I found that even when specifying the schema in the YAML files, it would just append that to main either as main.example_schema.table or main_example_schema.table. For enterprise purposes, this behavior is intended and explained in dbt docs. For my purposes, I would rather the format and reference behavior be schema.table. To do this, I found and modified a custom macro.\n\n\n\n\n\nModels\nThis is the core of dbt. Models are the SQL tables themselves, as well as the transformations when cleaning and aggregating data (from raw to reporting). If you have a raw schema (where the raw data is temporarily stored) and a clean schema (where cleaned data is persisted), you would have both a raw and clean folder within the models folder. Then, the individual queries would live within those subfolders as the actual tables and views.\nIt is where most of (if not all) your transformations live. So, can become computationally taxing if you aren‚Äôt careful.\nRun with dbt run or dbt run --select {model_directory_name}.\n\n\nSeeds\nThese are flat files containing static data used for mapping (or reference) data. Only use this if your project needs static data. For more on seeds.\nRun with dbt seed.\n\n\nSnapshots\nStores snapshot definitions for versioning and tracking changes in source data over time. These are commonly used for SCDs (slowly changing dimensions) or auditing.\nRun with dbt snapshot.\n\n\nTests\nFairly self explanatory, but this folder contains custom, SQL-defined tests for your models. Dbt allows for both custom tests defined in .sql files and generic tests defined in a YAML. The tests run on various models are defined in the dbt_project.yml file.\n\n\nExtra Notes\nDbt also has the docs/ and dbt_packages/ folders which are for advanced documentation and shareable, modularized code, respectively. Generally speaking, your workflow will really only involve the following parts of a dbt project:\n\nmodels/\ntests/\nmacros/\ndbt_project.yml\n\nThe others are optional and provide functionality, that while useful and powerful in many cases, is not always needed. Now that I‚Äôve got the local directory all configured, it‚Äôs time to start building the container for my PostgreSQL instance (server, cluster, whatever you want to call it).",
    "crumbs": [
      "Data Engineering and Architecture",
      "Basic Open Source Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/basic_oss.html#docker-and-containers",
    "href": "pages/projects/data_engineering/posts/basic_oss.html#docker-and-containers",
    "title": "Basic OSS Architecture",
    "section": "Docker and Containers",
    "text": "Docker and Containers\nDocker is a powerful open-source platform that simplifies the process of developing, packaging, and deploying applications using containers, which are lightweight, portable environments. Unlike traditional virtualization, which replicates an entire computer system, containers virtualize at the operating system (OS) level, creating isolated spaces where applications run with all their dependencies. By isolating apps in containers, Docker ensures that each environment is consistent across different systems, reducing conflicts caused by mismatched dependencies. This approach accelerates development, enhances portability, and enables scalability, making Docker a cornerstone of modern microservices architectures and containerized workflows.\nYou can learn more about Docker either through their open source documentation or DataCamp‚Äôs course by Tim Sangster!\n\nThe Dockerfile and Configuring Your Image\nThe Dockerfile is the foundation of Docker image creation, serving as a script of instructions to define the environment and behavior of your containerized application. Each instruction in the Dockerfile builds on the previous one, forming layers that together create a Docker image.\nA Dockerfile is composed of various instructions, such as:\n\nFROM: Specifies the base image to start with. Always begin with this instruction.\nFROM postgres\nRUN: Executes shell commands during the build process and creates a new layer.\nRUN apt-get update\nCOPY/ADD: Transfers files from your local system into the image.\nCOPY postgres-password.txt /usr/home/\nWORKDIR: Sets the working directory for subsequent instructions.\nWORKDIR /usr/home/\nCMD: Specifies the default command to run when the container starts. Unlike RUN, CMD is executed at runtime.\nCMD [\"postgres\"]\n\n\nOptimizing Builds with Caching\nDocker employs a layer-caching mechanism to optimize builds. Each instruction in the Dockerfile forms a layer, and Docker reuses unchanged layers in subsequent builds to save time. For example:\nRUN apt-get update\nRUN apt-get install -y libpq-dev\nIf you rebuild and these instructions remain unchanged, Docker uses cached results. However, if the base image or any instruction changes, the cache is invalidated for that layer and subsequent ones.\n\n\n\n\n\n\nMaximize Cache Efficiency\n\n\n\nReorder Dockerfile instructions to maximize cache efficiency. Place less frequently changing instructions higher in the file. Then, place the layers you need to test changes in more frequently, lower in the file.\n\n\n\n\nUsing Variables in Dockerfiles\nVariables make Dockerfiles more flexible and maintainable.\n\nARG: Sets build-time variables.\nARG APP_PORT=5000\nRUN echo \"Application port: $APP_PORT\"\nARG values are accessible only during the build process.\nENV: Sets environment variables for runtime.\nENV APP_ENV=production\nThese variables persist after the image is built and can be overridden when running the container using the --env flag.\n\n\n\n\n\n\n\nCredentials\n\n\n\nAvoid storing sensitive data like credentials in ARG or ENV, as they are visible in the image‚Äôs history.\n\n\n\n\nSecurity Best Practices\n\nUse Official Images: Base your Dockerfile on trusted, well-maintained images from sources like Docker Hub.\nMinimize Packages: Install only what your application needs to reduce potential vulnerabilities.\nAvoid Root Users: Run applications with restricted permissions by creating a non-root user:\nRUN useradd -m appuser\nUSER appuser\nUpdate Regularly: Keep your base images and software dependencies up to date.\n\n\n\n\nWrite a PostgreSQL Dockerfile and Build the Image\nBelow is an example Dockerfile for setting up PostgreSQL, with specified user, password, and database name:\n# Use an official PostgreSQL base image\nFROM postgres\n\n# Set environment variables from a local file\nCOPY .pg-password /etc/postgresql/.pg-password\n\n# Read the password from the file and set it as an environment variable\nRUN POSTGRES_PASSWORD=$(cat /etc/postgresql/.pg-password)\n\n# Set additional environment variables\nENV POSTGRES_USER=chris\nENV POSTGRES_DB=test\n\n# Expose the PostgreSQL port\nEXPOSE 5432\n\n# Run commands to configure the environment\nRUN apt-get update && apt-get install -y \\\n    libpq-dev \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Start PostgreSQL service when the container runs\nCMD [\"postgres\"]\nFor the purposes of simplicity in this guide, I‚Äôm just going to leave the POSTGRES_ variables as they are in the file. The password I‚Äôll keep separate to demonstrate how that would work. That being said, after you‚Äôve writen the image‚Äôs Dockerfile, you‚Äôll build the image.\ndocker build -t test .\nIf you need to remove the image at any point, first make sure there are no containers using it, then run the following:\ndocker rmi test\n\n\n\nRunning the Container\nNext, to run the container, you‚Äôll be adding a few flags, which I‚Äôll explain below. For simplicity sake, it‚Äôs probably easiest to store this in a script somewhere and then execute that on start up. **Note** to myself: Add a section on writing local scripts/executables like this later on in the guide.\ndocker run \\\n  --name pg_test \\\n  -e POSTGRES_PASSWORD_FILE=/etc/postgresql/.pg-password \\\n  -e POSTGRES_USER=chris \\\n  -e POSTGRES_DB=test \\\n  -p 5432:5432 \\\n  test\n\n\nFlags Explained\nThe code above will run a container using the Docker image test as the base.\n\nThe container will have the --name pg_test\nYou‚Äôll use the file located at /etc/postgresql/.pg-password to define the POSTGRES_PASSWORD_FILE environment variable\n\nIn Docker, when you use the -e option to pass environment variables, you typically use POSTGRES_PASSWORD_FILE instead of POSTGRES_PASSWORD for file-based password configuration because of how Docker processes environment variables and how the underlying system uses them.\n\nYou‚Äôll also pass the -environment variables for POSTGRES_USER and POSTGRES_DB\n\nThis isn‚Äôt necessary because they are defined in the Dockerfile, so they are globally available within the container.\nIt‚Äôs useful to specify these values in the run command if you want to override the default values or pass different values.\nSpecifying the user and database in the docker run command allows you to control the environment at runtime. More useful in production envrionments, less so for one-off projects like this\n\nFinally, you‚Äôll map the container‚Äôs -port 5432 to your port:5432\n\n\n\nVerifying a Successful Run\nTo verify your container is running, you can use docker ps to get a list of active containers, their image, and other bits of information.\ndocker ps\nCONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS          PORTS                    NAMES\n5793b64919f0   test      \"docker-entrypoint.s‚Ä¶\"   14 seconds ago   Up 13 seconds   0.0.0.0:5432-&gt;5432/tcp   pg_test\nIf you need to stop the running container, simply type:\ndocker kill pg_test\n\n\n\nConnecting to the PostgreSQL Server from your Command-Line Interface\nNow that the server is up and running, you can test an active connection (and your user permissions) from your CLI. To do so, run the following:\nuv run psql -h localhost -U chris -d test\n\nuv run ensures that the command is run in the context of uv‚Äôs environment\npsql is the CLI command for postgres, in the same way gh is for GitHub\n-h tells postgres that the server‚Äôs host is localhost\n-U tells postgres to use the user chris\n-d tells postgres to connect to the database test\n\nYou can close that connection at any time by typing \\q.\n\n\n\n\n\n\n\nNote\n\n\n\nI‚Äôm working on a MacOS laptop and manage local packages (like Python, Docker, gh, and Postgres) with Homebrew. Even though I had PostgreSQLv17 installed, it wasn‚Äôt added to my PATH for some reason. So, when I ran psql ... I got an error. To fix this, I simply edited the ~/.zshrc (the MacOS default terminal zsh configuration file in my home directory) and added export PATH=\"/opt/homebrew/opt/postgresql@17/bin:$PATH\".\n\n\n\n\nExploring the Environment\nNow that you‚Äôre all setup and connected, it‚Äôs time to explore the environment and verify your privileges. In this section I‚Äôll run through some basic commands that you can use to understand whatever postgres environment you‚Äôre in.\nFirst, the \\du command will tell you which roles are available and their attributes:\ntest=# \\du\n\nSecond, the \\l command lists all databases:\ntest=# \\l\n\nThird, the \\dn command lists all schemas:\ntest=# \\dn\n\nFourth, the \\dt command lists all tables. You should expect this output upon creation of a new database:\ntest=# \\dt\nDid not find any relations.\nHere are some other useful commands that will not be useful with a fresh database:\n\n\\dv lists all views\n\\d table_name lists all columns in a table with name table_name\n\\di table_name lists all indexes for a table with name table_name\n\nFinally, to really dive into a specific database, you‚Äôll use the \\c command, but in my case I connected when I first ran the psql command. If you just type psql without the -d flag, you‚Äôll simply connect to a server in general, not a specific database.\nAt this point, the initial project configuration is just about done. The next steps involve initializing the persistent DuckDB database to use as the Dev/Test environment and defining the raw data model with dbt. For now, you can close the connection to the Postgres server and shutdown container.\nThe basic data model and ingestion will all be handled in DuckDB and Python.",
    "crumbs": [
      "Data Engineering and Architecture",
      "Basic Open Source Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/basic_oss.html#duckdb",
    "href": "pages/projects/data_engineering/posts/basic_oss.html#duckdb",
    "title": "Basic OSS Architecture",
    "section": "DuckDB",
    "text": "DuckDB\nDuckDB is:\n\nSimple: Easy to install and deploy, has zero external dependencies, and runs in-process in its host app or as a single binary. Portable: Runs on Linux, macOS, Windows, and all popular hardware architectures, has client APIs for major programming languages Feature-rich: Offers a rich SQL dialect. Can read/write files (CSV, Parquet, and JSON), to and from the local file system, and remote endpoints such as S3 buckets. Fast: Runs analytical queries at blazing speed due to its columnar engine, which supports parallel execution and can process larger-than-memory workloads. Extensible: DuckDB is extensible by third-party features such as new data types, functions, file formats and new SQL syntax. Free: DuckDB and its core extensions are open-source under the permissive MIT License.\n\nTL;DR DuckDB is awesome.\nAs the block of text tells you, while DuckDB is both simple and portable, it is also feature-rich, which is why it‚Äôs great for a development or test environment. It‚Äôs even easy to spin-up an instance of DuckDB in-memory, while you‚Äôre testing or exploring data, and then you can choose to remove that or persist the file in storage.\n\nDuckDB can operate in both persistent mode, where the data is saved to disk, and in in-memory mode, where the entire data set is stored in the main memory. To create or open a persistent database, set the path of the database file, e.g., database.duckdb, when creating the connection. This path can point to an existing database or to a file that does not yet exist and DuckDB will open or create a database at that location as needed. The file may have an arbitrary extension, but .db or .duckdb are two common choices with .ddb also used sometimes. Starting with v0.10, DuckDB‚Äôs storage format is backwards-compatible, i.e., DuckDB is able to read database files produced by an older versions of DuckDB. DuckDB can operate in in-memory mode. In most clients, this can be activated by passing the special value :memory: as the database file or omitting the database file argument. In in-memory mode, no data is persisted to disk, therefore, all data is lost when the process finishes.\n\nFor more information, here are links to some important pieces of DuckDB:\n\nCompression\nConcurrency\nData Import\nDuckDB SQL\nWhy DuckDB?\n\nAs you‚Äôll see, I‚Äôm planning to use DuckDB with Python to ingest raw data from an API (or flat files) and then build the dbt models based off that (with some help from built-in DuckDB functionality). After that, I‚Äôll connect dbt to Postgres and copy the data model over.\n\nA Persistent Database with DuckDB\nAs you saw above, it‚Äôs very easy to get started with DuckDB. To keep with the structure of the project, I‚Äôm going to put the .duckdb file in the root project directory because the Dockerfile (which serves as the Postgres file) is there as well. By design, it‚Äôs easy to create a persistent database and manage the file (there‚Äôs only one). You have two options:\n\nLaunch DuckDB and create your database file on launch (example below)\nLaunch DuckDB, then run .open test.duckdb\n\nFor the purposes of this workflow, it‚Äôs better to create the file when you initially launch DuckDB; however, if you are using DuckDB in-memory for EDA or an adhoc ask, but want to persist your work the second option is valid.\nBasic_OSS_Architecture % duckdb test.duckdb\nv1.1.3 19864453f7\nEnter \".help\" for usage hints.\nD SELECT * FROM duckdb_tables();\n.exit\n\nYou can then run ls -lh to verify the file exists, and to see some basic file metadata: read/write/executable permissions, author, file size, last edited. Next, I need to configure DuckDB to work with dbt.",
    "crumbs": [
      "Data Engineering and Architecture",
      "Basic Open Source Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/basic_oss.html#dbt",
    "href": "pages/projects/data_engineering/posts/basic_oss.html#dbt",
    "title": "Basic OSS Architecture",
    "section": "dbt",
    "text": "dbt\nUp to this point, I‚Äôve initialized the project environment and outside of a few lines of code, most of the work has been automated:\n\nInitialized the project environment with uv and added Python packages\nInitialized the project repository with git and GitHub\nInitialized the documentation envrionment with Quarto\nInitialized the data model with dbt\nWrote a Docker image to contain the PostgreSQL server\nCreated the test database with DuckDB\n\nNow, it‚Äôs time to start connecting everything, which is exactly what dbt does best.\n\ndbt Core (data build tool) is an open-source framework designed for transforming raw data in a warehouse into a well-structured and actionable format. It operates by enabling analysts and engineers to write modular SQL queries and manage their transformations as code, following software engineering best practices like version control and testing. dbt is used to define, test, and document data pipelines, helping teams standardize and automate transformations in a consistent and scalable way. Key benefits include increased collaboration, improved data quality through built-in testing, and the ability to easily track and manage changes in data models over time. It integrates seamlessly with modern cloud data warehouses, making it a cornerstone for modern data engineering workflows.\n\nLearn more about getting started with dbt.\n\nConfiguring Database Connections with dbt\nThere are two contexts when working with dbt locally: the project dbt directory and the global dbt configuration. The first is what I initialized for the sake of this project using dbt init. The second is the profiles.yml file in the ~/.dbt directory, which is used by dbt to connect to different databases. You‚Äôll also need to install the specific connection type extensions, in this case dbt-duckdb and dbt-postgres‚Äì both of these are added like any other dependency with uv add.\nThe file and folder should be created when you first install dbt-core. If the file isn‚Äôt there, you can simply create one. The general syntax is as follows:\nbasic_oss:\n  target: test # The default output for dbt to target (either test or prod in this case)\n  outputs: # The defined outputs for dbt to target\n    test: # Name of the output\n      type: duckdb # Database type\n      path: /Users/chriskornaros/Documents/Basic_OSS_Architecture/test.duckdb # Database file path\n      schema: ingest # Schema to target\n      threads: 4 # The number of threads dbt is allowed to enable for concurrent use\n    prod:\n      type: postgres\n      host: localhost # Hostname of the PostgreSQL server/container\n      port: 5432 # Port postgres listens on\n      user: chris # User must be specified\n      password: # Unfortunately I had to type this in, I haven't found a way to link the key file to profiles.yml yet\n      database: test # Database must be specified\n      schema: stage # Schema to target\n      threads: 4 # Threads for concurrency\n\n\n\n\n\n\nCPUs and Threads\n\n\n\nThreads in a CPU are the smallest units of execution that can run independently, allowing a processor to handle multiple tasks concurrently. They improve performance by utilizing CPU cores more efficiently, especially in multithreaded applications. With Python 3.13, the GIL (global interpreter lock) is gone, you can now actually utilize multiple cores at once. If you want to find out how many cores you have, run sysctl -n hw.logicalcpu (MacOS) or lscpu (Linux).\nLogical CPUs represents the number of threads that the system can handle concurrently, a more accurate measure of how many threads your system can execute simultaneously than Physical CPUs because of hyperthreading.\n\n\nYou‚Äôll also need to configure the project specific YAML file‚Äì dbt_project.yml. This is easy and, for the most part, the default version of the file that is generated by dbt init is good enough for an initial connection. Later on, I‚Äôll go into some other features and properties that you can define which make dbt even more powerful.\n# Name your project! Project names should contain only lowercase characters\n# and underscores. A good package name should reflect your organization's\n# name or the intended use of these models\nname: 'basic_oss'\nversion: '1.0.0'\n\n# This setting configures which \"profile\" dbt uses for this project.\nprofile: 'basic_oss'\n\n# These configurations specify where dbt should look for different types of files.\n# The `model-paths` config, for example, states that models in this project can be\n# found in the \"models/\" directory. You probably won't need to change these!\nmodel-paths: [\"models\"]\nanalysis-paths: [\"analyses\"]\ntest-paths: [\"tests\"]\nseed-paths: [\"seeds\"]\nmacro-paths: [\"macros\"]\nsnapshot-paths: [\"snapshots\"]\n\nclean-targets:         # directories to be removed by `dbt clean`\n  - \"target\"\n  - \"dbt_packages\"\n  - \"logs\"\n\nmodels:\n  basic_oss:\n    ingest:\n      +schema: ingest\n      +materialized: table\n      +enabled: true\n    stage:\n      +schema: stage\n      +materialized: table\n      +enabled: true\nOnce you have those files configured and saved, you can verify that things were done correctly by running the code below and seeing a similar output to the subsequent image.\nNote: It is important that you use uv tool run to execute the command because the dbt-duckdb extension is installed within uv‚Äôs project context, not system wide. So, the debug will fail if you just use dbt debug. Additionally, when running from the main project directory, you need to specify the dbt project directory. Otherwise, dbt cannot find the dbt_project.yml file, there is a siimilar flag for the profiles.yml file, if yours is not in your ~/.dbt folder.\nFurthermore, while VS Code automatically picks up on uv and its environment, you will need to manually activate the virtual environment if working in a standalone terminal. Even inside of that virtual environment, you will need to use uv tool run\nuv tool run dbt debug --profile basic_oss --target test --project-dir basic_oss\nuv tool run dbt debug --profile basic_oss --target prod --project-dir basic_oss",
    "crumbs": [
      "Data Engineering and Architecture",
      "Basic Open Source Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/basic_oss.html#data-selection-and-ingestion",
    "href": "pages/projects/data_engineering/posts/basic_oss.html#data-selection-and-ingestion",
    "title": "Basic OSS Architecture",
    "section": "Data Selection and Ingestion",
    "text": "Data Selection and Ingestion\nNow that I have the environment good to go, the databases configured, and dbt connected, it‚Äôs time to pick a dataset and start building the data model. To do this, I‚Äôm first going to add a directory for notebooks (eventually I‚Äôll add one for scripts) and then create a Jupyter notebook. I prefer developing in notebook simply because I can quickly change and test code as I‚Äôm iterating. Then, as a reminder, if you haven‚Äôt installed the Python dependencies, you can do so with uv add.\nIn the following block, you‚Äôll see me use code. This command is the VS Code CLI tool, you can learn more here.\nmkdir notebooks\ncode notebooks/api_axploration.ipynb\n\nuv add dbt-core # The core dbt package, needed for base functionality\nuv add dbt-duckdb # The DuckDB extension, needed to connect with dbt\nuv add dbt-postgres # The postgres extension, needed to conenct with dbt\nuv add duckdb # DuckDB Python package\nuv add fsspec # Dependency for DuckDB JSON functionality\nuv add great-expectations # GX is for data validation/quality, later on\nuv add jupyter # Jupyter will launch a Python kernel and makes the notebooks possible\nuv add pandas # For standard functionality\nuv add pytest # For testing actualy pipeline/script performance, not so much data quality\nuv add quarto # For documentation\nuv add requests # For making API calls\nuv add ruff # An extremly fast Python linter made by Astral, the makers of uv\n\nConfiguring and Running a Jupyter Server\nBefore you can use the notebook, you‚Äôll need to launch and connect to a jupyter server. When developing locally, and not sharing your code or compiled code anywhere, it‚Äôs much easier to not require an authentication token. To allow this behavior, you‚Äôll need to modify your jupyter configuration.\nI modified my global configuration, because I would have to reconfigure jupyter with every project if I did it in the specific project directory. That being said, I‚Äôll include the local example as well.\n# For the global configuration\ncode ~/.jupyter/jupyter_server_config.py\n\nc.ServerApp.token = '' # Find this in the .py file, set the value to '' which means nothing\n# For the local configuration\nmkdir .jupyter\ncode .jupyter/jupyter_server_config.py\n\nc.IdentityProvider.token = ''\nAs you can see, the process is similar, you‚Äôll just manually create the .jupyter/ folder in your project directory first. THe next step is to start the server. Then, you‚Äôll see an ouput similar to the image below. It will tell you useful information like the server address (localhost in this case), version, et cetera; however, you can now leave that terminal window be, it will continue to run in the background. If you need to stop the server, just go back to the window and hit Ctrl-C.\nuv run jupyter server\n\n\n\nUsing The Notebook\nNow that the server is up and running, you can go back to the notebook you created. To use it, I‚Äôll first need to connect to the server (if you see the term kernel don‚Äôt be surprised, that‚Äôs what a single notebook runs on). Luckily, VS Code provides a really nice UI for connecting a notebook to the Jupyter server. In the images below, I‚Äôll walk you through the connection process. First begin by selecting Select Kernel in the top-right corner of the notebook UI. From there, VS Code‚Äôs command palette (the search bar at the top of the window) will interactively walk you through the next steps.\n    \nThen, you can verify your connection by running the first cell (Ctrl-Enter on MacOS).\n\n\n\nGET-ting my Initial Data\nNow that everything is setup and I can actually run Python code within the notebook, it‚Äôs time to pick an API and make a request to actually get the data. If you aren‚Äôt sure where to start looking for an API, PublicAPI has a great site of publicly available datasets. Many of which are accesible both programmatically or through GUIs.\nBefore I jump into the code, two quick notes about HTTP and GET:\n\nThese are protocols used for transferring data over the internet. HTTP (HyperText Transfer Protocol) and HTTPS (Secure HyperText Transfer Protocol) are foundational protocols that allow web browsers and servers to communicate with each other. HTTPS adds a layer of security by encrypting the data exchanged between the client and the server. GET is one of the most common HTTP methods used to request data from a specified resource. It retrieves information from the server without making any changes. This method is used when you want to fetch data, such as retrieving data from an API endpoint. It is simple and effective for read-only operations, such as querying data from a database or fetching information from a web service.\n\nNext, you‚Äôll choose the API you want to server as your data source. I decided to use NASA‚Äôs NEO (Near Earth Object) Feed for the following reasons:\n\nDataset: Information about NEOs, including size, orbital path, and speed. API Endpoint: https://api.nasa.gov/neo/rest/v1/neo/browse?api_key={API key} Data Characteristics: JSON structure with a list of NEOs, each containing details like id, name, nasa_jpl_url, absolute_magnitude_h, estimated_diameter, close_approach_date, and more. Use Case: Ideal for a beginner, as it provides a simple JSON structure with basic information and is updated regularly.\n\nBesides my interest in space, the other perk of using NASA‚Äôs API is that it‚Äôs free and provides a reasonable amount (1,000) of hourly calls. That being said, the documentation isn‚Äôt perfect, but it‚Äôs definitely easy enough to query. Below, I‚Äôll share the first few cells of my notebook. Learn more on the NASA API Website.\n# Import the required packages\nimport requests\nimport pprint # pprint similar to beautiful soup, but better for my specific use case\nimport duckdb\nimport pandas as pd\n\n\n\n\n\n\nNotebook Context\n\n\n\nIf you‚Äôre using VS Code, Jupyter notebook context is the parent directory that you opened the file from. In my case code notebooks/api_exploration.ipynb resulted in the context being the root dir above notebooks (Basic_OSS_Architecture/), instead of notebooks. So, I was initially connecting to a persistent DuckDB file created in my Documents directory, instead of the file within my project directory because my connection string below was ../test.duckdb instead of test.duckdb. As a result, I wasn‚Äôt able to interact with the environment from my CLI.\n\n\n# Open a connection to the persistent database\ncon = duckdb.connect('test.duckdb')\nduckdb.install_extension(\"httpfs\") # Lets you directly query HTTP endpoints\nduckdb.load_extension(\"httpfs\") # Trying this functionality out to simplify the ETL process\n\ncon.sql(\"SELECT * FROM duckdb_tables()\") # Note: Don't use a semi colon in DuckDB for Python\n\n\n\n\n\n\nhttpfs Extension for DuckDb\n\n\n\nI tried using the httpfs extension from DuckDB to handle the API calls; however, you need a specific file at the location, such as .csv or .parquet. It didn‚Äôt work for me, so I ended up doing things differently.\n\n\n# Store the base API information, for this, I'm using NASA's Near Earth Object (NEO) API\napi_key_path = \"/Users/chriskornaros/Documents/local-scripts/.api_keys/nasa/key.txt\"\nwith open(api_key_path, 'r') as file:\n    api_key = file.read().strip()\nstart_date, end_date = \"2024-12-01\", \"2024-12-04\"\n\n# Then Construct the URL with f-string formatting\nurl = f\"https://api.nasa.gov/neo/rest/v1/feed?start_date={start_date}&end_date={end_date}&api_key={api_key}\"\n# Standalone cell for the API response, ensures I don't call too many times\nresponse = requests.get(url)\n\n# Check if the response was successful, if so, print the data in a human readable format\nif response.status_code == 200:\n    data = response.json()\n    pprint.pprint(data)\nelse:\n    print(f\"Error: {response.status_code}\")\n\n\nCreating the Raw Table\nNow that you‚Äôve successfully connected to the API and made your first GET request, it‚Äôs time to get the API output in a format that DuckDB can read into a table. If your API output is anything like mine, it‚Äôll be messy, even with the help of pprint, as you can see below. So, before I can create the table, I need to flatten the dictionary output.\n\nLuckily, there are some simple functions in both Python and DuckDB that can help.\n# Flatten the output and then read it with DuckDB to get the column data types\nflat_data = pd.json_normalize(data['near_earth_objects']['2024-12-02'])\n\nraw = con.sql(\"\"\"CREATE TABLE asteroids AS\n                SELECT * EXCLUDE(nasa_jpl_url, close_approach_data, 'links.self')\n                , unnest(close_approach_data, recursive := true)\n                FROM flat_data\n                \"\"\")\n\ncon.sql(\"SELECT * FROM duckdb_tables()\") # Shows table metadata, validates the script worked as intended\nWhat flat_data did is tell Python that the dictionary values for the key ‚Äònear_earth_objects‚Äô are what I‚Äôm looking for and to get rid of the rest of the information, while elevating the nested hierarchy. It takes that one step further, by specifying the actual date (which are the keys in this dataset). That results in an output that you can view below; however, there is still a column with the DuckDB [struct](https://duckdb.org/docs/sql/data_types/struct) type. Simply put, the struct type is similar to tuples in Python because the data type is actually an ordered list of columns called entries. These entries can even be other lists or structs.\n\nSo, I‚Äôll need to further flatten my dataset. Luckily DuckDB SQL provides a native function for this exact behavior, UNNEST, and lets me execute it in the same statement I‚Äôm creating a table! This effectively does what flattening with Python functions does, the nice thing is you can set the recursive := parameter to true, letting DuckDB also handle nested structs, within a struct. As you can see, the DuckDB query selects everything except for the data source‚Äôs GUI URL, the original struct data, and the API query for the specific NEO object in that row.\nNow that the table structure is done and ready for data. It‚Äôs time to finish off some final configurations for our dbt project. Both files will go in the models/ folder, namely schema.yml and sources.yml. Here‚Äôs a quick note on the purpose of both.\n\nschema.yml\nThe schema.yml file documents and tests the models and columns created within your dbt project, ensuring data quality and consistency. It allows you to define metadata, such as column descriptions, and add constraints like uniqueness or nullability tests to enforce integrity. This fosters trust in your data pipelines, simplifies debugging, and enhances the readability of your data models for developers and stakeholders.\nversion: 1\n\nmodels:\n  - name: asteroids\n    description: \"A table containing detailed information about near-Earth objects, including their estimated dimensions and close approach data, in metric units.\"\n    columns:\n      - name: id\n        description: \"Unique identifier for the asteroid.\"\n      - name: neo_reference_id\n        description: \"Reference ID assigned by NASA's Near Earth Object program.\"\n      - name: name\n        description: \"Name or designation of the asteroid.\"\n\n\nsources.yml\nThe sources.yml file defines the external data sources your dbt project depends on, such as raw tables or views in your database, and ensures their existence and structure are tested before being used. It helps you document where your data originates, making your project more transparent and easier to maintain. By validating source data and organizing dependencies, it reduces the risk of downstream errors and improves collaboration across teams.\nversion: 1\n\nsources:\n  - name: ingest\n    description: \"Data in its base relational form, after being unnested from the API call\"\n    database: basic_oss\n    schema: ingest\n    tables:\n        - name: raw\n          description: \"A table containing the non-formatted API data\"\n          columns:\n            - name: id\n              description: \"Unique identifier for the asteroid.\"\n            - name: neo_reference_id\n              description: \"Reference ID assigned by NASA's Near Earth Object program.\"\n            - name: name\n              description: \"Name or designation of the asteroid.\"\n\n\n\nCreating dbt models\nNow that the dbt project is configured and I did some feature testing in a notebook, it‚Äôs time to actually write the models which will serve as the tables in our database. The sources and schema files live in the models/ directory. Within the models folder, you‚Äôll create subfolders that will house your actual models. I use these subfolders to group models by schema. I‚Äôm going to have two schemas with one model each, initially, to showcase some dbt features.\n\ningest/ will house the model that calls the API and creates the table.\nstage/ will house the data transformations necessary to get the table in a useful state\n\nThe nice thing about DuckDB is that you can directly query files on the internet. Unfortunately, the NASA API I‚Äôm calling returns the data as text, not a file. So, I‚Äôll need to use Python to make the initial call and fetch the raw data. Luckily, as of dbt-core v1.3, you can now add .py files as dbt models. So, the first step, after creating the models subdirectories, is to convert the code in the Jupyter notebook to a Python script.\nFortunately, as of writing this, GitHub Copilot is now free. So, I was able to easily convert the file and then check it using ruff in about 10 seconds. Copilot lets you define the context you‚Äôre working in, so I had it look at the notebook file and the raw file. Then, after making some slight changes (because of specific dbt functionality) I was able to click a button and have Copilot paste the code in the editor. Finally, I used ruff to check the file for any errors and format it. I‚Äôll share images below.\n\nPython Model\nIt‚Äôs important to know that the actual model has a specific format and output requirement. As you‚Äôll see below, instead of a main function, I created a model(dbt, session) function. It‚Äôs similar to a main function, but is specific for enabling dbt to read Python. Furthermore, the entire .py file needs to output one single DataFrame.\n# Import the required dependencies\nimport requests\nimport duckdb\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n\ndef read_api_key(file_path):\n    \"\"\"\n    Reads the API key from a file.\n\n    Args:\n        file_path (str): Path to the file containing the API key.\n\n    Returns:\n        str: The API key.\n    \"\"\"\n    with open(file_path, \"r\") as file:\n        return file.read().strip()\n\n\ndef construct_url(api_key, start_date, end_date):\n    \"\"\"\n    Constructs the API URL.\n\n    Args:\n        api_key (str): The API key.\n        start_date (str): The start date for the API query.\n        end_date (str): The end date for the API query.\n\n    Returns:\n        str: The constructed API URL.\n    \"\"\"\n    return f\"https://api.nasa.gov/neo/rest/v1/feed?start_date={start_date}&end_date={end_date}&api_key={api_key}\"\n\n\ndef fetch_data(url):\n    \"\"\"\n    Fetches data from the API.\n\n    Args:\n        url (str): The API URL.\n\n    Returns:\n        dict: The JSON response from the API.\n\n    Raises:\n        Exception: If the API request fails.\n    \"\"\"\n    response = requests.get(url)\n    if response.status_code == 200:\n        return response.json()\n    else:\n        raise Exception(f\"Error: {response.status_code}\")\n\n\ndef process_data(data):\n    \"\"\"\n    Processes the API response data.\n\n    Args:\n        data (dict): The JSON response from the API.\n\n    Returns:\n        DataFrame: The processed data as a pandas DataFrame.\n    \"\"\"\n    all_data = []\n    for date in data[\"near_earth_objects\"]:\n        flat_data = pd.json_normalize(data[\"near_earth_objects\"][date])\n        all_data.append(flat_data)\n\n    if all_data:\n        final_data = pd.concat(all_data, ignore_index=True)\n    else:\n        final_data = pd.DataFrame()\n\n    return final_data\n\n\ndef store_data_in_duckdb(con, data):\n    \"\"\"\n    Stores the processed data in DuckDB.\n\n    Args:\n        con (duckdb.DuckDBPyConnection): The DuckDB connection.\n        data (DataFrame): The processed data.\n    \"\"\"\n    con.sql(\"\"\"\n        CREATE TABLE IF NOT EXISTS asteroids AS\n        SELECT * EXCLUDE(nasa_jpl_url, close_approach_data, 'links.self')\n        , unnest(close_approach_data, recursive := true)\n        FROM data\n    \"\"\")\n\n\ndef main():\n    \"\"\"\n    Main function to execute the data ingestion process.\n    Returns a single DataFrame object.\n    \"\"\"\n    api_key_path = \"/Users/chriskornaros/Documents/local-scripts/.api_keys/nasa/key.txt\"\n    api_key = read_api_key(api_key_path)\n\n    # Define the start date and end date for the 7-day window\n    start_date = datetime.strptime(\"1900-01-01\", \"%Y-%m-%d\")\n    end_date = start_date + timedelta(days=7)\n\n    url = construct_url(\n        api_key, start_date.strftime(\"%Y-%m-%d\"), end_date.strftime(\"%Y-%m-%d\")\n    )\n\n    try:\n        data = fetch_data(url)\n        final_data = process_data(data)\n    except Exception as e:\n        print(f\"Failed to fetch data: {e}\")\n        final_data = pd.DataFrame()\n\n    # Open a connection to the persistent database\n    con = duckdb.connect(\"test.duckdb\")\n\n    # Store the data in DuckDB\n    store_data_in_duckdb(con, final_data)\n\n    # Verify the data was stored correctly\n    result = con.sql(\"SELECT * FROM asteroids\").fetchdf()\n    print(result)\n\n    # Close the connection\n    con.close()\n\n    return final_data\n\n\nif __name__ == \"__main__\":\n    df = main()\n    print(df)\n  \nruff check\n# All checks passed!\n\n\nSQL Model\nNext, I‚Äôm going to create a simple SQL model to only use the metric data from the table. Additionally, the model cleans up some column names.\n-- Column names and data types taken from the API Exploration Notebook\nWITH a_raw AS (\n    SELECT *\n    FROM main.asteroids\n)\n\nSELECT id, neo_reference_id, name, close_approach_date, close_approach_date_full, epoch_date_close_approach\n    , absolute_magnitude_h, is_potentially_hazardous_asteroid, is_sentry_object    \n    , kilometers_per_second, kilometers_per_hour, kilometers_miss, orbiting_body\n    , \"estimated_diameter.kilometers.estimated_diameter_min\" AS km_estimated_diameter_min\n    , \"estimated_diameter.kilometers.estimated_diameter_max\"AS km_estimated_diameter_max                                                                                 \n    , \"estimated_diameter.meters.estimated_diameter_min\" AS meters_estimated_diameter_min\n    , \"estimated_diameter.meters.estimated_diameter_max\" AS meters_estimated_diameter_max\nFROM a_raw\n\n\n\nRunning your models\nNow that everything is ready, it‚Äôs finally time for dbt to shine! To ensure functionality, as I expect it, I like to cd dbt_dir/ before running the models. Then, once I‚Äôm in the dbt project directory, all I have to do is execute dbt run from my CLI for DBT to start exectuing scripts. That being said, if you only want to test or run a specific model, you can tell dbt exactly which folder to look at using dbt run --select ingest. Just know, even though you may only run one specific folder, dbt will throw an error if there is an issue with a model in another subfolder.",
    "crumbs": [
      "Data Engineering and Architecture",
      "Basic Open Source Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/basic_oss.html#next-steps",
    "href": "pages/projects/data_engineering/posts/basic_oss.html#next-steps",
    "title": "Basic OSS Architecture",
    "section": "Next Steps",
    "text": "Next Steps\n\nSetup my Raspberry Pi server so I have an always-running VM for job orchestration\nSetup Airflow to schedule the jobs, so every 15 minutes the API is called\n\nThis won‚Äôt break the API rules and will take about 2 months to get all existing data\n\nContinue with the rest of the vision/integrations",
    "crumbs": [
      "Data Engineering and Architecture",
      "Basic Open Source Architecture"
    ]
  },
  {
    "objectID": "pages/about/about.html",
    "href": "pages/about/about.html",
    "title": "About",
    "section": "",
    "text": "Thanks for visiting! As you can see, this site is pretty messy. That is because my programming skills lie with data, not web design. I‚Äôve thrown this together with the help of Quarto documentation, SASS documentation, Bootstrap Documentation, _brand.yml documentation, and of course LLMs. Feel free to report issue as you find them, or suggest improvements because there is a link to the source repo on every page."
  },
  {
    "objectID": "pages/about/about.html#about-me",
    "href": "pages/about/about.html#about-me",
    "title": "About",
    "section": "About Me",
    "text": "About Me\nI‚Äôm a 26 year old Data Scientist, Engineer, Analyst, Architect, etc. that has two degrees from Tulane University in New Orleans‚Äì BSM, Marketing and Asian Studies; MS, Business Analytics. During COVID, I did an extra year of school and received a Masters in Data Science. As a result of my education, I fell in love with data science and machine learning, but due to common data challenges, I developed a professional passion and skillset for database architecture and data engineering.\nI currently work at General Motors in the Global Markets IT organization. Most of the projects I work on outside of GM use different tools and skillsets than I do during my day job. The reason being that I want to stay up to date with all of the tools and changes in the industry. As a result, I get to play with all of the cool licensed tools, like Databricks and Power BI, at work, but I get to build and learn with open source tools externally.\nSince really jumping in to open source development and learning this year, I‚Äôve come to enjoy the study and field of data even more. I know that long term I want to move into a data strategy and innovation role. Designing and architecting systems that enable people to get more out of their data is something I‚Äôm teaching myself now. That being said, I‚Äôm enthralled with both the process and my progress.\nIn the long run, I want to continue working full time as a Data Engineer (or something similar). That being said, I am interested in advising or consulting both individuals and companies on data strategy.\nYou can take a look at my resume down below. My email is listed."
  },
  {
    "objectID": "pages/guides/posts/quarto.html",
    "href": "pages/guides/posts/quarto.html",
    "title": "Quarto",
    "section": "",
    "text": "A non-exhaustive guide on using Quarto for project documentation and personal branding.",
    "crumbs": [
      "Guides",
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#overview",
    "href": "pages/guides/posts/quarto.html#overview",
    "title": "Quarto",
    "section": "Overview",
    "text": "Overview\nQuarto is:\n\n\nAn open-source scientific and technical publishing system\nAuthor using Jupyter notebooks or with plain text markdown in your favorite editor.\nCreate dynamic content with Python, R, Julia, and Observable.\nPublish reproducible, production quality articles, presentations, dashboards, websites, blogs, and books in HTML, PDF, MS Word ePub, and more.\nShare knowledge and insights organization-wide by publishing to Posit Connect, Confluence, or other publishing systems.\nWrite using Pandoc markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more.\n\n\nDownloading and Updating\nFor simple instructions and a download/install guide using a GUI, visit Quarto - Get Started.\nFor MacOS users, I recommend downloading and learning about Homebrew, the package manager. It drastically simplifies all phases of package management. To install, simply use brew install quarto and you‚Äôre done.",
    "crumbs": [
      "Guides",
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#projects",
    "href": "pages/guides/posts/quarto.html#projects",
    "title": "Quarto",
    "section": "Projects",
    "text": "Projects\nThis section, and the rest of the guide, assume you‚Äôre familiar with and using the uv package and project manager for Python, git for version control, and the GitHub CLI for collaboration. I‚Äôll be referencing all of these tools throughout the rest of the guide. You can read my guide to learn more about uv\n\nGeneral Workflow\nI‚Äôll be walking through the general workflow, but here‚Äôs a quick note about how I use Quarto for Data related projects. I use GitHub as my collaboration/repo hosting tool, so all of my projects have a README.md file. That way, if anyone visits the actual repo, they can view a nicely rendered markdown file, but when I‚Äôm ready to add a project to my website, I‚Äôll copy the contents into a .qmd file. Then, I can add the Quarto specific formatting.\nThis simplifies my general workflow a lot, and makes it easy to formally share and document my research.\n\n\nInitializing a Project\n\nThe create command\nI‚Äôm going to assume you‚Äôve already run the uv init command to initalize your uv project. From there, it‚Äôs easy to start a project with Quarto from the command line, and there are a few built-in project types to further simplify the startup process. Furthermore, Quarto provides a simple command for creating (or initializing) a project (or extension), quarto create project, and a handy setup guide to help you use it. The following code shows you my terminal input and outputs.\nchriskornaros@chriss-air test % quarto create project\n? Type ‚Ä∫ default\n? Directory ‚Ä∫ docs\n? Title (docs) ‚Ä∫ test_docs\nCreating project at /Users/chriskornaros/Documents/test/docs:\n  - Created _quarto.yml\n  - Created .gitignore\n  - Created test_docs.qmd\n? Open With ‚Ä∫ (don't open)\nFor a quick run through: quarto create project initializes a quarto project directory within your current working directory (the uv parent directory), type lets you choose the type of Quarto documentation (book, website, confluence, etc.), title is the title of your homepage (.qmd) file. Personally, I like to remove the docs/.gitignore file because uv creates one when you initialize a project, in the parent directory. So, having just one .gitignore file helps me keep track of things more easily.\nThe only directories I added to docs after it was created by quarto, was a pages directory for various subpages and a brand directory for .scss files, images, etc. For project, blog, or guide specific media files, I kept those within their subpage folder. Here, I keep the various landing pages and their sub directory structures. Ideally, I won‚Äôt have any files in there, but the _quarto.yml file will point to their locations in my personal GitHub repo.\n\n\n\n\n\n\nFile Context in Quarto\n\n\n\nIn my time developing this site, it seems that Quarto can only pickup on files within the context of the docs (or whatever you name your Quarto project) folder. Furthermore, it struggels with absolute context paths, and at most I could get it to work with ../../file.\n\n\n\n\n\nWorking on your Project\nNow that you‚Äôve initalized your project directory, you can begin work! Head over to Quarto Basics for documentation on the basics of .qmd files and authoring with Quarto markdown.\nJust remember, every webpage will need a .qmd file!\n\n\nRendering a Projects\nThis part is blank for now. Rendering websites have some specific components to websites and GitHub pages, that are covered later on. I will update this for other document types in the future.",
    "crumbs": [
      "Guides",
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#configurations",
    "href": "pages/guides/posts/quarto.html#configurations",
    "title": "Quarto",
    "section": "Configurations",
    "text": "Configurations\n\nThe _quarto.yml file\nThis YAML file serves as the primary configuration file for your Quarto project. Similar to other Quarto YAML files, this handles document configurations, but adds the Quarto project features to sync those across documents and for more environment control. You have the ability to define project metadata for all of the different document types. In this example, I used it to define the website configurations, but if you‚Äôre working on a book or dashboard, then it could be used to normalize chapters or visuals as well.\nYou can also specify the formatting, which connects with the _brand.yml file and enables cross referencing of variables and values. Learn more with Quarto Projects.\n\n\nThe _brand.yml File\nThis is a new feature with Quarto 1.6 that allows you to define and save your design specifications in a YAML file. While this file is specific to your Quarto project directories, you can store and share the file across projects or with others to maintain brand consistency. Luckily, there is great documentation if you want more details brand.yml. While there is a lot to cover, I‚Äôll go over some basics to get started. It‚Äôs important to remember that if you specify colors for anything within .qmd files, those will overwrite the defaults in the brand file. Furthermore, Quarto and _brand.yml both utilize the Bootstrap web development framework. For a list of its full default values, visit the repo.\n\nColor\nThis is obviously an important part of all branding. There are two main components:\n\npalette\ntheme colors\n\nPalette lets you specify hexcodes and assign those to various strings. Those string values could be generic terms, like green (if there is a specific shade you would like), or terms specific to brand.yml's theme colors. When you set your default colors in this way, you can then customize the output in the _quarto.yml file. To modify, for example, your navigation bar, just define the background and foreground properties under the navbar property.\nAnother thing to keep in mind with color, just because it‚Äôs available in _brand.yml, like tertiary, doesn‚Äôt mean it‚Äôs defined and functional in the _quarto.yml file. So, you may need to be creative with how you use protected terms, like success, danger, or warning. Doing so allows you to take advantage of the programmatic benefits of the brand file, while specifying several, possibly, similar shades that would be tricky to do just be renaming colors, such as red, blue, or yellow.\nIf you aren‚Äôt sure on what colors or palettes to choose, using an LLM based chatbot can be helpful. This allows you to describe the colors and themes you‚Äôre going for, as well as refine them over time.\n\n\nTypography\nThis section lets you control which font families are included in your Quarto project. Then, you can specify where various fonts are used and for some properties, even change their specific color. As a heads up, the _brand.yml documentation seems to be correct and updated; however, bash code blocks don‚Äôt render the monospace-background the same way. So, while in-line monospace backgrounds and monospace backgrounds for Python (at the very least) will be colored as the documentation says. Bash code blocks will have no background, just the code itself in the specified font color.\n\n\nDefaults\nThis section gives you more control over various defaults, for HTML Bootstrap, Quarto, Shiny, etc. When configuring specific design colors, using the bootstrap default section will allow you to keep your Quarto files simple, while providing a high level of control over design.\n\n\nSASS - Syntactically Awesome Style Sheets\nRemember, whatever you can‚Äôt configure simply in your _brand.yml file, you can do so in a .scss file. For example, if you want to create custom light and dark mode themes, just create .scss files with the appropriate code and place this in your docs (main Quarto project) directory. Below is an example of a dark mode theme. I set the default values for the scss bootstrap variables at the top. Then, I specified the specific rules for various parts of the page. For defined variables, blockquote, you don‚Äôt need a ., but for features specific to quarto rendered sites, add a . before. For example, to modify the look of code blocks, you must use the .sourceCode variable. For child classes, for example the .sourceCode css copy-with-code class, if you want to modify that you‚Äôll need to use .sourceCode pre.copy-with-code. To find out the name of a variable you don‚Äôt know, just inspect the specific element on the webpage, and the class name will translate 1:1 with the variable name. Additionally, for any property that you need to specifically update, you can add the !important tag, which means it will override existing rules, but be careful using this.\nFor a list of all CSS variable properties, visit CSS Web Docs.\n/*-- scss:defaults --*/\n$background: #2E4053;\n$foreground: #F7F7F7;\n$primary: #FF9900;\n$secondary: #56B3FA;\n$tertiary: #655D4B;\n$light: #F7F7F7;\n$dark: #1C1F24;\n$success: #33373D;\n$danger: #1A1D23;\n$info: #56B3FA;\n$warning: #FF7575;\n\n\n/*-- scss:rules --*/\nbody {\n  font-family: 'Open Sans', sans-serif;\n  background-color: $background;\n  color: $foreground;\n}\n\nh1, h2, h3 {\n  color: $danger;\n}\n\nh4, h5, h6 {\n  color: $danger;\n  font-weight: bold;\n}\n\nblockquote {\n  background-color: #2E6490; /* added background color */\n  border-color: $dark;\n  color: $danger !important;\n}\n\ncode {\n  background-color: $success;\n  color: $info;\n}\n\n.sourceCode {\n  background-color: $success;\n  color: $info;\n}\n\n.sourceCode pre.code-with-copy{\n  padding: 0;\n}\n\n.callout-title-container.flex-fill {\n  color: $danger;\n}\n\n\n\n\n\n\nCSS Variable Names\n\n\n\nThere are some weird naming convention differences between _brand.yml and Quarto. The big one is monotone being used to reference block quotes, code blocks, and in-line code in _brand, but in Quarto it renders the in-line code as code and the code blocks as sourceCode. Make sure to use inspect element to be sure on what you‚Äôre changing. CSS class names can get long, especially when referncing nested classes, just experiment and take your time with things.",
    "crumbs": [
      "Guides",
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#websites",
    "href": "pages/guides/posts/quarto.html#websites",
    "title": "Quarto",
    "section": "Websites",
    "text": "Websites\nWebsites really stretch and push the boundaries of what you can accomplish with Quarto. In this section, I‚Äôll walk through a few key points of developing them.\n\nBlogs\nBlogs are a special kind of Quarto website that consists of a collection of posts along with a navigational page that lists them in reverse chronological order. Pretty much all of the information you‚Äôll need about blogs is the same as the parts of this guide covering websites. Just know, it‚Äôs easy to integrate a blog as a subpage of a larger website.\nSimply add the blogs project structure as a subdirectory of pages/. To keep track of things, I made the title of the main blog page blogs.qmd, so it doesn‚Äôt conflict with the index.qmd that is the home page of my whole website. Then, I added post categories within the posts/ directory of the Quarto blogs/ directory.\nThat being said, and I‚Äôm not quite sure why, but the _metadata.yml file\n\n\nRendering Websites\nI run the following code block from my main project directory. My Quarto project directory is a folder called docs. So, I specify to Quarto that I want to render the entire Quarto project docs, but quarto render‚Äôs context is specific to the quarto project directory. Therefore, I need to use the . to specify that I want the rendered .html files put in the Quarto project folder, and sub folder.\nquarto render docs --output-dir .\nConversely, you can specify, within the output property of your _quarto.yml file that output-dir: .\nThis is also the same syntax when previewing your website, using quarto preview docs, the difference is there is no need to specify an output directory. What this does is spin up a jupyter kernel to render your .qmd files, then, it displays the output in a browser. When you hit save on your _quarto.yml, .scss, and .qmd files then the site will automatically update (it doesn‚Äôt for _brand.yml saves).\nOnce you‚Äôve rendered your website, and pushed the commit, the change is reflected in a few mintues.\n\n\n\n\n\n\nquarto preview with uv\n\n\n\nThe ease of using quarto preview is magnified when using uv as your project/package manager. Instead of having to manage various virtual environments and packages, as well as activation and deactivation, uv does it all. Even VS Code picks up on the context uv provides. The terminal will automatically realize you‚Äôre in a uv environment and display output as if you were using a virutal environment (even though you haven‚Äôt activated it).\n\n\n\n\nWebsite Navigation\n\nTop Navigation\nAfter you‚Äôve set your default color values in _brand.yml, make sure to specify the design details at the top of your navbar property. This is useful, even when using .scss files for more specific design control because you can utilize those variables in your light and dark themes.\nFor pages on your top navigation bar that just have a landing page, simple use the following syntax\nnavbar:\n  left:\n    text: \"Page name\"\n    href: path/to/page/file.qmd\n\n\n\n\n\n\nDashes and Intentation Matter in YAML\n\n\n\nNotice when I‚Äôm using a - and not. This is deliberate. In my development, I realized that where you use and specify the dash can affect functionality. Some places require it, some don‚Äôt, and it may depend on the order of various parameters.\n\n\nFor page categories that may have several landing pages, or even subcategories, you‚Äôll need to utilize hybrid navigation which combines Top and Side navigation. On the top, you‚Äôll use the following syntax:\nnavbar:\n  left:\n    text: \"Page group name\"\n    menu:\n      - path/to/page/group/landing.qmd\n      - path/to/page/group/1/landing.qmd\n      - path/to/page/group/2/landing.qmd\nThen, you‚Äôll need to handle the rest in Side Navigation; however, it isn‚Äôt perfect. You can‚Äôt have nested drop down options in your top navigation bar, so the best I came up with was having a landing page for the top level and first tier subcategories, then handled the rest on the sidebar (which only pops up on affiliated pages).\n\n\nSide Navigation\nFor some reason, Side Navigation in Quarto is much more robust and intuitive. That being said, by combining features here with the top bar, you can achieve a fairly dynamic navigation experience.\nThere are a few key differences. To start with, sidebar objects inherit properties from the first defined, so long as none are changed. Second, you‚Äôll want to use an id with the top level landing pages, because this allows you to reference those in your top navigation bar (for more advanced integrations) using the address sidebar:id, although I struggled with this functionality and didn‚Äôt end up using it.\nThe general structure for your first page group is as follows.\nsidebar:\n  - id: guides\n    title: \"Guides\"\n    style: \"docked\"\n    background: dark\n    foreground: light\n    collapse-level: 2\n    contents:\n      - section: \"Guides\"\n        href: pages/guides/guides.qmd\n        contents:\nNow, if that‚Äôs where things end, you could just list pages on and on using the text: href: syntax. That being said, you probably are going to have a few subcategories, and possibly even further nested subcategories. To enable this, don‚Äôt use the text: syntax, instead use section:. This tells Quarto that you are defining a section, rather than just one single page. As you might guess, you can further nest sections, or specific pages, depending on your use of text: and section: with href:. See an example below.\nid: projects\n      title: \"Projects\"\n      contents:\n        - pages/projects/projects.qmd\n        - section: \"Data Engineering and Architecture\"\n          href: pages/projects/data_engineering/data_engineering.qmd\n          contents:\n            - text: \"Bank Marketing ETL\"\n              href: pages/projects/data_engineering/posts/bank_etl.qmd\n            - text: \"Open Source Data and Analytics Architecture\"\n              href: pages/projects/data_engineering/posts/oss_data_arch.qmd\n            - text: \"Basic Open Source Architecture\"\n              href: pages/projects/data_engineering/posts/basic_oss.qmd\nFor subsections, the landing page‚Äôs .qmd file should be specified within an href paramter, under the section line. Additionally, for the collapsable functionality to work consistently in a sidebar, you‚Äôll need it docked. The behavior is inconsistent with floating sidebars. After you‚Äôve set your default color values in _brand.yml, make sure to specify the design details at the top of your sidebar property. Having a section provides the dropdown functionality on your sidebar.\n\n\n\nSharing Websites\nThere are two primary ways to publish your website once you‚Äôre done making edits, assuming you‚Äôre also using GitHub Pages.\n\nquarto render docs\nquarto publish docs\n\nFor simplicity, I chose to use quarto render docs (note that docs is used here because that‚Äôs the name of my main quarto project directory, not because it‚Äôs part of the command itself) because all I need to do is that and then push the changes. With quarto publish docs, it appeared to me that I would need to setup a branch for my git repository and possibly GitHub actions. I will probably do this in the future, for learning purposes, but didn‚Äôt want to for the sake of time.\nThat being said, the official documentation is very straightforward, and regardless of what you choose, there are two common steps:\n\ntouch .nojekyll\n\nThis tells GitHub pages not to do any additional processing of your website, include this in your docs directory\n\nIn a browser go to GitHubPagesRepo &gt; Settings &gt; Code and automation &gt; Pages\n\nThen, make sure Source is set to Deploy from a branch\nSet your branch to the quarto project directory, in your main project folder, docs in my case\n\n\nThen the classic:\n\ngit add docs\ngit commit -m \"Website updates.\"\ngit push\n\n\n\nWebsite Tools\nQuarto offers several out of the box tools to enhance websites. Some of these are incredibly common for marketing or sharing your content, but all add value in their own way: Google Analytics, Twitter Cards, Open Graph, and RSS Feeds to name a few. The nice thing is they are incredibly easy to setup, and begin working immediately. Google Analytics for example tracked me when I was testing changes in the preview mode.\nThat being said, I tried to implement an RSS feed for the website and it broke Quarto. I was still able to render the output, but I was receiving the ‚ÄúSource cannot be Target‚Äù error. To be able to use quarto preview and quarto render (and get a successfull STDOUT) again I had to remove the robot.txt file, the sitemap.xml file, and the feed: true property from the blog landing page files (my guides.qmd for example).",
    "crumbs": [
      "Guides",
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#other-notes",
    "href": "pages/guides/posts/quarto.html#other-notes",
    "title": "Quarto",
    "section": "Other Notes",
    "text": "Other Notes\nI‚Äôll update this section with more notes and tips that come to mind as I finish building out the site, version 1.0. Then, I‚Äôll reorganize what goes here into the proper places on the document.\n\nIf you want to use past .ipynb files as documentation, or add longer write ups to those files, there is a jupyter command\n\njupyter nbconvert file.ipynb --to markdown --output file.md\nmv file.md &gt; file.qmd\nDone! Just make any quarto specific modifications that you need",
    "crumbs": [
      "Guides",
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#conclusion",
    "href": "pages/guides/posts/quarto.html#conclusion",
    "title": "Quarto",
    "section": "Conclusion",
    "text": "Conclusion\nNow, you‚Äôre all done with this guide, thank you for reading!\nCurrently, this is only updated to include my notes and thoughts from when I built my personal website. As I use Quarto to create a variety of document types, I will update this Guide with more. Follow me on Bluesky to stay connected with me and up to date with my work.",
    "crumbs": [
      "Guides",
      "Quarto"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#starting-from-close-to-zero",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#starting-from-close-to-zero",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Starting from (close) to Zero",
    "text": "Starting from (close) to Zero\n\nHardware Requirements\nThis section will provide basic setup instructions, so you‚Äôll have the same tools I do and can follow along with this guide, step-by-step.\n\nRaspberry Pi 4 8GB\nMicro HDMI to HDMI cord (for direct access)\nAppropriate Power Supply\nKeyboard (connected via USB for direct access)\n1TB Samsung T7 SSD (connected via USB for boot media/core memory)\nAmazon Basics 128GB microSD card (or other microSD cards for backups media)\nSSH-capable devices for headless access\n\nI‚Äôm using a MacBook Air\nI prefer the MacOS and Terminal for personal development, because I use Windows at my day job\n\n\n\n\nImage requirements\nOnce you have your hardware ready to go, you can being setting up the software. I‚Äôm using Linux Server LTS because it‚Äôs a stable version of Linux intended for headless, server environments. LTS means long-term support, so unlike the more frequently updated versions, these OS versions are supported for 5 years. Additionally, you‚Äôll want to use public-key authentication for better security purposes, but more on that in the SSH Section.\n\nHave your SSD ready and able to connect to your laptop or desktop (whichever you plan to use with SSH)\nDownload Raspberry Pi Imager from the official website\nRun the Imager and configure your installation of the most recent Linux Server LTS image\n\nSelect your Raspberry Pi device\n\n\n\n\nUbuntu-Pi Device\n\n\n\nSelect the OS Image you want to flash\n\n\n\n\nUbuntu-Pi OS\n\n\n\nSelect the media storage device for the image\n\n\n\n\nUbuntu-Pi Storage\n\n\n\nConfigure settings\n\n Here, you‚Äôll configure your primary user ID and password; network connection; locale and timezone; and your hostname (the nickname your computer remembers the IP address as).\n Here, you‚Äôll configure your SSH settings. Remember to use public-key authentication only, and to keep the private key generated on your local machine safe.\n These are more preference based, but it‚Äôs nice to have the storage device automatically eject once the flashing is complete. Then, you just need to unplug it and plug it into your Raspberry Pi to get going."
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-ssh",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-ssh",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "SSH",
    "text": "SSH\nNow that you have your basic Ubuntu Pi server configured and connected to a network, it‚Äôs time to do some final configurations before beginning the more coding focused development. For the coding side of things, we‚Äôll want to remotely connect using a different computer and connection than the wired keyboard/monitor with our Raspberry Pi. To do this, we‚Äôll utilize VS Code, an open source IDE (integrated development environment) from Microsoft. Before that, we‚Äôll need to setup and configure SSH (Secure Shell), one of the most common ways to connect to a remote server. Simply put, SSH is a network protocol that creates an encrypted tunnel between computers, allowing secure remote management. Think of it as establishing a private, secure telephone line that only authorized parties can use to communicate.\nOnce we have SSH setup, configured, and secured, we‚Äôll use a feature in VS Code called Remote - SSH which lets us use the nice UI of an IDE while working on the actual server. This is really beneficial for a variety of reasons: one of them being the fantastic community-built extensions that drastically improve the development experience, another being the integration with other tools for things like CI/CD.\n\nUnderstanding SSH Configuration Files\n\nSSH Client vs Server Configuration\nThe SSH system uses two main configuration files with distinct purposes:\n\nssh_config:\n\nLives on your client machine (like your laptop)\nControls how your system behaves when connecting to other SSH servers\nAffects outgoing SSH connections\nLocated at /etc/ssh/ssh_config (system-wide) and ~/.ssh/config (user-specific)\n\nIf your server ever moves or connects to a new IP address, simply update it in the user config file\n\n\nsshd_config:\n\nLives on your server (the Raspberry Pi)\nControls how your SSH server accepts incoming connections\nDetermines who can connect and how\nLocated at /etc/ssh/sshd_config\nRequires root privileges to modify\nChanges require restarting the SSH service\n\n\n\n\n\nKey-Based Authentication Setup\n\nUnderstanding SSH Keys and Security\nThis guide uses ECDSA-384 keys, which offer several advantages:\n\nUses the NIST P-384 curve, providing security equivalent to 192-bit symmetric encryption\nBetter resistance to potential quantum computing attacks compared to smaller key sizes\nStandardized under FIPS 186-4\nExcellent balance between security and performance\n\n\n\nGenerating Your SSH Keys\nYou might remember from the beginning of this guide that you can generate an SSH key-pair when flashing the image using RPi Imager. If you didn‚Äôt do that and want to learn how you can handle this all, the old fashioned way, then on your laptop, generate a new SSH key pair:\n# Generate a new SSH key pair using ECDSA-384\nssh-keygen -t ecdsa -b 384 -C \"ubuntu-pi-server\"\nThis command:\n\n-t ecdsa: Specifies the ECDSA algorithm\n-b 384: Sets the key size to 384 bits\n-C \"ubuntu-pi-server\": Adds a descriptive comment\n\nThe command generates two files:\n\n~/.ssh/ubuntu_pi_ecdsa: Your private key (keep this secret!)\n~/.ssh/ubuntu_pi_ecdsa.pub: Your public key (safe to share)\n\n\n\nInstalling Your Public Key on the Raspberry Pi\nTransferring your public key to your Raspberry Pi is easy, just know the following will not work if you generated a key-pair when flashing the image. This only works if you currently have password authentication enabled.\nssh-copy-id -i ~/.ssh/ubuntu_pi_ecdsa.pub chris@ubuntu-pi-server\nThis command:\n\nConnects to your Pi using password authentication\n\nIf you‚Äôre restoring your config, you‚Äôll need to temporarily set PasswordAuthentication in /etc/ssh/sshd_config to yes\n\nCreates the .ssh directory if needed\nAdds your public key to authorized_keys\nSets appropriate permissions automatically\n\n\n\n\nServer-Side SSH Configuration\nA client-server relationship is a fundamental computing model that underpins most network communications and distributed systems. This architecture divides computing responsibilities between service requestors (clients, your laptop in this case) and service providers (servers, the Raspberry Pi in this case).\nA server is a computer program or device that provides functionality, resources, or services to multiple clients.\n\nService Provider: Responds to client requests rather than initiating communication\nResource Management: Manages shared resources (files, databases, computational power)\nContinuous Operation: Typically runs continuously, waiting for client requests\nScalability: Often designed to handle multiple concurrent client connections\nExamples: Web servers, database servers, file servers, mail servers, authentication servers\n\nClient-server communication follows a request-response pattern:\n\nConnection: The client establishes a connection to the server\nRequest: The client sends a formatted request for a specific service\nProcessing: The server processes the request according to its business logic\nResponse: The server returns appropriate data or status information\nDisconnection or Persistence: The connection may be terminated or maintained for future requests\n\nThis communication typically occurs over TCP/IP networks using standardized protocols that define the format and sequence of messages exchanged.\n\nUnderstanding Server Host Keys\nYour Pi‚Äôs /etc/ssh directory contains several important files: - Host key pairs (public and private) for different algorithms - Configuration files and directories - The moduli file for key exchange\n\n\nOptimizing Server Security\n\nBack up the original configuration:\n\nsudo cp /etc/ssh/sshd_config /etc/ssh/sshd_config.backup-$(date +%Y%m%d)\n\nOptimize host key settings in sshd_config:\n\n# Specify host key order (prioritize ECDSA)\nHostKey /etc/ssh/ssh_host_ecdsa_key\nHostKey /etc/ssh/ssh_host_ed25519_key\nHostKey /etc/ssh/ssh_host_rsa_key\n\nStrengthen the moduli file:\n\n# Back up the existing file\nsudo cp /etc/ssh/moduli /etc/ssh/moduli.backup\n\n# Remove moduli less than 3072 bits\nsudo awk '$5 &gt;= 3072' /etc/ssh/moduli &gt; /tmp/moduli\nsudo mv /tmp/moduli /etc/ssh/moduli\n\nApply changes:\n\n# Test the configuration\nsudo sshd -t\n\n# Restart the SSH service (on Ubuntu Server)\nsudo systemctl restart ssh\n\n# Verify the service status\nsudo systemctl status ssh\nJust note, you‚Äôll probably need to reboot (sudo reboot) your server before all of the changes fully take place. Once you‚Äôve done that, you may need to run sudo systemctl start ssh.\n\n\n\nClient-Side Configuration\nA client is a computer program or device that requests services, resources, or information from a server.\n\nRequest Initiator: Clients always initiate communication with servers\nUser Interface: Often provides the interface through which users interact with remote services\nLimited Resources: Typically has fewer resources than servers and offloads intensive processing\nDependency: Relies on servers to fulfill requests and cannot function independently for networked operations\nExamples: Web browsers, email clients, SSH clients, mobile applications\n\nFrom a technical perspective, clients:\n\nFormulate and send requests using specific protocols (HTTP, FTP, SMTP, etc.)\nWait for and process server responses\nPresent results to users or use them for further operations\n\n\nSetting Up Your SSH Config\nCreate or edit ~/.ssh/config on your laptop:\nHost ubuntu-pi-server\n    HostName ubuntu-pi-server\n    User chris\n    IdentityFile ~/.ssh/ubuntu_pi_ecdsa\n    Port 45000\n\n\n\n\n\n\nSSH Config: Include\n\n\n\nIf your ssh isn‚Äôt picking up on the ~/.ssh/ssh_config then you might need to specify it in the system config. Find the line in /etc/ssh/ssh_config that says Include and add the absolute file path. If you need to include more than your user specific config, such as the default /etc/ssh/ssh_config.d/* just add that absolute path separated by a space from any other path included.\n\n\n\n\nManaging Known Hosts\n\nBack up your current known_hosts file:\n\ncp ~/.ssh/known_hosts ~/.ssh/known_hosts.backup\n\nView current entries:\n\nssh-keygen -l -f ~/.ssh/known_hosts\n\nRemove old entries:\n\n# Remove specific host\nssh-keygen -R ubuntu-pi-server\n\nHash your known_hosts file for security:\n\nssh-keygen -H -f ~/.ssh/known_hosts\n\n\nSecuring the Key File\nWhen using SSH key-based authentication, adding a password to your key enhances security by requiring a passphrase to use the key. This guide explains how to add and remove a password from an existing SSH key.\nAdding a Password to an SSH Key\nIf you already have an SSH key and want to add a password to it, use the following command:\nssh-keygen -p -f ~/.ssh/id_rsa\nExplanation:\n-p : Prompts for changing the passphrase.\n-f ~/.ssh/id_rsa : Specifies the key file to modify (adjust if your key has a different name).\nYou will be asked for the current passphrase (leave blank if none) and then set a new passphrase.\nRemoving a Password from an SSH Key\nIf you want to remove the passphrase from an SSH key, run:\nssh-keygen -p -f ~/.ssh/id_rsa -N \"\"\nExplanation:\n-N \"\" : Sets an empty passphrase (removes the password).\nThe tool will ask for the current passphrase before removing it.\nVerifying the Changes\nAfter modifying the key, test the SSH connection from your CLI, or using an SSH tunnel.\nssh -i ~/.ssh/id_rsa user@your-server\nIf you added a passphrase, you‚Äôll be prompted to enter it when connecting.\nBy using a passphrase, your SSH key is protected against unauthorized use in case it gets compromised. If you frequently use your SSH key, consider using an SSH agent (ssh-agent) to cache your passphrase securely.\n\n\n\nAdditional Security Measures\nNetwork security for a Linux server relies on multiple layers of protection. Two essential components are a properly configured firewall and intrusion prevention systems. This section covers UFW (Uncomplicated Firewall) and Fail2Ban, two powerful yet accessible tools for hardening your Ubuntu Pi Server.\n\nFirewall Configuration with ufw\nA firewall acts as a barrier between your server and potentially hostile networks by controlling incoming and outgoing traffic based on predetermined rules. UFW provides a user-friendly interface to the underlying iptables firewall system in Linux.\n\nDefault Deny Policy: Start with blocking all connections and only allow specific permitted traffic\nStateful Inspection: Track the state of active connections rather than just examining individual packets\nPort Control: Allow or block access based on specific network ports\nSource Filtering: Control traffic based on originating IP addresses or networks\n\n# Install UFW (if it isn't already)\nsudo apt install ufw\n\n# Allow SSH connections\nsudo ufw allow ssh\n\n# Enable the firewall\nsudo ufw enable\nBehind the scenes, UFW translates these simple commands into complex iptables rules, making firewall management accessible without sacrificing security. The underlying iptables system uses a chain-based architecture to process packets through INPUT, OUTPUT, and FORWARD chains. Now, you‚Äôll want to add rules for example, allowing traffic on a specific port if you took the step to choose a nonstandard, one that isn‚Äôt the default Port 22, for this guide, I‚Äôm choosing 45000.\n# Add a new rule in the port/protocol format\nsudo ufw add 45000/tcp\n\n# Allow traffic between subnets 10.0.1.0/24 and 10.0.2.0/24\nsudo ufw allow from 10.0.1.0/24 to 10.0.2.0/24\n\n# See a list of all rules\nsudo ufw status numbered\n\n# Remove the default rules\nsudo ufw delete 1\n\n\nFail2Ban\nFail2Ban is a security tool designed to protect servers from brute force attacks. It works by monitoring log files for specified patterns, identifying suspicious activity (like multiple failed login attempts), and banning the offending IP addresses using firewall rules for a set period. It‚Äôs especially useful for securing SSH, FTP, and web services.\nThe best part is the project is entirely open source, you can view the source code and contribute here.\n# Install Fail2Ban\nsudo apt update\nsudo apt install fail2ban\n\n# Start and enable Fail2Ban\nsudo systemctl start fail2ban\nsudo systemctl enable fail2ban\n\n# Check the status of all jails\nsudo fail2ban-client status\n\n# Check the status of a specific jail\nsudo fail2ban-client status sshd\n\n# View banned IPs\nsudo iptables -L -n | grep f2b\nI want to add that fail2ban automatically pulls values for its jails depending on how you‚Äôve configured things on your system, at least I assume so. I‚Äôm assuming that because I never configured specific ssh rules for fail2ban, but it knows to allow the port I set in my sshd_config. That being said, you can see how simple it was to setup these tools, and how they work together to create a comprehensive security system:\n\nUFW establishes the baseline by controlling which ports are accessible\nFail2Ban adds behavioral analysis by monitoring authentication attempts\nTogether they provide both static and dynamic protection\n\nThis layered approach follows the defense-in-depth principle essential to modern cybersecurity. By combining a properly configured firewall with an intrusion prevention system, you significantly reduce the attack surface of your Ubuntu Pi Server.\n\n\nRegular Security Checks\n\nMonitor SSH login attempts:\n\nsudo journalctl -u ssh\n\nCheck authentication logs:\n\nsudo tail -f /var/log/auth.log\n\n\n\nSCP (Secure Copy Protocol) and rsync (Remote Sync)\nThis section outlines the process of securely copying files between your Ubuntu Pi Server and your Client machine. I‚Äôll cover two powerful methods: SCP (Secure Copy Protocol) and rsync. Both tools operate over SSH, ensuring your file transfers remain encrypted and secure.\nSCP is a simple file transfer utility built on SSH that allows you to copy files between computers. It‚Äôs straightforward for basic transfers but lacks advanced features for large or frequent transfers.\nrsync is a more sophisticated file synchronization and transfer utility that offers several advantages over SCP:\n\nIncremental transfers: Only sends parts of files that changed\nResume capability: Can continue interrupted transfers\nBandwidth control: Can limit how much network it uses\nPreservation options: Maintains file timestamps, permissions, etc.\nDirectory synchronization: Can mirror directory structures\nExclusion patterns: Can skip specified files/directories\n\n\nEnsuring Your SSH Configuration Works\nBefore attempting file transfers, verify your SSH connection is properly configured:\nssh -F ~/.ssh/config chris@ubuntu-pi-server\nThis command explicitly specifies the user configuration file location with the -F flag.\nNote: To ensure SSH always uses your user-specific config:\n\nSet proper permissions on your config file:\n\nchmod 600 ~/.ssh/config\n\nUpdate the system-wide SSH config to include your user config:\n\nsudo nano /etc/ssh/ssh_config\nAdd this line:\nInclude ~/.ssh/config\nAfter applying these changes, you should be able to connect using the simplified command:\nssh ubuntu-pi-server\n\n\nCopying Individual Files from Server to Client\nThe basic syntax for copying files from your server to your local machine is shown below. Know that in all code examples in this section, you should run it in a terminal on your client/local machine:\nscp ubuntu-pi-server:~/configs/wpa_supplicant-wlan0.conf ~/Documents/raspberry_pi_server/configs\nscp ubuntu-pi-server:~/configs/25-wireless.network ~/Documents/raspberry_pi_server/configs\nEach command performs the following actions:\n\nscp: Invokes the secure copy program\nubuntu-pi-server:~/configs/wpa_supplicant-wlan0.conf: Specifies the source file on the remote server\n~/Documents/raspberry_pi_server/25-wireless.network: Specifies the destination directory on your local machine\n\n\n\nCopying Multiple Files at Once\nTo copy all Bash scripts from a directory in one command:\nscp chris@ubuntu-pi-server:~/configs/*.sh ~/Documents/raspberry_pi_server/configs\nThe wildcard pattern *.sh tells SCP to match all files with the .sh extension. Here, I‚Äôve included the username chris@ explicitly, which can help resolve connection issues if your SSH config isn‚Äôt being properly recognized.\n\n\nRecursively Copying Directories\nTo copy entire directories with their contents:\nscp -r chris@ubuntu-pi-server:~/mnt/backups/ ~/Documents/raspberry_pi_server/backups/configs\nThe -r flag (recursive) tells SCP to copy directories and their contents.\n\n\nCopying Files from Client to Server\nTo send files in the opposite direction (local to remote):\nscp -r ~/Documents/pi-scripts chris@ubuntu-pi-server:~/scripts\n\n\nTransferring Files with rsync\nFor larger files or when you need to synchronize directories, rsync offers significant advantages over SCP.\nTo copy a single file from server to client:\nrsync -avz ubuntu-pi-server:~/configs/25-wireless.network ~/Documents/raspberry_pi_server/configs\nLet‚Äôs break down these common flags:\n\n-a: Archive mode, preserves permissions, timestamps, etc. (shorthand for -rlptgoD)\n-v: Verbose, shows detailed progress\n-z: Compresses data during transfer, saving bandwidth\n\n\n\nSyncing Directories with rsync\nTo sync an entire directory from server to client:\nrsync -avz --progress ubuntu-pi-server:~/configs/ ~/Documents/raspberry_pi_server/configs\nThe --progress flag shows a progress bar for each file transfer, which is particularly useful for large files.\nImportant Note: The trailing slash on the source path (~/configs/) is significant\n\nWith trailing slash: Copies the contents of the directory\nWithout trailing slash: Copies the directory itself and its contents\n\n\n\nSyncing in Reverse (Client to Server)\nTo sync files from your client to the server:\nrsync -avz --progress ~/Documents/raspberry_pi_server/configs chris@ubuntu-pi-server:~/configs/\n\n\nUsing rsync with Dry Run\nBefore performing large transfers, you can see what would happen without actually making changes:\nrsync -avzn --progress ~/Documents/raspberry_pi_server/configs chris@ubuntu-pi-server:~/configs/\nThe -n flag (or --dry-run) simulates the transfer without changing any files, letting you verify what would happen.\n\n\nIncremental Backups with rsync\nrsync excels at keeping directories in sync over time. After the initial transfer, subsequent runs only transfer what‚Äôs changed:\nrsync -avz --delete ~/Documents/raspberry_pi_server/configs chris@ubuntu-pi-server:~/configs/\nThe --delete flag removes files from the destination that no longer exist in the source, creating a perfect mirror. Use with caution!\n\n\nAdvanced rsync Examples\n\n\nCustom SSH Parameters\nTo specify specific ssh paramters, such as key file or port:\nrsync -avz --progress -e 'ssh -p 45000 -i ~/.ssh/ubuntu_pi_ecdsa'  chris@192.168.1.151:/mnt/backups/configs/master backups/configs/\nThe e flag tells rsync to execute ssh with those specific flags, when it initiates the connection.\n\n\nExcluding Files or Directories\nTo skip certain files or directories during transfer:\nrsync -avz --exclude=\"*.tmp\" --exclude=\"node_modules\" ~/Documents/raspberry_pi_server/configs chris@ubuntu-pi-server:~/configs/\nThis command excludes all .tmp files and the node_modules directory.\n\n\nSetting Bandwidth Limits\nIf you need to limit how much network bandwidth rsync uses:\nrsync -avz --bwlimit=1000 ~/Documents/raspberry_pi_server/configs chris@ubuntu-pi-server:~/configs/\nThe --bwlimit=1000 restricts transfer speed to 1000 KB/s (approximately 1 MB/s).\n\n\nPreserving Hard Links\nWhen backing up systems that use hard links (like Time Machine or some backup solutions):\nrsync -avH ~/Documents/raspberry_pi_server/configs chris@ubuntu-pi-server:~/configs/\nThe -H flag preserves hard links, which can save significant space in backups.\n\n\nChoosing Between SCP and rsync\nUse SCP when: - You need a quick, one-time file transfer - You want a simple command with minimal options - The files are small and not changing frequently\nUse rsync when: - You need to synchronize directories - You‚Äôre transferring large files that might get interrupted - You want to maintain exact mirrors of directory structures - You‚Äôre setting up automated backups - You need to preserve file attributes like permissions and timestamps - You need to exclude certain files or patterns\n\nSSH Configuration: Ensure your SSH config is properly set up before attempting file transfers\nSCP: Simple, straightforward file copying between systems\nrsync: More powerful synchronization tool with many options for efficiency\n\nSSH is now correctly configured and working using ssh ubuntu-pi-server.\nBash scripts can be securely copied from the Ubuntu Pi Server to the client machine using scp.\n\nJust take note of the specific syntax used, namely server-name:path/to/files\n\nThe user can now maintain local backups of important scripts efficiently.\n\nEnables you to develop where you‚Äôd like and then easily move files to test scripts\n\n\nTrailing Slashes: Pay attention to trailing slashes in paths, as they change behavior\nDry Run: Use --dry-run with rsync to preview what will happen\nAutomation: Consider creating scripts for routine backup tasks\n\nBoth SCP and rsync are invaluable tools for managing files on your Raspberry Pi server. While SCP is perfect for quick, simple transfers, rsync provides the power and flexibility needed for maintaining backups and keeping systems synchronized."
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-introduction",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-introduction",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "",
    "text": "This guide provides step-by-step instructions and explanations for configuring a Raspberry Pi 4 to learn about hardware, servers, containerization, and self-hosting principles. To be clear, this guide is not-exhaustive and I‚Äôm sure there were areas I made mistakes or misunderstood a topic. I‚Äôm inviting (encouraging) you to let me know! You can submit an issue via GitHub on the guide on my website. That being said, the primary purpose of this guide is so that I can go back and reference what I previously did, as well as understand the thought process, when I need to troubleshoot or recreate something. The secondary purpose is to provide a helpful resource for others in a similar situation to me, because I struggled to find the sort of comprehensive document I‚Äôm aiming to create.\nBack to this guide, eventually I‚Äôd like to setup an actual server cluster and self-host some interesting, more resource-intensive applications. Before I make that kind of commitment, I wanted to learn the basics and see if this is something I would enjoy‚Äì the good news, I learned I do. The great news, Raspberry Pi makes their hardware very affordable and easy to purchase. Here‚Äôs the official webpage for the exact computer I bought.\nIt‚Äôs worth adding, I bought the 8GB Raspberry Pi 4. The price difference isn‚Äôt that great, but the performance is, between the lesser 2GB and 4GB models. Additionally, because I‚Äôm planning to host and experiment with CI/CD, I also bought a case and cooling fan to help with longevity. All in, the base price for that (with the power supply and HDMI cable) is $107.30 before taxes, shipping, and other fees.\nFinally, you‚Äôll see an outline below, you can gloss over it to get a general idea of what we‚Äôll be doing and in what order. At the start of each section I‚Äôll include a key terms list that has all of the fundamental terms which are important for a given topic.\n\n\n\nIntroduction\n\nPurpose and scope of the guide\nWhat you‚Äôll learn and build\nPrerequisites and assumptions\n\nInitial Setup\n\nHardware Requirements\n\nRaspberry Pi 4 specifications\nStorage devices (Thumbdrive, SSD, microSD cards)\nAccessories and peripherals (Keyboard, monitor, etc.)\n\nImage Requirements\n\nSelecting and downloading Ubuntu Server LTS\nUsing Raspberry Pi Imager\nInitial configuration options\n\nGet Started\n\nThe physical setup of the Raspberry Pi, what to plug in, where, etc.\nWhat to expect as things turn on\n\n\nLinux Server Basics\n\nFirst Boot Process\n\nConnection and startup\nUnderstanding initialization\n\nService Management with systemd\n\nUnderstanding systemd units and targets\nBasic service commands\n\nUnderstanding Your Home Directory\n\nShell configuration files\nHidden application directories\n\nThe Root Filesystem\n\nFilesystem Hierarchy Standard (FHS)\nKey directories and their purposes\n\nUser and Group Permissions\n\nBasic permission concepts\nchmod and chown usage\nUnderstanding advanced permissions\n\n\nNetworking Basics\n\nComputer Networking Fundamentals\n\nOSI and TCP/IP models\nKey networking protocols\n\nNetwork Connections\n\nWired vs wireless configurations\nUnderstanding IP addressing\n\nUbuntu Server Networking Tools\n\nTesting connectivity\nViewing network statistics\n\nsystemd-networkd\n\nConfiguration file structure\nWired and wireless setup\n\nConverting Netplan to networkd\n\nWhy and how to transition\nTroubleshooting network issues\n\nAdvanced Networking\n\nSubnets and routing\nSecurity considerations\n\n\nSSH (Secure Shell)\n\nUnderstanding SSH Configuration\n\nClient vs server setup\nKey-based authentication\n\nKey-Based Authentication\n\nTypes of SSH key encryption\nGenerating keys\nInstalling the public key\n\nServer-Side SSH Configuration\n\nHost keys and security options\nOptimizing for security\n\nClient-Side Configuration\n\nSetting up SSH config\nManaging known hosts\n\nAdditional Security Measures\n\nFirewall configuration with UFW\nIntrusion prevention with Fail2Ban\n\nSecure File Transfers\n\nUsing SCP (Secure Copy Protocol)\nUsing rsync for efficient transfers\n\n\nPartitions\n\nPartitioning Basics\n\nUnderstanding partition tables and types\nFilesystem options and considerations\n\nPartitioning Tools\n\nUsing parted and other utilities\n\nPartitioning for Backups\n\nSetting up microSD cards\nMount points and fstab configuration\n\nPartitioning your SSD\n\nBoot and root partitions\nFormatting and preparation\n\nAdvanced Partitioning\n\nMonitoring usage\nResizing partitions\n\n\nBackups and Basic Automation\n\nSetting Up the Backup Directory\n\nDirectory structure and permissions\n\nConfiguration Files Backup\n\nUsing rsync for system configurations\nRemote transfers of backups\n\nRestoring from Backup\n\nCreating restoration scripts\nTesting recovery procedures\n\n\nChanging Your Boot Media Device\n\nBoot Configuration Transition\n\nFlashing OS to new media\nProper shutdown procedures\nPhysically changing boot devices\nTesting the new boot configuration\nRestoring configurations\n\n\nRemote Development with VS Code\n\nSetting Up VS Code Remote SSH\nManaging Remote Projects\nDebugging and Terminal Integration\n\nMonitoring and Maintenance\n\nSystem Monitoring Tools\nLog Management\nPerformance Optimization\nSecurity Updates and Patching\n\nContainerization with Docker\n\nContainerization and Virtualization Basics\nDocker Installation and Setup\nCreating Images with Dockerfile\nManaging Images and Containers\nDocker Compose for Multi-container Applications\nCI/CD Integration\n\nContainer Orchestration with Kubernetes\n\nOrchestration Basics\nKubernetes Concepts, Installation, and Setup\nSetting Up a Kubernetes Cluster\nDeployment Strategies\nManaging Resources\nScaling Applications"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Starting from (close) to Zero",
    "text": "Starting from (close) to Zero\n\nHardware Requirements\nThis section will provide basic setup instructions, so you‚Äôll have the same tools I do and can follow along with this guide, step-by-step.\n\nRaspberry Pi 4 8GB\nMicro HDMI to HDMI cord (for direct access)\nAppropriate Power Supply\nKeyboard (connected via USB for direct access)\n1TB Samsung T7 SSD (connected via USB for boot media/core memory)\nAmazon Basics 128GB microSD card (or other microSD cards for backups media)\nSSH-capable devices for headless access\n\nI‚Äôm using a MacBook Air\nI prefer the MacOS and Terminal for personal development, because I use Windows at my day job\n\n\n\n\nImage requirements\nOnce you have your hardware ready to go, you can being setting up the software. I‚Äôm using Linux Server LTS because it‚Äôs a stable version of Linux intended for headless, server environments. LTS means long-term support, so unlike the more frequently updated versions, these OS versions are supported for 5 years. Additionally, you‚Äôll want to use public-key authentication for better security purposes, but more on that in the SSH Section.\n\nHave your SSD ready and able to connect to your laptop or desktop (whichever you plan to use with SSH)\nDownload Raspberry Pi Imager from the official website\nRun the Imager and configure your installation of the most recent Linux Server LTS image\n\nSelect your Raspberry Pi device\n\n\n\n\nUbuntu-Pi Device\n\n\n\nSelect the OS Image you want to flash\n\n\n\n\nUbuntu-Pi OS\n\n\n\nSelect the media storage device for the image\n\n\n\n\nUbuntu-Pi Storage\n\n\n\nConfigure settings\n\n Here, you‚Äôll configure your primary user ID and password; network connection; locale and timezone; and your hostname (the nickname your computer remembers the IP address as).\n Here, you‚Äôll configure your SSH settings. Remember to use public-key authentication only, and to keep the private key generated on your local machine safe.\n These are more preference based, but it‚Äôs nice to have the storage device automatically eject once the flashing is complete. Then, you just need to unplug it and plug it into your Raspberry Pi to get going."
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-requirements",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-requirements",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Starting from (close) to Zero",
    "text": "Starting from (close) to Zero\n\nHardware Requirements\nThis section will provide basic setup instructions, so you‚Äôll have the same tools I do and can follow along with this guide, step-by-step.\n\nRaspberry Pi 4 8GB\n\nMicro HDMI to HDMI cord (for direct access)\nProtective case\nCooling fan\nAppropriate Power Supply\n\nKeyboard (connected via USB for direct access)\nMonitor (for direct access)\n1TB Samsung T7 SSD (connected via USB for boot media/core memory)\n64GB Generic Flash Drive (used as the boot media when partitioning the SSD)\nAmazon Basics 128GB microSD card (or other microSD cards for backups media)\nSSH-capable devices for headless access\n\nI‚Äôm using a MacBook Air\nI prefer the MacOS and Terminal for personal development, because I use Windows at my day job\n\n\n\n\nImage requirements\nOnce you have your hardware ready to go, you can being setting up the software. I‚Äôm using Linux Server LTS because it‚Äôs a stable version of Linux intended for headless, server environments. LTS means long-term support, so unlike the more frequently updated versions, these OS versions are supported for 5 years. Additionally, you‚Äôll want to use public-key authentication for better security purposes, but more on that in the SSH Section.\n\nHave your Thumb Drive ready and able to connect to either a laptop or desktop (whichever you plan to use with SSH)\nDownload Raspberry Pi Imager from the official website\nRun the Imager and configure your installation of the most recent Linux Server LTS image\n\nSelect your Raspberry Pi device\n\n\n\nSelect the OS Image you want to flash\n\n\n\nSelect the media storage device for the image\n\n\n\nConfigure settings\n\n Here, you‚Äôll configure your primary user ID and password; network connection; locale and timezone; and your hostname (the nickname your computer remembers the IP address as).\n Here, you‚Äôll configure your SSH settings. You should probably use public-key authentication only when dealing with SSH in your leave, but for learning purposes, you don‚Äôt need to at this time. Later on in this guide, I‚Äôll walk you through the steps to manually configure SSH, if you are unfamiliar.\n These are more preference based, but it‚Äôs nice to have the storage device automatically eject once the flashing is complete. Then, you just need to unplug it and plug it into your Raspberry Pi to get going.\n\n\n\nGet Started\nIt‚Äôs time to get the actual Raspberry Pi device setup. For most of this guide, I recommend leaving the Pi outside of the case, because it‚Äôll be easier to plug and unplug some of the devices‚Äì the microSD slot is not accessible while the case is on. Later, once we‚Äôve got everything configured and setup as we like, we will attach the fan and case, so it‚Äôs a bit safer and able to run in an always-on state. I‚Äôll share a picture of what my server looks like during the early development, and then later I‚Äôll show what it looks like with everything connected and setup.\n\nNow you‚Äôre ready to plug your boot media device (the Thumb Drive) into your Raspberry Pi. You should also connect a keyboard, monitor, and power supply. Once all of this is connected, your Raspberry Pi will boot up. Connecting a monitor and keyboard will allow you to directly interact with the system‚Äôs terminal. Ideally, you‚Äôll use SSH, but it may be helpful to have direct access in case there are any network issues. Eventually, the SSD will serve as the boot media and primary storage device for the server; however, we can‚Äôt modify its partitions while it‚Äôs serving as the boot device. So, we‚Äôll use a thumb drive as the boot media device, until we complete the partitions.\nWhen first connecting from the wired keyboard and monitor, let all of the start up processes finish running (these will hopefully have brackets with the word Success in green). Then, type in the name of the User ID you wish to login with, in my case it‚Äôs chris. Then, enter the password (no characters will show up as you type it in) and hit enter. You‚Äôll see a plaintext message telling you the OS version, some system information (memory usage, temperature, etc.), and you‚Äôll see a line where you can enter commands (the CLI). In my case, it looks like this: chris@ubuntu-pi-server:~$\nNow you can run some basic commands to see where you are and what you have available to you. Spoiler alert, you‚Äôre in your home directory and have no files. In my case it‚Äôs /home/chris, where the /home directory is owned by root and /chris is owned by my user‚Äì UID 1000 (the default for new users on a fresh system/image). Right now your directory will be empty, outside of some hidden folders like .ssh. More on this later.\nNext we‚Äôll cover what happened during the boot process, the basic structure of the Linux Server OS, and some important information related to permissions, before we move on to basic networking concepts and configurations."
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#linux",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#linux",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Linux",
    "text": "Linux"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#linux-server-basics",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#linux-server-basics",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Linux Server Basics",
    "text": "Linux Server Basics\n\nFirst Boot Process\nWhen you first boot a fresh Ubuntu Server LTS image on your Raspberry Pi, several important initialization processes occur that don‚Äôt happen during subsequent boots. The first boot of your Ubuntu Server LTS on the Raspberry Pi is fundamentally different from subsequent boots because it performs one-time initialization tasks. While later boots will simply load the configured system, this first boot sets up critical system components.\n\nHardware Detection: The system performs comprehensive hardware detection to identify and configure your Raspberry Pi‚Äôs components.\nInitial RAM Disk (initrd): The bootloader loads a temporary filesystem into memory that contains essential drivers and modules needed to mount the real root filesystem.\nFilesystem Check and Expansion: On first boot, the system checks the integrity of the filesystem and often expands it to utilize the full available space on your Flash Drive.\nCloud-Init Processing: Ubuntu Server uses cloud-init to perform first-boot configuration tasks (the processes you see running on the monitor on startup)\n\nSetting the hostname\nGenerating SSH host keys\nCreating the default user account\nRunning package updates\n\nMachine ID Generation: A unique machine ID is generated and stored in /etc/machine-id.\nNetwork Configuration: The system attempts initial network setup based on detected hardware.\n\nThe key difference is that subsequent boots skip these initialization steps since they‚Äôve already been completed, making them significantly faster.\n\n\nService Management with systemd\nSystemd is the modern initialization and service management system for Linux. It‚Äôs responsible for bootstrapping the user space and managing all processes afterward. Key components of systemd include:\n\nUnits: Everything systemd manages is represented as a ‚Äúunit‚Äù with a corresponding configuration file. Units include:\n\nService units (.service): Define how to start, stop, and manage daemons (background processes that are always on)\nSocket units (.socket): Manage network/IPC sockets\nTimer units (.timer): Trigger other units based on timers\nMount units (.mount): Control filesystem mount points\n\nTarget units: Represent system states (similar to runlevels in older systems)\n\nmulti-user.target: Traditional text-mode system\ngraphical.target: Graphical user interface\nnetwork.target: Network services are ready\n\n\nFor example, let‚Äôs take a look at a generic SSH service file.\n[Unit]\nDescription=OpenSSH server daemon\nDocumentation=man:sshd(8) man:sshd_config(5)\nAfter=network.target auditd.service\nWants=network.target\n\n[Service]\nEnvironmentFile=-/etc/default/ssh\nExecStartPre=/usr/sbin/sshd -t\nExecStart=/usr/sbin/sshd -D $SSHD_OPTS\nExecReload=/usr/sbin/sshd -t\nExecReload=/bin/kill -HUP $MAINPID\nKillMode=process\nRestart=on-failure\nRestartPreventExitStatus=255\nType=notify\n\n[Install]\nWantedBy=multi-user.target\nTo break this down:\n\n[Unit]: Metadata and dependencies\n\nDescription: Human-readable service description\nDocumentation: Where to find documentation\nAfter: Units that should be started before this one\nWants: Soft dependencies\n\n[Service]: Runtime behavior\n\nExecStart: Command to start the service\nExecReload: Command to reload the service\nRestart: When to restart the service\nType: How systemd should consider the service started\n\n[Install]: Installation information\n\nWantedBy: Target that should include this service\n\n\nUbuntu Server‚Äôs current standard is systemd, but previously it was SysV. A few key improvements include:\n\nParallel Service Startup: Systemd can start services in parallel, improving boot times.\nDependency Management: Systemd handles service dependencies more effectively.\nService Supervision: Systemd continuously monitors and can automatically restart services.\nSocket Activation: Services can be started on-demand when a connection request arrives.\n\nManaging services is easy using the command line, a crucial component of headless applications, a few examples are:\n\nView service status: systemctl status ssh\nStart a service: sudo systemctl start ssh\nStop a service: sudo systemctl stop ssh\nEnable at boot: sudo systemctl enable ssh\nDisable at boot: sudo systemctl disable ssh\nView logs: journalctl -u ssh\n\n\n\nUnderstanding Your Home Directory\nNow that you‚Äôve logged in and can work on your server, you may wonder where you are and what‚Äôs there. Running pwd will return the file path of your current location. Running ls -a will show you all available files and directories in your current location. Running these, you‚Äôll see a few things specifically for Shell configuration (your terminal/CLI):\n\n.bash_history: Contains a record of commands you‚Äôve executed in the bash shell. This helps with command recall using the up arrow or history command.\n.bash_logout: Executed when you log out of a bash shell. Often used for cleanup tasks like clearing the screen.\n.bashrc: The primary bash configuration file that‚Äôs loaded for interactive non-login shells. It defines aliases, functions, and shell behavior. When you open a terminal window, this file is read.\n.profile: Executed for login shells. It typically sets environment variables and executes commands that should run once per login session, not for each new terminal.\n\n# Sample .bashrc section\n# enable color support of ls and also add handy aliases\nif [ -x /usr/bin/dircolors ]; then\n    test -r ~/.dircolors && eval \"$(dircolors -b ~/.dircolors)\" || eval \"$(dircolors -b)\"\n    alias ls='ls --color=auto'\n    alias grep='grep --color=auto'\n    alias fgrep='fgrep --color=auto'\n    alias egrep='egrep --color=auto'\nfi\n\n# some more ls aliases\nalias ll='ls -alF'\nalias la='ls -A'\nalias l='ls -CF'\nBeyond those, you‚Äôll also find hidden application directories.\n\n.cache: Contains non-essential data that can be regenerated as needed. Applications store temporary files here to improve performance on subsequent runs.\n.dotnet: Contains .NET Core SDK and runtime files if you‚Äôve installed the .NET development platform.\n.ssh: Stores SSH configuration files and keys:\n\nauthorized_keys: Lists public keys that can authenticate to your account\nubuntu_pi_ecdsa & ubuntu_pi_ecdsa.pub: Your private and public ECDSA keys (More on this in the SSH Section)\nknown_hosts: Tracks hosts you‚Äôve connected to previously\nssh_config: Optional configuration file for SSH connections\n\n.sudo_as_admin_successful: A marker file created when you successfully use sudo. Its presence suppresses the ‚Äúsudo capabilities‚Äù message when opening a terminal.\n.vscode-server: Created when you connect to your server using Visual Studio Code‚Äôs remote development feature. Contains the VS Code server components. (More on this in the SSH Section)\n.wget-hsts: Wget‚Äôs HTTP Strict Transport Security database. Tracks websites that require secure (HTTPS) connections.\n\n\n\nThe Root Filesystem\nThe Linux filesystem follows the Filesystem Hierarchy Standard (FHS), which defines the directory structure and contents of Unix-like systems. Here‚Äôs a breakdown of key directories:\n\n/bin: Contains essential command binaries (programs) needed for basic system functionality. These commands are available to all users and are required during boot or in single-user mode.\n\nHistorical note: Originally separated from /usr/bin because early Unix systems had limited disk space on the root partition.\n\n/boot: Contains boot loader files including the Linux kernel, initial RAM disk (initrd), and bootloader configuration (GRUB).\n\nFor Raspberry Pi, this contains the firmware and various boot-related files.\n\n/dev: Contains device files that represent hardware devices. These are not actual files but interfaces to device drivers in the kernel.\n\nExample: /dev/sda represents the first SATA disk.\n\n/etc: Contains system-wide configuration files. The name originated from ‚Äúet cetera‚Äù but is now often interpreted as ‚ÄúEditable Text Configuration.‚Äù Critical files include:\n\n/etc/fstab: Filesystem mount configuration\n/etc/passwd: User account information\n/etc/ssh/sshd_config: SSH server configuration\n\n/home: Contains user home directories where personal files and user-specific configuration files are stored.\n/lib: Contains essential shared libraries needed by programs in /bin and system boot.\n\nOn modern 64-bit systems, you‚Äôll also find /lib64 for 64-bit libraries.\n\n/media: Mount point for removable media like USB drives and DVDs.\n/mnt: Temporarily mounted filesystems. This is often used as a manual mount point.\n/opt: Optional application software packages. Used for third-party applications that don‚Äôt follow the standard file system layout.\n/proc: A virtual filesystem providing process and kernel information. Files here don‚Äôt exist on disk but represent system state.\n\nExample: /proc/cpuinfo shows CPU information.\n\n/root: Home directory for the root user. Separated from /home to ensure it‚Äôs available even if /home is on a separate partition.\n/run: Runtime data for processes started since last boot. This is a tmpfs (memory-based) filesystem.\n/sbin: System binaries for system administration tasks, typically only usable by the root user.\n/srv: Data for services provided by the system, such as web or FTP servers.\n/snap: The /snap directory is, by default, where the files and folders from installed snap packages appear on your system.\n/sys: Another virtual filesystem exposing device and driver information from the kernel. Provides a more structured view than /proc.\n/tmp: Temporary files that may be cleared on reboot. Applications should not rely on data here persisting.\n/usr: Contains the majority of user utilities and applications. Originally stood for ‚ÄúUnix System Resources.‚Äù\n\n/usr/bin: User commands\n/usr/lib: Libraries for the commands in /usr/bin\n/usr/local: Locally installed software\n/usr/share: Architecture-independent data\n\n/var: Variable data files that change during normal operation:\n\n/var/log: System log files\n/var/mail: Mail spool\n/var/cache: Application cache data\n/var/spool: Spool for tasks waiting to be processed (print queues, outgoing mail)\n\n\nThe core philosophy behind this structure separates:\n\nStatic vs.¬†variable content\nShareable vs.¬†non-shareable files\nEssential vs.¬†non-essential components\n\nUnderstanding this hierarchy helps you navigate any Linux system and locate important files intuitively.\n\n\nUser and Group Permissions\n\nBasics\nLinux inherits its permission system from Unix, providing a robust framework for controlling access to files and directories. Understanding this system is essential for maintaining security and proper functionality of your Raspberry Pi server, as well as any Linux based system. At its core, the Linux permission model operates with three basic permission types applied to three different categories of users:\n\nPermission Types:\n\nRead (r): Allows viewing file contents or listing directory contents\nWrite (w): Allows modifying file contents or creating/deleting files within a directory\nExecute (x): Allows running a file as a program or accessing files within a directory\n\nUser Categories:\n\nOwner (u): The user who owns the file or directory\nGroup (g): Users who belong to the file‚Äôs assigned group\nOthers (o): All other users on the system\n\n\nIt‚Äôs not only important to know how to set permissions, but also how to view existing ones. When you run ls -l in a directory, you‚Äôll see a detailed listing including permission information.\n-rw-r--r-- 1 chris chris 1234 May 6 14:32 example.txt\nIn this example, the owner can read and write, while group members and others can only read. The first string of characters -rw-r‚Äìr‚Äì represents the permissions:\n\nFirst character: File type (- for regular file, d for directory, l for symbolic link)\nCharacters 2-4: Owner permissions (rw-)\nCharacters 5-7: Group permissions (r‚Äì)\nCharacters 8-10: Others permissions (r‚Äì)\n\n\n\nchmod\nThe chmod command modifies file permissions in Linux. You can use it in two ways: symbolic mode or numeric (octal) mode.\nSymbolic mode uses letters to represent permission categories (u, g, o, a) and permissions (r, w, x):\n# Give the owner execute permission\nchmod u+x script.sh\n\n# Remove write permission from group and others\nchmod go-w important_file.txt\n\n# Set read and execute for everyone (a=all users)\nchmod a=rx application\n\n# Add write permission for owner and group\nchmod ug+w shared_document.txt\nEach symbol has a specific meaning:\n\nu: Owner permissions\ng: Group permissions\no: Other user permissions\na: All permissions\n+: Add permissions\n-: Remove permissions\n=: Set exact permissions\n\nOctal mode represents permissions as a 3-digit number, where each digit represents the permissions for owner, group, and others:\n\nRead (r) = 4\nWrite (w) = 2\nExecute (x) = 1\n\nPermissions are calculated by adding these values:\n\n7 (4+2+1) = Read, write, and execute\n6 (4+2) = Read and write\n5 (4+1) = Read and execute\n4 (4) = Read only\n0 = No permissions\n\n# rwxr-xr-x (755): Owner can read, write, execute; group and others can read and execute\nchmod 755 script.sh\n\n# rw-r--r-- (644): Owner can read and write; group and others can read only\nchmod 644 document.txt\n\n# rwx------ (700): Owner has all permissions; group and others have none\nchmod 700 private_directory\nBeyond the basic rwx permissions, Linux has three special permission bits:\n\nsetuid (4000): When set on an executable file, it runs with the privileges of the file owner instead of the user executing it.\nsetgid (2000): Similar to setuid but for group permissions. When set on a directory, new files created within inherit the directory‚Äôs group.\nsticky bit (1000): When set on a directory, files can only be deleted by their owner, the directory owner, or root (commonly used for /tmp).\n\n\n\nchown\nThe chown command changes the owner and/or group of files and directories. Do not change ownership in the root directories because many require specific ownership/permissions to function properly.\n# Change the owner of a file\nsudo chown chris file.txt\n\n# Change both owner and group\nsudo chown chris:developers project_files\n\n# Change only the group\nsudo chown :developers shared_documents\n\n# Change recursively for a directory and all its contents\nsudo chown -R chris:chris /home/chris/projects\nThe flags do the following:\n\n-R, --recursive: Change ownership recursively\n-c, --changes: Report only when a change is made\n-f, --silent: Suppress most error messages\n-v, --verbose: Output a diagnostic for every file processed\n\n# Verbose recursive ownership change\nsudo chown -Rv chris:developers /opt/application\n\n\nUnderstanding Permissions\nLinux manages permissions through users and groups:\n\nEach user has a unique User ID (UID)\nEach group has a unique Group ID (GID)\nUsers can belong to multiple groups\nThe first 1000 UIDs/GIDs are typically reserved for system users/groups\n\nImportant files include:\n\n/etc/passwd: Contains basic user account information\n\nFields: username, password placeholder, UID, primary GID, full name, home directory, login shell\n\n\nchris:x:1000:1000:Chris Kornaros:/home/chris:/bin/bash\n\n/etc/shadow: Contains encrypted passwords and password policy information\n\nFields: username, encrypted password, days since epoch of last change, min days between changes, max days password valid, warning days, inactive days, expiration date\n\n\nchris:$6$xyz...hash:19000:0:99999:7:::\n\n/etc/group: Contains group definitions\n\nFields: group name, password placeholder, GID, comma-separated list of members\n\n\ndevelopers:x:1001:chris,bob,alice\nThere are two categories of groups you should understand, Primary and Supplementary:\n\nPrimary Group: Set in /etc/passwd, used as the default group for new files\nSupplementary Groups: Additional groups a user belongs to, defined in /etc/group\n\nYou can view your current user‚Äôs groups with the groups command, or view them for a specific user with groups chris (replace chris with the name of the user). That being said, directory permissions differ slightly from file permissions:\n\nRead (r): List directory contents\nWrite (w): Create, delete, or rename files within the directory\nExecute (x): Access files within the directory (crucial for navigation)\n\n\n\n\n\n\n\nTip\n\n\n\nA common confusion: You may have read permission for a file but not execute permission for its parent directory, preventing access.\n\n\nThe umask (user mask) determines the default permissions for newly created files and directories:\n\nDefault for files: 666 (rw-rw-rw-)\nDefault for directories: 777 (rwxrwxrwx)\nThe umask is subtracted from these defaults, for example, a umask of 022 results in:\n\nFiles: 644 (rw-r‚Äìr‚Äì)\nDirectories: 755 (rwxr-xr-x)\n\n\n# View current umask (in octal)\numask\n\n# Set a new umask\numask 027  # More restrictive: owner full access, group read/execute, others no access\nTraditional Unix permissions have limitations regarding inheritance: new files don‚Äôt inherit permissions from parent directories and changing permissions doesn‚Äôt affect existing files. Modern solutions, however, include: the setgid bit on directories for group inheritance and ACLs (Access Control Lists) with default entries that apply to new files. To setup a collaborative directory with proper permissions:\n# Create a shared directory for developers\nsudo mkdir /opt/projects\nsudo chown chris:developers /opt/projects\nsudo chmod 2775 /opt/projects  # setgid bit ensures new files get 'developers' group\n\n\nAdvanced Permission Concepts\nLike I previously wrote, part of the modern permission solutions include ACLs, or Access Control Lists. ACLs extend the traditional permission model to allow specifying permissions for multiple users and groups. When ACLs are in use, ls -l will show a + after the permission bits. Here‚Äôs a basic example of how to create and manage an ACL:\n# Install ACL support (if not already installed)\nsudo apt install acl\n\n# Set an ACL allowing a specific user read access\nsetfacl -m u:chris:r file.txt\n\n# Set an ACL allowing a specific group write access\nsetfacl -m g:developers:rw file.txt\n\n# Set default ACLs on a directory (inherited by new files)\nsetfacl -d -m g:developers:rw directory/\n\n# View ACLs on a file\ngetfacl file.txt\n-rw-rw-r--+ 1 chris developers 1234 May 6 14:32 file.txt\nA few final notes on permissions that are especially relevant for this project, becaue you‚Äôll be working with external storage devices:\n\nNot all filesystems support the same permission features:\n\next4: Full support for traditional permissions, ACLs, and extended attributes\nNTFS (via NTFS-3G): Simulated Unix permissions, basic ACL support\nFAT32: No native permission support (mounted with fixed permissions)\nexFAT: No native permission support\n\nCommon Permission Patterns:\n\nConfiguration Files: 644 or 640 (owner can edit, restricted read access)\nProgram Binaries: 755 (everyone can execute, only owner can modify)\nWeb Content: 644 for regular files, 755 for directories\nSSH Keys: 600 for private keys (owner only), 644 for public keys\nScripts: 700 or 750 (executable by owner or group)"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#partitioning-your-memory",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#partitioning-your-memory",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Partitioning Your Memory",
    "text": "Partitioning Your Memory\n\nPurpose of MicroSD Cards\n\nExperiment with other OS installations (e.g., NetBSD).\nUse one card as a backup Linux bootloader.\nAllocate one card for portable environments or additional storage.\n\n\n\nSteps to Use MicroSD Cards\n\nFormat the Cards:\n\nUse gparted on Linux or similar tools to format the cards.\nChoose FAT32 for compatibility or ext4 for Linux systems.\n\nInstall Operating Systems:\n\nDownload the desired OS images (e.g., NetBSD).\nFlash the image to the card using balenaEtcher or Raspberry Pi Imager.\n\nSwitching OS:\n\nInsert the appropriate microSD card and reboot the Raspberry Pi."
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#comprehensive-linux-configuration-backup-guide-for-raspberry-pi-server",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#comprehensive-linux-configuration-backup-guide-for-raspberry-pi-server",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Comprehensive Linux Configuration Backup Guide for Raspberry Pi Server",
    "text": "Comprehensive Linux Configuration Backup Guide for Raspberry Pi Server\nThis guide explains how to create a complete backup of Linux configurations and system files on a Raspberry Pi Server running Ubuntu Server LTS using rsync. We‚Äôll use rsync because it provides several important advantages over simple copy commands:\n\nIncremental backups that only transfer changed files\nPreservation of file permissions, ownership, and timestamps\nBuilt-in compression for efficient transfers\nDetailed progress information and logging\nThe ability to resume interrupted transfers\n\n\nPrerequisites\n\nRaspberry Pi Server running Ubuntu Server LTS\nPhysical keyboard access\nRoot or sudo privileges\nMounted backup drive at /mnt/backups/\nrsync (typically pre-installed on Ubuntu Server)\n\n\n\nSetting Up the Backup Directory\nFirst, we‚Äôll prepare the backup directory structure and set appropriate permissions:\n# Create backup directories if they don't exist\nsudo mkdir -p /mnt/backups/configs\nsudo mkdir -p /mnt/backups/system\n\n# Change ownership to your user (replace 'chris' with your username)\nsudo chown -R chris:chris /mnt/backups\n\n# Set appropriate permissions\nsudo chmod -R 700 /mnt/backups  # Only owner can read/write/execute\n\n\nConfiguration Files Backup\nWe‚Äôll use rsync to create a structured backup of essential configuration files. The following script demonstrates how to perform the backup while preserving all file attributes:\n#!/bin/bash\n# Using the {} around DATEYMD in the file path ensure it's specified as the variable's value, and the subsequent parts are not included\n\nDATEYMD=$(date +%Y%m%d)\nBACKUP_DIR=\"/mnt/backups/configs/$DATEYMD\"\nLOG_DIR=\"/mnt/backups/logs\"\nLOG_FILE=\"$LOG_DIR/${DATEYMD}_config_backup.log\"\n\n# Create backup directory\nmkdir -p \"$BACKUP_DIR\"\n\n{\n    # 1. User and Group Information\n    sudo rsync -aAXv /etc/passwd \"$BACKUP_DIR/passwd.bak\"\n    sudo rsync -aAXv /etc/group \"$BACKUP_DIR/group.bak\"\n    sudo rsync -aAXv /etc/shadow \"$BACKUP_DIR/shadow.bak\"\n    sudo rsync -aAXv /etc/gshadow \"$BACKUP_DIR/gshadow.bak\"\n\n    # 2. Crontab Configurations\n    sudo rsync -aAXv /etc/crontab \"$BACKUP_DIR/\"\n    sudo rsync -aAXv /var/spool/cron/crontabs/. \"$BACKUP_DIR/crontabs/\"\n\n    # 3. SSH Configuration\n    sudo rsync -aAXv /etc/ssh/. \"$BACKUP_DIR/ssh/\"\n    sudo rsync -aAXv ~/.ssh/. \"$BACKUP_DIR/user_ssh/\"\n\n    # 4. UFW (Uncomplicated Firewall) Configuration\n    sudo rsync -aAXv /etc/ufw/. \"$BACKUP_DIR/ufw/\"\n    sudo ufw status verbose &gt; \"$BACKUP_DIR/ufw_rules.txt\"\n\n    # 5. Fail2Ban Configuration\n    sudo rsync -aAXv /etc/fail2ban/. \"$BACKUP_DIR/fail2ban/\"\n\n    # 6. Network Configuration\n    sudo rsync -aAXv /etc/network/. \"$BACKUP_DIR/network/\"\n    sudo rsync -aAXv /etc/netplan/. \"$BACKUP_DIR/netplan/\"\n    sudo rsync -aAXv /etc/NetworkManager/. \"$BACKUP_DIR/NetworkManager/\"\n    sudo rsync -aAXv /etc/hosts \"$BACKUP_DIR/hosts.bak\"\n    sudo rsync -aAXv /etc/hostname \"$BACKUP_DIR/hostname.bak\"\n    sudo rsync -aAXv /etc/resolv.conf \"$BACKUP_DIR/resolv.conf.bak\"\n    sudo rsync -aAXv /etc/wpa_supplicant/wpa_supplicant.conf \"$BACKUP_DIR/wpa_supplicant.conf.bak\"\n\n    # 7. Package Manager Configurations (apt)\n    sudo rsync -aAXv /etc/apt/. \"$BACKUP_DIR/apt/\"\n\n    # 8. Systemd Services and Timers\n    sudo rsync -aAXv /etc/systemd/system/. \"$BACKUP_DIR/systemd/\"\n\n    # 9. Logrotate Configuration\n    sudo rsync -aAXv /etc/logrotate.conf \"$BACKUP_DIR/logrotate.conf.bak\"\n    sudo rsync -aAXv /etc/logrotate.d/. \"$BACKUP_DIR/logrotate.d/\"\n\n    # 10. Timezone and Locale\n    sudo rsync -aAXv /etc/timezone \"$BACKUP_DIR/timezone.bak\"\n    sudo rsync -aAXv /etc/localtime \"$BACKUP_DIR/localtime.bak\"\n    sudo rsync -aAXv /etc/default/locale \"$BACKUP_DIR/locale.bak\"\n\n    # 11. Keyboard Configuration\n    sudo rsync -aAXv /etc/default/keyboard \"$BACKUP_DIR/keyboard.bak\"\n\n    # 12. Package List\n    dpkg --get-selections &gt; \"$BACKUP_DIR/package_list.txt\"\n\n    # Set appropriate permissions\n    sudo chown -R chris:chris \"$BACKUP_DIR\"\n    sudo chmod -R 600 \"$BACKUP_DIR\"\n\n    echo \"Configuration backup completed at: $BACKUP_DIR\"\n\n} &gt; \"$LOG_FILE\" 2&gt;&1\n\necho \"Logs available at: $LOG_FILE\"\n# Make the script executable\nchmod +x /scripts/config_backup.sh\n\n\nSystem Files Backup\nFor system files, we‚Äôll create a separate rsync script that handles system directories efficiently:\n#!/bin/bash\n\nDATEYMD=$(date +%Y%m%d)\nBACKUP_DIR=\"/mnt/backups/system/$DATEYMD\"\nLOG_DIR=\"/mnt/backups/logs\"\nLOG_FILE=\"$LOG_DIR/${DATEYMD}_system_backup.log\"\n\n\n\n# Create backup directory\nmkdir -p \"$BACKUP_DIR\"\n\n{\n    # Starting script\n    echo \"Starting system backup at: $(date)\"\n    echo \"Backup directory: $BACKUP_DIR\"\n\n    # The --one-file-system option prevents crossing filesystem boundaries\n    # --hard-links preserves hard links\n    # --acls and --xattrs preserve extended attributes\n    sudo rsync -aAXv --one-file-system --hard-links \\\n        --exclude=\"/mnt/\" \\\n        / \"$BACKUP_DIR\"\n\n    # 2. System Information Files\n    # Partition layout\n    sudo fdisk -l &gt; \"$BACKUP_DIR/partition_layout.txt\"\n    # Disk UUIDs\n    sudo blkid &gt; \"$BACKUP_DIR/disk_uuids.txt\"\n\n    # Set appropriate permissions\n    sudo chown -R chris:chris \"$BACKUP_DIR\"\n    sudo chmod -R 600 \"$BACKUP_DIR\"\n\n    echo \"System backup completed at: $BACKUP_DIR.\"\n\n} &gt; \"$LOG_FILE\" 2&gt;&1\n\necho \"Logs available at: $LOG_FILE\"\n# Make the script executable\nchmod +x /scripts/system_backup.sh\n\n\nUnderstanding the rsync Options\nThe rsync commands use several important options:\n\n-a: Archive mode, preserves almost everything\n-A: Preserve ACLs (Access Control Lists)\n-X: Preserve extended attributes\n-v: Verbose output\n--one-file-system: Don‚Äôt cross filesystem boundaries\n--hard-links: Preserve hard links\n--exclude: Skip specified directories\n\n\n\nStoring your backups externally\nWhile it‚Äôs definitely beneficial to have a local copy of your backups to easily roll back changes, it isn‚Äôt the most secure solution to have all of your information in one place. So, how can you go about transferring your system and configuration backups to another storage system?\nFor the purpose of this guide, I‚Äôll be showing you how to use rsync for a remote transfer and how to flash the backup onto a microSD.\n\nRsync for Remote Transfers\nRsync is specifically designed for copying and transferring files, so it offers more sophisticated file synchronization capabilities than basic tools like SCP.\nrsync -avz --partial --progress --update chris@ubuntu-pi-server:/mnt/backups/system/master/ ~/Documents/raspberry_pi_server/backups/system/master\nThe flags do the following:\n\n-a: Archive mode, which preserves permissions, timestamps, symbolic links, etc.\n-v: Verbose output, showing what files are being transferred\n-z: Compress data during transfer for faster transmission\n--partial: Keep partially transferred files, allowing you to resume interrupted transfers\n--progress: Show progress during transfer\n--update: Skip files that are newer on the receiver (only transfer if source is newer)\n\n\n\nHardware Transfers\n\n\n\nNote:\nEverything up until this point has been tested and works‚Äì relatively efficiently for such a simple setup. That being said, I still haven‚Äôt tested the restore script, nor have I tried to setup simple cron jobs to automate and cleanup the backups.\n\n\nRestoring from Backup\nTo restore your system from these backups:\n\n\nImportant Notes\n\nThe --delete option during restore will remove files at the destination that don‚Äôt exist in the backup. Use with caution.\nConsider using rsync‚Äôs --dry-run option to test backups and restores without making changes.\nThe backup includes sensitive system files. Store it securely and restrict access.\nConsider encrypting the backup directory for additional security.\nTest the restore process in a safe environment before using in production.\n\n\n\nAutomating the Backup\nCreate a master backup script that runs both configuration and system backups:\n# Create master backup script (save as master-backup.sh)\ncat &lt;&lt; 'EOF' &gt; /mnt/backups/master-backup.sh\n#!/bin/bash\n\n# Set up logging\nexec 1&gt; &gt;(logger -s -t $(basename $0)) 2&gt;&1\n\n# Run configuration backup\n/mnt/backup/backup-configs.sh\n\n# Run system backup\n/mnt/backup/backup-system.sh\n\n# Remove backups older than 30 days\nfind /mnt/backups/configs/ -type d -mtime +30 -exec rm -rf {} +\nfind /mnt/backups/system/ -type d -mtime +30 -exec rm -rf {} +\nEOF\n\n# Make the script executable\nchmod +x /mnt/backup/master-backup.sh\n\n# Add to crontab (run daily at 2 AM)\n(crontab -l 2&gt;/dev/null; echo \"0 2 * * * /mnt/backup/master-backup.sh\") | crontab -\n\n\nTroubleshooting\nIf you encounter issues:\n\nCheck rsync error messages with --verbose option\nVerify sufficient disk space with df -h\nMonitor backup progress with --progress option\nCheck system logs: sudo journalctl -u cron\nVerify file permissions and ownership\nTest network connectivity for remote backups\n\nRemember to regularly verify your backups by checking the log files and occasionally testing the restore process in a safe environment."
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#partitions",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#partitions",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Partitions",
    "text": "Partitions\n\nPartitioning Basics\nPartitions are logical divisions of a physical storage device. Think of a storage device like a large piece of land, and partitions as fenced areas within that land dedicated to different purposes. Each partition appears to the operating system as a separate disk, even though physically they‚Äôre on the same device. Remember from the beginning of this guide, I‚Äôm currently using a Flash Drive for my primary memory and a microSD card for backups; however, the SSD is what I want to serve as the boot device. Once we complete the partitioning, we can flash the base image from RPi onto the SSD and then reboot, with the SSD as the boot device.\n\nSeparation of concerns: Isolate the operating system from user data, which improves security and simplifies backups\nPerformance optimization: Different filesystems can be used for different workloads\nMulti-boot capability: Install multiple operating systems on the same physical device\nData protection: Limiting the scope of filesystem corruption to a single partition\nResource management: Setting size limits for specific system functions\n\nFor our Raspberry Pi server, proper partitioning creates a solid foundation for everything else you‚Äôll build. We‚Äôll primarily use ext4 for Linux partitions and FAT32 for the microSD card that needs broader compatibility.\n\n\n\n\n\n\n\n\nFilesystem\nBest For\nFeatures\n\n\n\n\next4\nLinux\n\nJournaling\nLarge file support\nBackwards compatible\n\n\n\nFAT32\nCross-platform compatibility\n\nWorks with virtually all operating systems\nLimited to 4GB Files\n\n\n\nexFAT\nModern cross-platform\n\nSupports large files\nNo built-in journaling\n\n\n\nNTFS\nWindows compatibility\n\nJournaling\nPermissions\nCompression\n\n\n\nBtrfs\nAdvanced Linux systems\n\nSnapshots\nChecksums\nCompression\n\n\n\n\nFinally, let‚Äôs cover some important terms:\n\nPartition Table: A data structure on a disk that describes how the disk is divided\n\nMBR (Master Boot Record): Traditional partition scheme limited to 2TB drives and 4 primary partitions\nGPT (GUID Partition Table): Modern scheme supporting larger drives and more partitions\n\nPartition Types:\n\nPrimary: Can be bootable and hold an operating system\nExtended: Acts as a container for logical partitions (MBR only)\nLogical: Created within an extended partition (MBR only)\n\nFilesystem: The method used to organize and store data within a partition\n\nCommon Linux filesystems: ext4, Btrfs\nCross-platform filesystems: FAT32, exFAT\n\n\n\n\nPartitioning Tools\nSeveral command-line tools are available for disk partitioning on Linux. Each has strengths for different scenarios:\n\n\n\n\n\n\n\n\n\nTool\nStrengths\nLimitations\nBest For\n\n\nfdisk\n\nSimple interface\nWidely available\n\n\nLimited GPT support in older versions\n\n\nBasic partitioning tasks\n\n\n\nparted\n\nFull GPT support\nHandles large drives\n\n\nMore complex syntax\n\n\nAdvanced partitioning needs\n\n\n\ngdisk\n\nGPT focused\nSimilar to fdisk\n\n\nLess common on minimal installations\n\n\nGPT-specific operations\n\n\n\nsfdisk\n\nScriptable for automation\n\n\nLess user-friendly\n\n\nAutomated deployments\n\n\n\n\nFor this project, and after doing some research, I chose parted for both the microSD card and SSD partitioning because:\n\nIt fully supports both MBR and GPT partition tables\nIt can handle drives larger than 2TB (relevant for the SSD)\nIt provides a more consistent interface across different partition table types\nIt supports both interactive and command-line usage\nIt‚Äôs included in most Ubuntu installations\n\n\n\nPartitioning a MicroSD Card for Backups\nLet‚Äôs partition our microSD card to serve as backup media. You can get great quality cards from Amazon Basics that are perfect for this use case. We‚Äôll use a simple, effective partition scheme. Before we dive into the actual commands, it‚Äôs important to remember that you can‚Äôt modify the memory of the active primary drive. Meaning, that you‚Äôll need to use an SSD or thumb drive as the boot media while you modify the SD card. Similarly, you‚Äôll need to use a different piece of boot media (you could use the micro SD) when partitioning the SSD.\nNow, let‚Äôs walk through this step-by-step:\n\nIdentify the device name of the microSD. Your microSD card will typically appear as something like /dev/mmcblk0 (what mine showed as) or /dev/sdX (where X is a letter like a, b, c). This command lists block devices with key information:\n\nsudo lsblk -o NAME,SIZE,FSTYPE,TYPE,MOUNTPOINT\nThe lsblk command lists all block devices, which includes your storage devices. The -o flag specifies which columns to display in the output. This final check ensures you‚Äôre working with the correct device and helps you confirm the partition structure you just created.\n\nNAME: Device identifier\nSIZE: Storage capacity\nFSTYPE: Current filesystem type\nTYPE: Whether it‚Äôs a disk or partition\nMOUNTPOINT: Where it‚Äôs currently mounted (if applicable)\n\n\nFor a backup microSD card, we‚Äôll use a simple partition layout with a single partition using ext4 filesystem, which provides good performance and Linux compatibility.\n\n# Start parted on the microSD card (replace /dev/mmcblk0 with your device)\nsudo parted /dev/mmcblk0\n\n# View the partition table for a specific device, or all\nprint mmcblk0\nprint all\n\n# Inside parted, create a new GPT partition table\n&gt; (parted) mklabel gpt\nWarning: The existing disk label on /dev/mmcblk0 will be destroyed and all data on this disk will be lost. Do you want to continue?\nYes/No? Yes\n\n# Create a single partition using the entire card\n&gt; (parted) mkpart primary ext4 0% 100%\n\n# Set a name for easy identification\n&gt; (parted) name 1 backups\n\n# Verify the partition layout\n&gt; (parted) print\n\n# Exit parted\n&gt; (parted) quit\n\nmklabel gpt: Creates a new GPT partition table (preferred over MBR for modern systems)\nmkpart primary ext4 0% 100%: Creates a primary partition using the ext4 filesystem that spans the entire device\nname 1 backup: Names the first partition ‚Äúbackup‚Äù for easy identification\nprint: Shows the current partition layout\nquit: Exits the parted utility\n\n\nAfter creating the partition, we need to format it with the ext4 filesystem. Double check the current layout of memory on your system with sudo lsblk -o NAME,SIZE,FSTYPE,TYPE,MOUNTPOINT before formatting the filesystem, to get the specific SD Card partitions device name:\n\n# Format the partition (adjust if your device/partition is different)\nsudo mkfs.ext4 -L backups /dev/mmcblk0p1\n\n-L backup: Sets the filesystem label to ‚Äúbackup‚Äù\n/dev/mmcblk0p1: The partition we just created (p1 indicates the first partition)\nYou‚Äôll see an output similar to this:\n\n\n\nNow, we need to prepare the SD card for backups. You can make those changes with the following commands:\n\n# Create a mount point\nsudo mkdir -p /mnt/backups\n\n# Add an entry to /etc/fstab for automatic mounting\necho \"UUID=$(sudo blkid -s UUID -o value /dev/mmcblk0p1) /mnt/backups ext4 defaults,noatime 0 2\" | sudo tee -a /etc/fstab\n\n# Restart the systemd daemon to get the changes made to fstab\nsudo systemctl daemon-reload\n\n# Mount the filesystem from fstab\nsudo mount /dev/mmcblk0\n\n# Create backup directories\nsudo mkdir -p /mnt/backups/{configs,logs}\n\n# Set ownership (replace 'chris' with your username)\nsudo chown -R chris:chris /mnt/backups/\n\n# Set secure permissions\nsudo chmod -R 700 /mnt/backups/\n\nmkdir -p: Creates directories and parent directories if they don‚Äôt exist\nblkid -s UUID -o value: Gets the UUID (unique identifier) of the partition\ndefaults,noatime: Mount options for good performance (noatime disables recording access times)\n0 2: The fifth field (0) disables dumping, the sixth field (2) enables filesystem checks\nmount -a: Mounts all filesystems specified in fstab\nchmod -R 700: Sets permissions so only the owner can read/write/execute\n\n\n\nPartitioning your SSD\nFor a Raspberry Pi server, a two-partition scheme offers the perfect balance of simplicity and functionality. This approach mirrors what RPi Imager creates automatically, but gives us control over the sizes:\n\nA small FAT32 boot partition for firmware and boot files\nA large ext4 root partition for the entire operating system and data\n\nThis simplified structure eliminates the complexity of separate swap and data partitions while maintaining full functionality. The Raspberry Pi can use swap files instead of dedicated partitions, which provides more flexibility for managing memory as your needs change.\n\nFor the Samsung T7 SSD, we‚Äôll follow a similar workflow. The Samsung T7 SSD will likely appear as /dev/sdX (where X is a letter like a, b, c), mine is /dev/sdb.\n\nsudo lsblk -o NAME,SIZE,FSTYPE,TYPE,MOUNTPOINT\n\nIn this code block, I‚Äôll show you a way to use parted without entering the interactive mode.\n\n# Create a new GPT partition table\nsudo parted /dev/sdb mklabel gpt\n\n# Create the EFI System Partition (ESP)\nsudo parted /dev/sdb mkpart boot fat32 1MiB 513MiB\nsudo parted /dev/sdb set 1 esp on\nsudo parted /dev/sdb set 1 boot on\n\n# Create the root partition\nsudo parted /dev/sdb mkpart ubuntu-root ext4 513MiB 100%\n\n# Verify the partition layout\nsudo parted /dev/sdb print\n\n/dev/sdc1: 512MB FAT32 partition for boot files.\n/dev/sdc2: Remaining space (about 931GB) ext4 partition for the entire system.\nThe set 1 boot on command marks the partition as bootable.\nThe set 1 esp on marks it as an EFI System Partition, ensuring compatibility with both legacy and UEFI boot methods.\n\n\nNow we need to format each partition.\n\n# Format the ESP partition\nsudo mkfs.fat -F32 -n BOOT /dev/sdb1\n\n# Format the root partition\nsudo mkfs.ext4 -L ubuntu-root /dev/sdb2\n\nmkfs.fat -F32: Creates a FAT32 filesystem\n-n ESP: Sets the volume label to ‚ÄúESP‚Äù\nmkfs.ext4: Creates an ext4 filesystem\n-L ubuntu-root: Sets the filesystem label\n/dev/sdb1, /dev/sdb2, etc.: The specific partitions we created\n\n\nNow, we will verify that the partitions went as we hoped\n\n# Check the partition layout\nsudo lsblk -o NAME,SIZE,FSTYPE,LABEL,TYPE,MOUNTPOINT\n\n# Verify the filesystem types and labels\nsudo blkid | grep sdc\nThis approach to partitioning offers several advantages:\n\nMatches RPi Imager default: Aligns with what users expect from standard Raspberry Pi installations\nEasier to manage: Fewer partitions mean simpler maintenance and troubleshooting\nThe Raspberry Pi firmware requires a FAT32 boot partition to find and load the kernel.\nThe 512MB size ensures plenty of space for kernel updates and multiple kernel versions if needed.\n\nNow you‚Äôre ready to flash Ubuntu Server to these properly prepared partitions! The RPi Imager will use this partition structure and write the system files to the correct locations.\n\nNow we‚Äôll need to mount the partitions by setting up mount points and telling the system to use them.\n\n# Check the partition layout\nsudo lsblk -o NAME,SIZE,FSTYPE,LABEL,TYPE,MOUNTPOINT\n\n# Verify the filesystem types and labels\nsudo blkid | grep sdc\n\n\nAdvanced Partitioning\nAs you begin to utilize your server more, you‚Äôre bound to use up more memory. So, it‚Äôs important to monitor your partition space usage.\n# View disk usage\ndf -h\n\n# View inode usage (for number of files)\ndf -i\n\n# View detailed filesystem information\nsudo tune2fs -l /dev/sda2 | grep -E 'Block count|Block size|Inode count|Inode size'\n\ndf -h: Shows disk usage in human-readable format\ndf -i: Shows inode usage (inode = index node, representing a file)\ntune2fs -l: Lists filesystem information for ext2/3/4 filesystems\ngrep -E: Filters output for specified patterns\n\nFurthermore, you may realize that you want to reformat your SSD at some point because your storage needs changed. You can reformat the partitions using the following code.\n# For online resizing of ext4 (unmounting not required)\nsudo parted /dev/sda\n(parted) resizepart 4 100%  # Resize partition 4 to use all available space\n(parted) quit\n\n# After resizing the partition, expand the filesystem\nsudo resize2fs /dev/sda4\n\nresizepart 4 100%: Resizes partition 4 to use 100% of the remaining available space\nresize2fs: Resizes an ext2/3/4 filesystem to match the partition size"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#backups-and-basic-automation",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#backups-and-basic-automation",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Backups and Basic Automation",
    "text": "Backups and Basic Automation\nNow that we‚Äôve configured the basics, from permissions to networking and ssh to partitions, we‚Äôll want to save those changes in case something happens and to ensure a seamless transition to the SSD for boot media. You‚Äôve already seen some basic backups. The process is the same, essentially creating a folder and then putting a copy of the current file into it and maybe adding a .bak extension to make it clear this is a previous version. That being said, to go through and do this for each and every folder we‚Äôve made changes in is impractical now, let alone in the future when more complex configurations are done. So, in this section, we‚Äôll go over creating a basic script to backup all of our configs and automating the backups.\nFor this section, we‚Äôll use rsync because it provides several important advantages over simple copy commands, that you may remember from the section on ssh:\n\nIncremental backups that only transfer changed files\nPreservation of file permissions, ownership, and timestamps\nBuilt-in compression for efficient transfers\nDetailed progress information and logging\nThe ability to resume interrupted transfers\n\nBefore we start, make sure you have:\n\nA mounted backup drive at /mnt/backups/\n\n\nSetting Up the Backup Directory\nFirst, in case you didn‚Äôt do this earlier, we‚Äôll prepare the backup directory structure and set appropriate permissions:\n# Create backup directories if they don't exist\nsudo mkdir -p /mnt/backups/configs\nsudo mkdir -p /mnt/backups/system\n\n# Change ownership to your user (replace 'chris' with your username)\nsudo chown -R chris:chris /mnt/backups\n\n# Set appropriate permissions\nsudo chmod -R 700 /mnt/backups  # Only owner can read/write/execute\n\n\nConfiguration Files Backup\nWhile it‚Äôs definitely beneficial to have a local copy of your backups to easily roll back changes, it isn‚Äôt the most secure solution to have all of your information in one place. Furthermore, the SSD is partitioned, but it doesn‚Äôt currently have an OS or any files stored. So, now it‚Äôs time to take advantage of the microSD card we formatted earlier.\nFor the purpose of this guide, I‚Äôll be showing you how to use rsync for a remote transfer to your client machine and how to automatically store backups on the SD card. The script we‚Äôll use saves all of the key user and system information (things like passwords), as well as the configuration changes we made. Additionally, as long as your SD card is mounted to /mnt/backups the backup will automatically be saved to the external memory. The following script demonstrates how to perform the backup while preserving all file attributes:\n#!/bin/bash\n# Using the {} around DATEYMD in the file path ensure it's specified as the variable's value, and the subsequent parts are not included\n\nDATEYMD=$(date +%Y%m%d)\nBACKUP_DIR=\"/mnt/backups/configs/$DATEYMD\"\nLOG_DIR=\"/mnt/backups/logs\"\nLOG_FILE=\"$LOG_DIR/${DATEYMD}_config_backup.log\"\n\n# Check if script is run as root\nif [ \"$(id -u)\" -ne 0 ]; then\n    echo \"Error: This script must be run as root. Try using sudo.\"\n    exit 1\nfi\n\n# Create backup directory\nmkdir -p \"$BACKUP_DIR\"\n\n{\n    # 1. User and Group Information\n    echo \"Backing up User and Group configuration...\"\n    sudo rsync -aAXv /etc/passwd \"$BACKUP_DIR/passwd.bak\"\n    sudo rsync -aAXv /etc/group \"$BACKUP_DIR/group.bak\"\n    sudo rsync -aAXv /etc/shadow \"$BACKUP_DIR/shadow.bak\"\n    sudo rsync -aAXv /etc/gshadow \"$BACKUP_DIR/gshadow.bak\"\n\n    # 2. Crontab Configurations\n    echo \"Backing up Crontab configuration...\"\n    sudo rsync -aAXv /etc/crontab \"$BACKUP_DIR/\"\n    sudo rsync -aAXv /var/spool/cron/crontabs/. \"$BACKUP_DIR/crontabs/\"\n\n     # 3. SSH Configuration\n    echo \"Backing up SSH configuration...\"\n    sudo rsync -aAXv /etc/ssh/. \"$BACKUP_DIR/ssh/\"\n    \n    # Create user_ssh directory\n    mkdir -p \"$BACKUP_DIR/user_ssh\"\n    \n    # Copy SSH user configuration with explicit handling of authorized_keys\n    rsync -aAXv /home/chris/.ssh/config \"$BACKUP_DIR/user_ssh/\" 2&gt;/dev/null || true\n    rsync -aAXv /home/chris/.ssh/id_* \"$BACKUP_DIR/user_ssh/\" 2&gt;/dev/null || true\n    rsync -aAXv /home/chris/.ssh/known_hosts \"$BACKUP_DIR/user_ssh/\" 2&gt;/dev/null || true\n    \n    # Explicitly backup authorized_keys if it exists\n    if [ -f /home/chris/.ssh/authorized_keys ]; then\n        echo \"Backing up authorized_keys file...\"\n        rsync -aAXv /home/chris/.ssh/authorized_keys \"$BACKUP_DIR/user_ssh/\"\n    else\n        echo \"No authorized_keys file found in /home/chris/.ssh/\"\n    fi\n\n    # 4. UFW (Uncomplicated Firewall) Configuration\n    echo \"Backing up ufw configuration...\"\n    sudo rsync -aAXv /etc/ufw/. \"$BACKUP_DIR/ufw/\"\n    sudo ufw status verbose &gt; \"$BACKUP_DIR/ufw_rules.txt\"\n\n    # 5. Fail2Ban Configuration\n    echo \"Backing up fail2ban configuration...\"\n    sudo rsync -aAXv /etc/fail2ban/. \"$BACKUP_DIR/fail2ban/\"\n\n    # 6. Network Configuration\n    echo \"Backing up Network configuration...\"\n    sudo rsync -aAXv /etc/network/. \"$BACKUP_DIR/network/\"\n    sudo rsync -aAXv /etc/systemd/network/. \"$BACKUP_DIR/systemd/network/\"\n    sudo rsync -aAXv /etc/netplan/. \"$BACKUP_DIR/netplan/\"\n    sudo rsync -aAXv /etc/hosts \"$BACKUP_DIR/hosts.bak\"\n    sudo rsync -aAXv /etc/hostname \"$BACKUP_DIR/hostname.bak\"\n    sudo rsync -aAXv /etc/resolv.conf \"$BACKUP_DIR/resolv.conf.bak\"\n    sudo rsync -aAXv /etc/wpa_supplicant/. \"$BACKUP_DIR/wpa_supplicant/\"\n\n    # 7. Systemd Services and Timers\n    echo \"Backing up Systemd Timers configuration...\"\n    sudo rsync -aAXv /etc/systemd/system/. \"$BACKUP_DIR/systemd/\"\n\n    # 8. Logrotate Configuration\n    echo \"Backing up Logrotate configuration...\"\n    sudo rsync -aAXv /etc/logrotate.conf \"$BACKUP_DIR/logrotate.conf.bak\"\n    sudo rsync -aAXv /etc/logrotate.d/. \"$BACKUP_DIR/logrotate.d/\"\n\n    # 9. Timezone and Locale\n    echo \"Backing up Timezone and Locale configuration...\"\n    sudo rsync -aAXv /etc/timezone \"$BACKUP_DIR/timezone.bak\"\n    sudo rsync -aAXv /etc/localtime \"$BACKUP_DIR/localtime.bak\"\n    sudo rsync -aAXv /etc/default/locale \"$BACKUP_DIR/locale.bak\"\n\n    # 10. Keyboard Configuration\n    echo \"Backing up Keyboard configuration...\"\n    sudo rsync -aAXv /etc/default/keyboard \"$BACKUP_DIR/keyboard.bak\"\n\n    # 11. Filesystem Table (fstab)\n    echo \"Backing up filesystem table (fstab)...\"\n    sudo rsync -aAXv /etc/fstab \"$BACKUP_DIR/fstab.bak\"\n\n    # Set appropriate permissions\n    echo \"Configuring backup directory permissions...\"\n    sudo chown -R chris:chris \"$BACKUP_DIR\"\n    sudo chmod -R 600 \"$BACKUP_DIR\"\n\n    echo \"Configuration backup completed at: $BACKUP_DIR\"\n\n} &gt; \"$LOG_FILE\" 2&gt;&1\n\necho \"Logs available at: $LOG_FILE\"\n# Make the script executable\nchmod +x /scripts/config_backup.sh\n\n# Run the script\n./scripts/config_backup.sh\nThe rsync commands use several important options:\n\n-a: Archive mode, preserves almost everything\n-A: Preserve ACLs (Access Control Lists)\n-X: Preserve extended attributes\n-v: Verbose output\n--one-file-system: Don‚Äôt cross filesystem boundaries\n--hard-links: Preserve hard links\n--exclude: Skip specified directories\n\n\nRemote Transfers of Backups\nWe covered rsync vs.¬†scp earlier, so remember that rsync is specifically designed for copying and transferring files, so it offers more sophisticated file synchronization capabilities than basic tools like SCP. If you need a refresher, run the following command from your client machine (laptop), just change the paths to match what your system uses.\nrsync -avz --partial --progress --update chris@ubuntu-pi-server:/mnt/backups/configs/master/ ~/Documents/raspberry_pi_server/backups/configs/master\nThe flags do the following:\n\n-a: Archive mode, which preserves permissions, timestamps, symbolic links, etc.\n-v: Verbose output, showing what files are being transferred\n-z: Compress data during transfer for faster transmission\n--partial: Keep partially transferred files, allowing you to resume interrupted transfers\n--progress: Show progress during transfer\n--update: Skip files that are newer on the receiver (only transfer if source is newer)\n\n\n\n\nRestoring from Backup\nNow that we‚Äôve backed up all of the configurations we‚Äôve made so far, it‚Äôs time to create a script that restores that backup. At the time of writing this, I‚Äôve probably had to reflash a fresh image and reconfigure things between 10 and 20 times. I‚Äôm so good at it, that I can now do it all in under 20 minutes. That being said, it‚Äôs much easier to do when you can just run a script that takes all of the configurations from your Master backup and overwrites the defaults.\nFirst, here are some important things to remember:\n\nThe --delete option during restore will remove files at the destination that don‚Äôt exist in the backup. Use with caution.\nConsider using rsync‚Äôs --dry-run option to test backups and restores without making changes.\nThe backup includes sensitive system files. Store it securely and restrict access.\nConsider encrypting the backup directory for additional security.\nTest the restore process in a safe environment before using in production.\n\nAfter writing this, you can test the script by running it on your server with the boot media you‚Äôve been using (not the SSD)‚Äì just make sure you save the master/ backup and any scripts/configs externally first. You‚Äôll know this succeeds, if nothing changes after the reboot. When you‚Äôve verified that‚Äôs done, we‚Äôll shutdown the server and make the SSD the boot media. For now, let‚Äôs write the config_restore script.\n#!/bin/bash\n\n# Simple Configuration Restoration Script for Ubuntu Pi Server\nBACKUP_DIR=${1:-\"/mnt/backups/configs/master\"}\n\n# Check if script is run as root\nif [ \"$(id -u)\" -ne 0 ]; then\n    echo \"Error: This script must be run as root. Try using sudo.\"\n    exit 1\nfi\n\n# Check if the backup directory exists\nif [ ! -d \"$BACKUP_DIR\" ]; then\n    echo \"Error: Backup directory not found: $BACKUP_DIR\"\n    echo \"Usage: $0 [backup_directory_path]\"\n    exit 1\nfi\n\n# Begin restoration process\necho \"Starting configuration restoration from $BACKUP_DIR...\"\necho \"This will overwrite current system configurations with those from the backup.\"\nread -p \"Continue with restoration? (y/n): \" CONFIRM\nif [[ \"$CONFIRM\" != \"y\" && \"$CONFIRM\" != \"Y\" ]]; then\n    echo \"Restoration aborted by user.\"\n    exit 0\nfi\n\n# 1. Restore User and Group Information\necho \"Restoring user and group information...\"\n[ -f \"$BACKUP_DIR/passwd.bak\" ] && rsync -a \"$BACKUP_DIR/passwd.bak\" /etc/passwd\n[ -f \"$BACKUP_DIR/group.bak\" ] && rsync -a \"$BACKUP_DIR/group.bak\" /etc/group\n[ -f \"$BACKUP_DIR/shadow.bak\" ] && rsync -a \"$BACKUP_DIR/shadow.bak\" /etc/shadow\n[ -f \"$BACKUP_DIR/gshadow.bak\" ] && rsync -a \"$BACKUP_DIR/gshadow.bak\" /etc/gshadow\n\n# Explicitly Set Permissions for Critical System Files\necho \"Fixing critical system file permissions...\"\nchmod 644 /etc/passwd   # Read-write for root, read-only for everyone else\nchmod 644 /etc/group    # Read-write for root, read-only for everyone else  \nchmod 640 /etc/shadow   # Read-write for root, read-only for shadow group\nchmod 640 /etc/gshadow  # Read-write for root, read-only for shadow group\n\n# 2. Restore SSH Configuration\necho \"Restoring SSH configuration...\"\n[ -d \"$BACKUP_DIR/ssh\" ] && rsync -a \"$BACKUP_DIR/ssh/\" /etc/ssh/\nchmod 600 /etc/ssh/ssh_host_*_key 2&gt;/dev/null || true\nchmod 644 /etc/ssh/ssh_host_*_key.pub 2&gt;/dev/null || true\n\n# 3. Restore UFW Configuration\necho \"Restoring UFW configuration...\"\nif [ -d \"$BACKUP_DIR/ufw\" ]; then\n    apt-get install -y ufw &gt;/dev/null\n    rsync -a \"$BACKUP_DIR/ufw/\" /etc/ufw/\nfi\n\n# 4. Restore Fail2Ban Configuration\necho \"Restoring Fail2Ban configuration...\"\nif [ -d \"$BACKUP_DIR/fail2ban\" ]; then\n    apt-get install -y fail2ban &gt;/dev/null\n    rsync -a \"$BACKUP_DIR/fail2ban/\" /etc/fail2ban/\nfi\n\n# 5. Restore Network Configuration\necho \"Restoring network configuration...\"\n[ -d \"$BACKUP_DIR/network\" ] && rsync -a \"$BACKUP_DIR/network/\" /etc/network/\n[ -d \"$BACKUP_DIR/systemd/network\" ] && rsync -a \"$BACKUP_DIR/systemd/network/\" /etc/systemd/network/\n[ -d \"$BACKUP_DIR/netplan\" ] && rsync -a \"$BACKUP_DIR/netplan/\" /etc/netplan/\n[ -f \"$BACKUP_DIR/hosts.bak\" ] && rsync -a \"$BACKUP_DIR/hosts.bak\" /etc/hosts\n[ -f \"$BACKUP_DIR/hostname.bak\" ] && rsync -a \"$BACKUP_DIR/hostname.bak\" /etc/hostname\n[ -f \"$BACKUP_DIR/resolv.conf.bak\" ] && rsync -a \"$BACKUP_DIR/resolv.conf.bak\" /etc/resolv.conf\n[ -d \"$BACKUP_DIR/wpa_supplicant\" ] && rsync -a \"$BACKUP_DIR/wpa_supplicant/\" /etc/wpa_supplicant/\n\n# 6. Restore Filesystem Table (fstab)\necho \"Restoring filesystem table (fstab)...\"\n[ -f \"$BACKUP_DIR/fstab.bak\" ] && rsync -a \"$BACKUP_DIR/fstab.bak\" /etc/fstab\n\n# 7. Restore Package List\n#echo \"Reinstalling packages from backup...\"\n#if [ -f \"$BACKUP_DIR/package_list.txt\" ]; then\n#    apt-get update && apt-get install -y dselect\n#    dpkg --set-selections &lt; \"$BACKUP_DIR/package_list.txt\"\n#    apt-get dselect-upgrade -y\n#fi\n\n# Restart services\nsystemctl restart systemd-networkd wpa_supplicant@wlan0.service ssh ufw fail2ban \n\necho \"Configuration restoration completed. A system reboot is recommended.\"\nread -p \"Would you like to reboot now? (y/n): \" REBOOT\n[[ \"$REBOOT\" == \"y\" || \"$REBOOT\" == \"Y\" ]] && reboot\n\nexit 0\nYou probably have some questions about the script let me explain some of the decisions I made while doing some trial and error testing.\n\nOriginally, I had the backup directory as a value in the script call itself, now it just defaults to the master/ backup\n\nThis backup is one I know that works and is in the format I‚Äôm hoping\nEasier to have a standard version to reference than relying on monthly backups\n\nI had a lot of issues with incorrect permissions after restoring backups previously, so it needs to be run with sudo\nFirst, the user and group information is important, a lot of processes behind the scenes rely on these configurations\n\nPart of this, I added an explicit chmod call because after the reboot, I was getting an error with whoami\n\nThe command whoami returns which user you are/currently running commands as\nThe user and group info wasn‚Äôt exactly the same, it was leaving my user chris as UID 1000, but changing the group to 1003\nThe chmod call fixes that\nYou can use getent group | grep 'chris' to view all group IDs and assignments\n\n\nSecond, restoring the ssh configurations ensures security and remote connectivity\nThird, ufw increases your system security\nFourth, fail2ban does the same by improving security\nFifth, restoring the network configurations, originally, I had issues because networkd wasn‚Äôt included\n\nThis block ensures all of the systemd configurations are included\n\nSixth Package List TBD\nThen, the specific services we modified are all explicitly started\n\nWhile developing this, some of the services wouldn‚Äôt necessarily start, so I would run into network or ssh issues post-reboot\n\nFinally, the script asks you to reboot your system so all of the changes take affect\n\n# Make the script executable\nchmod +x /scripts/config_restore.sh\n\n# Run the script\nsudo ./scripts/config_restore.sh\n\n\nFinal Thoughts\nAt the time of writing this, I‚Äôve tested and revised everything up until this point. There are a few things I haven‚Äôt added yet, that I want to later, once it‚Äôs necessary.\n\nSaving the installed packages and their requirements, similar to a uv lockfile or .venv requirements.txt file.\n\nModify the restore script to restore packages to exactly where they should be\nNot necessary now, because only Traceroute, ufw, and fail2ban were installed; however, when other apps are added this will be helpful for tracking dependencies and versioning issues\n\nCreating a private repoistory for storing configuration files, scripts, etc. and then using that, rather than rsync with my laptop\nDo some more granular network configurations, inlcuding subnets and static IPs"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#changing-your-boot-media-device",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#changing-your-boot-media-device",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Changing Your Boot Media Device",
    "text": "Changing Your Boot Media Device\nWhen you‚Äôre moving from one boot device to another on a Raspberry Pi, you‚Äôre essentially telling the system where to find its operating system files. The Raspberry Pi‚Äôs bootloader looks for specific files on a FAT32 partition to begin the boot process, then loads the main operating system from the root partition. This transition is a fundamental server administration skill that every system administrator should understand.\nThe transition involves several critical steps.\n\nFirst, we prepare the SSD with a fresh operating system installation.\nThen we properly shut down the system to ensure no data corruption occurs.\nNext, we physically reconfigure the hardware to make the SSD the primary boot device.\nFinally, we verify that everything works correctly and remove the temporary boot media.\n\nThink of this process like moving into a new house. You‚Äôve already built the structure (partitioning), but now you need to move all your belongings (the operating system) into it, update your mailing address (boot configuration), and ensure everything works in the new location.\n\nBoot Configuration Transition Process\nThe good news, we‚Äôve already done most of the work required and what we haven‚Äôt is going to be something we did for the original boot device. We‚Äôll first run a few commands from the server, do one thing on a different computer, and then we‚Äôll be back to the server.\n\nFirst, you‚Äôll need to flash a fresh Ubuntu Server LTS image onto your newly partitioned SSD. This process will write the operating system files to the appropriate partitions you created. This should be done on your Raspberry Pi server.\n\n# Before removing the SSD, check its device identifier one more time\nsudo lsblk -o NAME,SIZE,FSTYPE,LABEL,TYPE,MOUNTPOINT\nNow you‚Äôll need to use RPi Imager on your MacBook:\n\nConnect the SSD to your MacBook\nOpen Raspberry Pi Imager\nSelect Ubuntu Server LTS (same version you used before)\nSelect your SSD as the storage device\nUse the exact same advanced settings you used when first setting up your Raspberry Pi:\n\nSame username (chris)\nSame password\nSame SSH key settings\nSame WiFi credentials\n\n\nThe reason we use identical settings is to reduce the amount of changes needed to replicate the environment we previously configured. Your SSH keys, user permissions, and network configurations will all match what we know works for an initial boot, eliminating the need to reconfigure everything from scratch.\n\nBefore making any hardware changes, we need to ensure all data is written to disk and all processes are safely terminated. This prevents corruption and data loss during the transition.\n\n# Save any unsaved work and exit all applications\n# Ensure no important processes are running\n\n# Sync all file system buffers to disk\nsudo sync\n\n# Check for any open files on your current boot device (thumb drive)\nsudo lsof | grep -E '^[^ ]+ +[^ ]+ +[^ ]+ +[^ ]+ +[^ ]+ +[^ ]+ +FIFO'\n\n# Display active processes to ensure nothing critical is running\nps aux | grep -v '\\['\n\nThe sync command forces all pending disk writes to complete immediately. This is crucial because Linux uses write caching for performance, meaning data might still be in memory waiting to be written.\nThe lsof command lists open files, helping you identify any processes that might be accessing the current boot device.\nThe ps aux command shows all active processes, giving you a final check that nothing important is running.\n\n\nA proper shutdown sequence ensures all services stop gracefully and file systems are cleanly unmounted:\n\n# Perform a clean system shutdown\nsudo shutdown -h now\nThe shutdown command initiates a clean shutdown sequence. The -h flag tells the system to halt (power off) after shutdown. This is important because the system may not power off, especially on older systems‚Äîit could just bring it to single-user mode or runlevel 1, depending on configuration. While now indicates the shutdown should happen immediately. This command:\n\nSends a termination signal to all running processes\nAllows services to save their state and clean up\nUnmounts all filesystems in the correct order\nFinally powers down the system\n\n\nNow comes the physical hardware transition. With your Raspberry Pi powered off:\n\n\nRemove the thumb drive (current boot device)\nConnect the SSD via USB to the Raspberry Pi\nSet aside the microSD card (backup device)\nEnsure all connections are secure\n\nThis step is straightforward but crucial. The Raspberry Pi will attempt to boot from the first bootable device it finds. By removing the thumb drive and connecting the SSD, we‚Äôre ensuring the Pi finds and uses the SSD as its boot device. We do not need to remove the SD card used for backups, because it never had an OS flashed onto it. The card‚Äôs file system just provides extra memory, instead of being an extra operating system.\n\nPower on your Raspberry Pi and observe the boot process.\n\nMake sure to connect your monitor and keyboard before the boto begins. The boot process should proceed similarly to your initial setup. The Raspberry Pi firmware reads the configuration from the SSD‚Äôs boot partition, loads the kernel, and then mounts the root filesystem. If everything works correctly, you should see the familiar Ubuntu Server boot messages and eventually reach a login prompt.\nGood news, you won‚Äôt need to do this again (for this server). After this, you‚Äôll have your core system, memory, and configurations complete.\n\nAfter booting, you‚Äôll need to log in directly to the Raspberry Pi using a keyboard and monitor. This is because the SSH service may not be running automatically on the fresh installation.\n\n\nConnect a keyboard and monitor to your Raspberry Pi\nLog in with your username and password\nStart the SSH service manually:\n\n# Start the SSH service to enable remote connections\nsudo systemctl start ssh\n\n# Verify the service is running\nsudo systemctl status ssh\n\n# Test a remote connection from your client machine\n# Test SSH connection (initially with password authentication)\nssh chris@192.168.1.151\n\nYou‚Äôll need to specify the local IP because none of the ssh configs are updated yet\n\nYour nonstandard port, 45000 in this guide\nThe local IP may be a different one in the 192.168.0.0/16 range (which is reserved for local IPs)\n\nIf this is working and you can remotely connect, then we can move on to the next step\n\n\nWith SSH access established, transfer your configuration backup and restore scripts from your computer to the Raspberry Pi:\n\nFirst, make sure to create the directory, because it won‚Äôt exist on a fresh boot. The, from your client computer, a MacBook in my case, you‚Äôll run the rsync commands to move the master backup and scripts. Notice that you can use either the ~ shortcut to denote your home directory, or write the path explicitly. We‚Äôll also need to move the master backup to the proper backup directory, so the restore script works properly.\n# On the Raspberry Pi, create the backup directory structure\nsudo mkdir -p /mnt/backups/configs/\n\n# From your MacBook, copy your backup files to the Pi\nrsync -avz ~/path/to/backups/configs/master chris@ubuntu-pi-server:/home/chris/\n\n# Copy your restore script to the Pi\nrsync -avz ~/path/to/scripts/config_restore.sh chris@ubuntu-pi-server:~/scripts/\n\n# Move the master backup directory to the correct location\nsudo mv /home/chris/master /mnt/backups/configs/\n\nRun the Configuration Restore Script\n\n# Make sure the script is executable\nchmod +x ~/scripts/config_restore.sh\n\n# Run the restore script\nsudo ./scripts/config_restore.sh\n\nVerify the System\n\n# Verify the root filesystem device\ndf -h /\n\n# Verify the microSD card is mounted properly\ndf -h /mnt/backups\n\n# Check that your backup files are accessible\nls -la /mnt/backups/configs/\n\n# Verify your network settings\nip addr show\n\n# Check System configurations\nsudo systemctl status ssh\nsudo systemctl status ufw\nsudo systemctl status fail2ban\nsudo systemctl status systemd-networkd\nsudo systemctl status wpa_supplicant@wlan0.service\nThe df -h / command shows disk usage statistics for the root filesystem, including which device it‚Äôs mounted from. You should see /dev/sdb2 (or similar) listed as the root device, and /dev/sdb1 as the boot device, not the thumb drive identifier you used before. You‚Äôll see a similar output, just focused on your /backups directory when running the second command. The ls -la command shows you all of the contents of a directory, as well as the permissions. The other commands you should be familiar with by now.\n\n\nFinal Thoughts\nOur Ubuntu Raspberry Pi Server is now booting from the SSD, and all your previous configurations have been restored. You may notice that on reboot, running sudo lsblk will show your SSD under a different This configuration provides a solid foundation for the more advanced server features we‚Äôll implement in the next sections. By moving to SSD boot, we‚Äôve significantly improved our server‚Äôs performance profile. SSDs offer several advantages over traditional storage media:\n\nFaster boot times: Your Raspberry Pi will start much more quickly\nImproved I/O performance: Database operations, file access, and application loading will be noticeably faster\nBetter reliability: SSDs have no moving parts, making them more resilient to physical impacts\nLower power consumption: SSDs typically use less power than traditional hard drives\n\nThe transition to SSD boot also aligns with modern server practices, where solid-state storage is becoming the standard for production environments. This configuration will serve us well as we expand into containerization with Docker and orchestration with Kubernetes."
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#remote-development-with-vs-code",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#remote-development-with-vs-code",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Remote Development with VS Code",
    "text": "Remote Development with VS Code"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#automation-and-monitoring",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#automation-and-monitoring",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Automation and Monitoring",
    "text": "Automation and Monitoring"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#docker",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#docker",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Docker",
    "text": "Docker"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#kubernetes",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#kubernetes",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Kubernetes",
    "text": "Kubernetes"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-setup",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-setup",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Initial Setup",
    "text": "Initial Setup\n\nKey Terms\n\nHDMI:\nSSD:\nFlash Drive:\nSD Card:\nMemory Sizes:\nBit:\nByte:\n\n\n\nHardware Requirements\nThis section will provide basic setup instructions, so you‚Äôll have the same tools I do and can follow along with this guide, step-by-step.\n\nRaspberry Pi 4 8GB\n\nMicro HDMI to HDMI cord (for direct access)\nProtective case\nCooling fan\nAppropriate Power Supply\n\nKeyboard (connected via USB for direct access)\nMonitor (for direct access)\n1TB Samsung T7 SSD (connected via USB for boot media/core memory)\n64GB Generic Flash Drive (used as the boot media when partitioning the SSD)\nAmazon Basics 128GB microSD card (or other microSD cards for backups media)\nSSH-capable devices for headless access\n\nI‚Äôm using a MacBook Air\nI prefer the MacOS and Terminal for personal development, because I use Windows at my day job\n\n\n\n\nImage Requirements\nOnce you have your hardware ready to go, you can being setting up the software. I‚Äôm using Linux Server LTS because it‚Äôs a stable version of Linux intended for headless, server environments. LTS means long-term support, so unlike the more frequently updated versions, these OS versions are supported for 5 years. Additionally, you‚Äôll want to use public-key authentication for better security purposes, but more on that in the SSH Section.\n\nHave your Thumb Drive ready and able to connect to either a laptop or desktop (whichever you plan to use with SSH)\nDownload Raspberry Pi Imager from the official website\nRun the Imager and configure your installation of the most recent Linux Server LTS image\n\nSelect your Raspberry Pi device\n\n\n\nSelect the OS Image you want to flash\n\n\n\nSelect the media storage device for the image\n\n\n\nConfigure settings\n\n Here, you‚Äôll configure your primary user ID and password; network connection; locale and timezone; and your hostname (the nickname your computer remembers the IP address as).\n Here, you‚Äôll configure your SSH settings. You should probably use public-key authentication only when dealing with SSH in your leave, but for learning purposes, you don‚Äôt need to at this time. Later on in this guide, I‚Äôll walk you through the steps to manually configure SSH, if you are unfamiliar.\n These are more preference based, but it‚Äôs nice to have the storage device automatically eject once the flashing is complete. Then, you just need to unplug it and plug it into your Raspberry Pi to get going.\n\n\n\nGet Started\nIt‚Äôs time to get the actual Raspberry Pi device setup. For most of this guide, I recommend leaving the Pi outside of the case, because it‚Äôll be easier to plug and unplug some of the devices‚Äì the microSD slot is not accessible while the case is on. Later, once we‚Äôve got everything configured and setup as we like, we will attach the fan and case, so it‚Äôs a bit safer and able to run in an always-on state. I‚Äôll share a picture of what my server looks like during the early development, and then later I‚Äôll show what it looks like with everything connected and setup.\n\nNow you‚Äôre ready to plug your boot media device (the Thumb Drive) into your Raspberry Pi. You should also connect a keyboard, monitor, and power supply. Once all of this is connected, your Raspberry Pi will boot up. Connecting a monitor and keyboard will allow you to directly interact with the system‚Äôs terminal. Ideally, you‚Äôll use SSH, but it may be helpful to have direct access in case there are any network issues. Eventually, the SSD will serve as the boot media and primary storage device for the server; however, we can‚Äôt modify its partitions while it‚Äôs serving as the boot device. So, we‚Äôll use a thumb drive as the boot media device, until we complete the partitions.\nWhen first connecting from the wired keyboard and monitor, let all of the start up processes finish running (these will hopefully have brackets with the word Success in green). Then, type in the name of the User ID you wish to login with, in my case it‚Äôs chris. Then, enter the password (no characters will show up as you type it in) and hit enter. You‚Äôll see a plaintext message telling you the OS version, some system information (memory usage, temperature, etc.), and you‚Äôll see a line where you can enter commands (the CLI). In my case, it looks like this: chris@ubuntu-pi-server:~$\nNow you can run some basic commands to see where you are and what you have available to you. Spoiler alert, you‚Äôre in your home directory and have no files. In my case it‚Äôs /home/chris, where the /home directory is owned by root and /chris is owned by my user‚Äì UID 1000 (the default for new users on a fresh system/image). Right now your directory will be empty, outside of some hidden folders like .ssh. More on this later.\nNext we‚Äôll cover what happened during the boot process, the basic structure of the Linux Server OS, and some important information related to permissions, before we move on to basic networking concepts and configurations."
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-linux_basics",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-linux_basics",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Linux Server Basics",
    "text": "Linux Server Basics\n\nKey Terms\n\nHardware:\nSoftware:\nFilesystem:\nOS (Operating System):\nUnix:\nLinux:\nFilesystem Hierarchy Standard:\nRAM:\nKernel:\n\n\n\nFirst Boot Process\nWhen you first boot a fresh Ubuntu Server LTS image on your Raspberry Pi, several important initialization processes occur that don‚Äôt happen during subsequent boots. The first boot of your Ubuntu Server LTS on the Raspberry Pi is fundamentally different from subsequent boots because it performs one-time initialization tasks. While later boots will simply load the configured system, this first boot sets up critical system components.\n\nHardware Detection: The system performs comprehensive hardware detection to identify and configure your Raspberry Pi‚Äôs components.\nInitial RAM Disk (initrd): The bootloader loads a temporary filesystem into memory that contains essential drivers and modules needed to mount the real root filesystem.\nFilesystem Check and Expansion: On first boot, the system checks the integrity of the filesystem and often expands it to utilize the full available space on your Flash Drive.\nCloud-Init Processing: Ubuntu Server uses cloud-init to perform first-boot configuration tasks (the processes you see running on the monitor on startup)\n\nSetting the hostname\nGenerating SSH host keys\nCreating the default user account\nRunning package updates\n\nMachine ID Generation: A unique machine ID is generated and stored in /etc/machine-id.\nNetwork Configuration: The system attempts initial network setup based on detected hardware.\n\nThe key difference is that subsequent boots skip these initialization steps since they‚Äôve already been completed, making them significantly faster.\n\n\nService Management with systemd\nSystemd is the modern initialization and service management system for Linux. It‚Äôs responsible for bootstrapping the user space and managing all processes afterward. Key components of systemd include:\n\nUnits: Everything systemd manages is represented as a ‚Äúunit‚Äù with a corresponding configuration file. Units include:\n\nService units (.service): Define how to start, stop, and manage daemons (background processes that are always on)\nSocket units (.socket): Manage network/IPC sockets\nTimer units (.timer): Trigger other units based on timers\nMount units (.mount): Control filesystem mount points\n\nTarget units: Represent system states (similar to runlevels in older systems)\n\nmulti-user.target: Traditional text-mode system\ngraphical.target: Graphical user interface\nnetwork.target: Network services are ready\n\n\nFor example, let‚Äôs take a look at a generic SSH service file.\n[Unit]\nDescription=OpenSSH server daemon\nDocumentation=man:sshd(8) man:sshd_config(5)\nAfter=network.target auditd.service\nWants=network.target\n\n[Service]\nEnvironmentFile=-/etc/default/ssh\nExecStartPre=/usr/sbin/sshd -t\nExecStart=/usr/sbin/sshd -D $SSHD_OPTS\nExecReload=/usr/sbin/sshd -t\nExecReload=/bin/kill -HUP $MAINPID\nKillMode=process\nRestart=on-failure\nRestartPreventExitStatus=255\nType=notify\n\n[Install]\nWantedBy=multi-user.target\nTo break this down:\n\n[Unit]: Metadata and dependencies\n\nDescription: Human-readable service description\nDocumentation: Where to find documentation\nAfter: Units that should be started before this one\nWants: Soft dependencies\n\n[Service]: Runtime behavior\n\nExecStart: Command to start the service\nExecReload: Command to reload the service\nRestart: When to restart the service\nType: How systemd should consider the service started\n\n[Install]: Installation information\n\nWantedBy: Target that should include this service\n\n\nUbuntu Server‚Äôs current standard is systemd, but previously it was SysV. A few key improvements include:\n\nParallel Service Startup: Systemd can start services in parallel, improving boot times.\nDependency Management: Systemd handles service dependencies more effectively.\nService Supervision: Systemd continuously monitors and can automatically restart services.\nSocket Activation: Services can be started on-demand when a connection request arrives.\n\nManaging services is easy using the command line, a crucial component of headless applications, a few examples are:\n\nView service status: systemctl status ssh\nStart a service: sudo systemctl start ssh\nStop a service: sudo systemctl stop ssh\nEnable at boot: sudo systemctl enable ssh\nDisable at boot: sudo systemctl disable ssh\nView logs: journalctl -u ssh\n\n\n\nUnderstanding Your Home Directory\nNow that you‚Äôve logged in and can work on your server, you may wonder where you are and what‚Äôs there. Running pwd will return the file path of your current location. Running ls -a will show you all available files and directories in your current location. Running these, you‚Äôll see a few things specifically for Shell configuration (your terminal/CLI):\n\n.bash_history: Contains a record of commands you‚Äôve executed in the bash shell. This helps with command recall using the up arrow or history command.\n.bash_logout: Executed when you log out of a bash shell. Often used for cleanup tasks like clearing the screen.\n.bashrc: The primary bash configuration file that‚Äôs loaded for interactive non-login shells. It defines aliases, functions, and shell behavior. When you open a terminal window, this file is read.\n.profile: Executed for login shells. It typically sets environment variables and executes commands that should run once per login session, not for each new terminal.\n\n# Sample .bashrc section\n# enable color support of ls and also add handy aliases\nif [ -x /usr/bin/dircolors ]; then\n    test -r ~/.dircolors && eval \"$(dircolors -b ~/.dircolors)\" || eval \"$(dircolors -b)\"\n    alias ls='ls --color=auto'\n    alias grep='grep --color=auto'\n    alias fgrep='fgrep --color=auto'\n    alias egrep='egrep --color=auto'\nfi\n\n# some more ls aliases\nalias ll='ls -alF'\nalias la='ls -A'\nalias l='ls -CF'\nBeyond those, you‚Äôll also find hidden application directories.\n\n.cache: Contains non-essential data that can be regenerated as needed. Applications store temporary files here to improve performance on subsequent runs.\n.dotnet: Contains .NET Core SDK and runtime files if you‚Äôve installed the .NET development platform.\n.ssh: Stores SSH configuration files and keys:\n\nauthorized_keys: Lists public keys that can authenticate to your account\nubuntu_pi_ecdsa & ubuntu_pi_ecdsa.pub: Your private and public ECDSA keys (More on this in the SSH Section)\nknown_hosts: Tracks hosts you‚Äôve connected to previously\nssh_config: Optional configuration file for SSH connections\n\n.sudo_as_admin_successful: A marker file created when you successfully use sudo. Its presence suppresses the ‚Äúsudo capabilities‚Äù message when opening a terminal.\n.vscode-server: Created when you connect to your server using Visual Studio Code‚Äôs remote development feature. Contains the VS Code server components. (More on this in the SSH Section)\n.wget-hsts: Wget‚Äôs HTTP Strict Transport Security database. Tracks websites that require secure (HTTPS) connections.\n\n\n\nThe Root Filesystem\nThe Linux filesystem follows the Filesystem Hierarchy Standard (FHS), which defines the directory structure and contents of Unix-like systems. Here‚Äôs a breakdown of key directories:\n\n/bin: Contains essential command binaries (programs) needed for basic system functionality. These commands are available to all users and are required during boot or in single-user mode.\n\nHistorical note: Originally separated from /usr/bin because early Unix systems had limited disk space on the root partition.\n\n/boot: Contains boot loader files including the Linux kernel, initial RAM disk (initrd), and bootloader configuration (GRUB).\n\nFor Raspberry Pi, this contains the firmware and various boot-related files.\n\n/dev: Contains device files that represent hardware devices. These are not actual files but interfaces to device drivers in the kernel.\n\nExample: /dev/sda represents the first SATA disk.\n\n/etc: Contains system-wide configuration files. The name originated from ‚Äúet cetera‚Äù but is now often interpreted as ‚ÄúEditable Text Configuration.‚Äù Critical files include:\n\n/etc/fstab: Filesystem mount configuration\n/etc/passwd: User account information\n/etc/ssh/sshd_config: SSH server configuration\n\n/home: Contains user home directories where personal files and user-specific configuration files are stored.\n/lib: Contains essential shared libraries needed by programs in /bin and system boot.\n\nOn modern 64-bit systems, you‚Äôll also find /lib64 for 64-bit libraries.\n\n/media: Mount point for removable media like USB drives and DVDs.\n/mnt: Temporarily mounted filesystems. This is often used as a manual mount point.\n/opt: Optional application software packages. Used for third-party applications that don‚Äôt follow the standard file system layout.\n/proc: A virtual filesystem providing process and kernel information. Files here don‚Äôt exist on disk but represent system state.\n\nExample: /proc/cpuinfo shows CPU information.\n\n/root: Home directory for the root user. Separated from /home to ensure it‚Äôs available even if /home is on a separate partition.\n/run: Runtime data for processes started since last boot. This is a tmpfs (memory-based) filesystem.\n/sbin: System binaries for system administration tasks, typically only usable by the root user.\n/srv: Data for services provided by the system, such as web or FTP servers.\n/snap: The /snap directory is, by default, where the files and folders from installed snap packages appear on your system.\n/sys: Another virtual filesystem exposing device and driver information from the kernel. Provides a more structured view than /proc.\n/tmp: Temporary files that may be cleared on reboot. Applications should not rely on data here persisting.\n/usr: Contains the majority of user utilities and applications. Originally stood for ‚ÄúUnix System Resources.‚Äù\n\n/usr/bin: User commands\n/usr/lib: Libraries for the commands in /usr/bin\n/usr/local: Locally installed software\n/usr/share: Architecture-independent data\n\n/var: Variable data files that change during normal operation:\n\n/var/log: System log files\n/var/mail: Mail spool\n/var/cache: Application cache data\n/var/spool: Spool for tasks waiting to be processed (print queues, outgoing mail)\n\n\nThe core philosophy behind this structure separates:\n\nStatic vs.¬†variable content\nShareable vs.¬†non-shareable files\nEssential vs.¬†non-essential components\n\nUnderstanding this hierarchy helps you navigate any Linux system and locate important files intuitively.\n\n\nUser and Group Permissions\n\nBasics\nLinux inherits its permission system from Unix, providing a robust framework for controlling access to files and directories. Understanding this system is essential for maintaining security and proper functionality of your Raspberry Pi server, as well as any Linux based system. At its core, the Linux permission model operates with three basic permission types applied to three different categories of users:\n\nPermission Types:\n\nRead (r): Allows viewing file contents or listing directory contents\nWrite (w): Allows modifying file contents or creating/deleting files within a directory\nExecute (x): Allows running a file as a program or accessing files within a directory\n\nUser Categories:\n\nOwner (u): The user who owns the file or directory\nGroup (g): Users who belong to the file‚Äôs assigned group\nOthers (o): All other users on the system\n\n\nIt‚Äôs not only important to know how to set permissions, but also how to view existing ones. When you run ls -l in a directory, you‚Äôll see a detailed listing including permission information.\n-rw-r--r-- 1 chris chris 1234 May 6 14:32 example.txt\nIn this example, the owner can read and write, while group members and others can only read. The first string of characters -rw-r‚Äìr‚Äì represents the permissions:\n\nFirst character: File type (- for regular file, d for directory, l for symbolic link)\nCharacters 2-4: Owner permissions (rw-)\nCharacters 5-7: Group permissions (r‚Äì)\nCharacters 8-10: Others permissions (r‚Äì)\n\n\n\nchmod\nThe chmod command modifies file permissions in Linux. You can use it in two ways: symbolic mode or numeric (octal) mode.\nSymbolic mode uses letters to represent permission categories (u, g, o, a) and permissions (r, w, x):\n# Give the owner execute permission\nchmod u+x script.sh\n\n# Remove write permission from group and others\nchmod go-w important_file.txt\n\n# Set read and execute for everyone (a=all users)\nchmod a=rx application\n\n# Add write permission for owner and group\nchmod ug+w shared_document.txt\nEach symbol has a specific meaning:\n\nu: Owner permissions\ng: Group permissions\no: Other user permissions\na: All permissions\n+: Add permissions\n-: Remove permissions\n=: Set exact permissions\n\nOctal mode represents permissions as a 3-digit number, where each digit represents the permissions for owner, group, and others:\n\nRead (r) = 4\nWrite (w) = 2\nExecute (x) = 1\n\nPermissions are calculated by adding these values:\n\n7 (4+2+1) = Read, write, and execute\n6 (4+2) = Read and write\n5 (4+1) = Read and execute\n4 (4) = Read only\n0 = No permissions\n\n# rwxr-xr-x (755): Owner can read, write, execute; group and others can read and execute\nchmod 755 script.sh\n\n# rw-r--r-- (644): Owner can read and write; group and others can read only\nchmod 644 document.txt\n\n# rwx------ (700): Owner has all permissions; group and others have none\nchmod 700 private_directory\nBeyond the basic rwx permissions, Linux has three special permission bits:\n\nsetuid (4000): When set on an executable file, it runs with the privileges of the file owner instead of the user executing it.\nsetgid (2000): Similar to setuid but for group permissions. When set on a directory, new files created within inherit the directory‚Äôs group.\nsticky bit (1000): When set on a directory, files can only be deleted by their owner, the directory owner, or root (commonly used for /tmp).\n\n\n\nchown\nThe chown command changes the owner and/or group of files and directories. Do not change ownership in the root directories because many require specific ownership/permissions to function properly.\n# Change the owner of a file\nsudo chown chris file.txt\n\n# Change both owner and group\nsudo chown chris:developers project_files\n\n# Change only the group\nsudo chown :developers shared_documents\n\n# Change recursively for a directory and all its contents\nsudo chown -R chris:chris /home/chris/projects\nThe flags do the following:\n\n-R, --recursive: Change ownership recursively\n-c, --changes: Report only when a change is made\n-f, --silent: Suppress most error messages\n-v, --verbose: Output a diagnostic for every file processed\n\n# Verbose recursive ownership change\nsudo chown -Rv chris:developers /opt/application\n\n\nUnderstanding Permissions\nLinux manages permissions through users and groups:\n\nEach user has a unique User ID (UID)\nEach group has a unique Group ID (GID)\nUsers can belong to multiple groups\nThe first 1000 UIDs/GIDs are typically reserved for system users/groups\n\nImportant files include:\n\n/etc/passwd: Contains basic user account information\n\nFields: username, password placeholder, UID, primary GID, full name, home directory, login shell\n\n\nchris:x:1000:1000:Chris Kornaros:/home/chris:/bin/bash\n\n/etc/shadow: Contains encrypted passwords and password policy information\n\nFields: username, encrypted password, days since epoch of last change, min days between changes, max days password valid, warning days, inactive days, expiration date\n\n\nchris:$6$xyz...hash:19000:0:99999:7:::\n\n/etc/group: Contains group definitions\n\nFields: group name, password placeholder, GID, comma-separated list of members\n\n\ndevelopers:x:1001:chris,bob,alice\nThere are two categories of groups you should understand, Primary and Supplementary:\n\nPrimary Group: Set in /etc/passwd, used as the default group for new files\nSupplementary Groups: Additional groups a user belongs to, defined in /etc/group\n\nYou can view your current user‚Äôs groups with the groups command, or view them for a specific user with groups chris (replace chris with the name of the user). That being said, directory permissions differ slightly from file permissions:\n\nRead (r): List directory contents\nWrite (w): Create, delete, or rename files within the directory\nExecute (x): Access files within the directory (crucial for navigation)\n\n\n\n\n\n\n\nTip\n\n\n\nA common confusion: You may have read permission for a file but not execute permission for its parent directory, preventing access.\n\n\nThe umask (user mask) determines the default permissions for newly created files and directories:\n\nDefault for files: 666 (rw-rw-rw-)\nDefault for directories: 777 (rwxrwxrwx)\nThe umask is subtracted from these defaults, for example, a umask of 022 results in:\n\nFiles: 644 (rw-r‚Äìr‚Äì)\nDirectories: 755 (rwxr-xr-x)\n\n\n# View current umask (in octal)\numask\n\n# Set a new umask\numask 027  # More restrictive: owner full access, group read/execute, others no access\nTraditional Unix permissions have limitations regarding inheritance: new files don‚Äôt inherit permissions from parent directories and changing permissions doesn‚Äôt affect existing files. Modern solutions, however, include: the setgid bit on directories for group inheritance and ACLs (Access Control Lists) with default entries that apply to new files. To setup a collaborative directory with proper permissions:\n# Create a shared directory for developers\nsudo mkdir /opt/projects\nsudo chown chris:developers /opt/projects\nsudo chmod 2775 /opt/projects  # setgid bit ensures new files get 'developers' group\n\n\nAdvanced Permission Concepts\nLike I previously wrote, part of the modern permission solutions include ACLs, or Access Control Lists. ACLs extend the traditional permission model to allow specifying permissions for multiple users and groups. When ACLs are in use, ls -l will show a + after the permission bits. Here‚Äôs a basic example of how to create and manage an ACL:\n# Install ACL support (if not already installed)\nsudo apt install acl\n\n# Set an ACL allowing a specific user read access\nsetfacl -m u:chris:r file.txt\n\n# Set an ACL allowing a specific group write access\nsetfacl -m g:developers:rw file.txt\n\n# Set default ACLs on a directory (inherited by new files)\nsetfacl -d -m g:developers:rw directory/\n\n# View ACLs on a file\ngetfacl file.txt\n-rw-rw-r--+ 1 chris developers 1234 May 6 14:32 file.txt\nA few final notes on permissions that are especially relevant for this project, becaue you‚Äôll be working with external storage devices:\n\nNot all filesystems support the same permission features:\n\next4: Full support for traditional permissions, ACLs, and extended attributes\nNTFS (via NTFS-3G): Simulated Unix permissions, basic ACL support\nFAT32: No native permission support (mounted with fixed permissions)\nexFAT: No native permission support\n\nCommon Permission Patterns:\n\nConfiguration Files: 644 or 640 (owner can edit, restricted read access)\nProgram Binaries: 755 (everyone can execute, only owner can modify)\nWeb Content: 644 for regular files, 755 for directories\nSSH Keys: 600 for private keys (owner only), 644 for public keys\nScripts: 700 or 750 (executable by owner or group)"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-monitor_maintain",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-monitor_maintain",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Monitoring and Maintenance",
    "text": "Monitoring and Maintenance"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-docker",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-docker",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Docker",
    "text": "Docker"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-kubernetes",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-kubernetes",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Kubernetes",
    "text": "Kubernetes"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-remote_vs_code",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-remote_vs_code",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Remote Development with VS Code",
    "text": "Remote Development with VS Code"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-change_boot_media",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-change_boot_media",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Changing Your Boot Media Device",
    "text": "Changing Your Boot Media Device\nWhen you‚Äôre moving from one boot device to another on a Raspberry Pi, you‚Äôre essentially telling the system where to find its operating system files. The Raspberry Pi‚Äôs bootloader looks for specific files on a FAT32 partition to begin the boot process, then loads the main operating system from the root partition. This transition is a fundamental server administration skill that every system administrator should understand.\nThe transition involves several critical steps.\n\nFirst, we prepare the SSD with a fresh operating system installation.\nThen we properly shut down the system to ensure no data corruption occurs.\nNext, we physically reconfigure the hardware to make the SSD the primary boot device.\nFinally, we verify that everything works correctly and remove the temporary boot media.\n\nThink of this process like moving into a new house. You‚Äôve already built the structure (partitioning), but now you need to move all your belongings (the operating system) into it, update your mailing address (boot configuration), and ensure everything works in the new location.\n\nBoot Configuration Transition Process\nThe good news, we‚Äôve already done most of the work required and what we haven‚Äôt is going to be something we did for the original boot device. We‚Äôll first run a few commands from the server, do one thing on a different computer, and then we‚Äôll be back to the server.\n\nFirst, you‚Äôll need to flash a fresh Ubuntu Server LTS image onto your newly partitioned SSD. This process will write the operating system files to the appropriate partitions you created. This should be done on your Raspberry Pi server.\n\n# Before removing the SSD, check its device identifier one more time\nsudo lsblk -o NAME,SIZE,FSTYPE,LABEL,TYPE,MOUNTPOINT\nNow you‚Äôll need to use RPi Imager on your MacBook:\n\nConnect the SSD to your MacBook\nOpen Raspberry Pi Imager\nSelect Ubuntu Server LTS (same version you used before)\nSelect your SSD as the storage device\nUse the exact same advanced settings you used when first setting up your Raspberry Pi:\n\nSame username (chris)\nSame password\nSame SSH key settings\nSame WiFi credentials\n\n\nThe reason we use identical settings is to reduce the amount of changes needed to replicate the environment we previously configured. Your SSH keys, user permissions, and network configurations will all match what we know works for an initial boot, eliminating the need to reconfigure everything from scratch.\n\nBefore making any hardware changes, we need to ensure all data is written to disk and all processes are safely terminated. This prevents corruption and data loss during the transition.\n\n# Save any unsaved work and exit all applications\n# Ensure no important processes are running\n\n# Sync all file system buffers to disk\nsudo sync\n\n# Check for any open files on your current boot device (thumb drive)\nsudo lsof | grep -E '^[^ ]+ +[^ ]+ +[^ ]+ +[^ ]+ +[^ ]+ +[^ ]+ +FIFO'\n\n# Display active processes to ensure nothing critical is running\nps aux | grep -v '\\['\n\nThe sync command forces all pending disk writes to complete immediately. This is crucial because Linux uses write caching for performance, meaning data might still be in memory waiting to be written.\nThe lsof command lists open files, helping you identify any processes that might be accessing the current boot device.\nThe ps aux command shows all active processes, giving you a final check that nothing important is running.\n\n\nA proper shutdown sequence ensures all services stop gracefully and file systems are cleanly unmounted:\n\n# Perform a clean system shutdown\nsudo shutdown -h now\nThe shutdown command initiates a clean shutdown sequence. The -h flag tells the system to halt (power off) after shutdown. This is important because the system may not power off, especially on older systems‚Äîit could just bring it to single-user mode or runlevel 1, depending on configuration. While now indicates the shutdown should happen immediately. This command:\n\nSends a termination signal to all running processes\nAllows services to save their state and clean up\nUnmounts all filesystems in the correct order\nFinally powers down the system\n\n\nNow comes the physical hardware transition. With your Raspberry Pi powered off:\n\n\nRemove the thumb drive (current boot device)\nConnect the SSD via USB to the Raspberry Pi\nSet aside the microSD card (backup device)\nEnsure all connections are secure\n\nThis step is straightforward but crucial. The Raspberry Pi will attempt to boot from the first bootable device it finds. By removing the thumb drive and connecting the SSD, we‚Äôre ensuring the Pi finds and uses the SSD as its boot device. We do not need to remove the SD card used for backups, because it never had an OS flashed onto it. The card‚Äôs file system just provides extra memory, instead of being an extra operating system.\n\nPower on your Raspberry Pi and observe the boot process.\n\nMake sure to connect your monitor and keyboard before the boto begins. The boot process should proceed similarly to your initial setup. The Raspberry Pi firmware reads the configuration from the SSD‚Äôs boot partition, loads the kernel, and then mounts the root filesystem. If everything works correctly, you should see the familiar Ubuntu Server boot messages and eventually reach a login prompt.\nGood news, you won‚Äôt need to do this again (for this server). After this, you‚Äôll have your core system, memory, and configurations complete.\n\nAfter booting, you‚Äôll need to log in directly to the Raspberry Pi using a keyboard and monitor. This is because the SSH service may not be running automatically on the fresh installation.\n\n\nConnect a keyboard and monitor to your Raspberry Pi\nLog in with your username and password\nStart the SSH service manually:\n\n# Start the SSH service to enable remote connections\nsudo systemctl start ssh\n\n# Verify the service is running\nsudo systemctl status ssh\n\n# Test a remote connection from your client machine\n# Test SSH connection (initially with password authentication)\nssh chris@192.168.1.151\n\nYou‚Äôll need to specify the local IP because none of the ssh configs are updated yet\n\nYour nonstandard port, 45000 in this guide\nThe local IP may be a different one in the 192.168.0.0/16 range (which is reserved for local IPs)\n\nIf this is working and you can remotely connect, then we can move on to the next step\n\n\nWith SSH access established, transfer your configuration backup and restore scripts from your computer to the Raspberry Pi:\n\nFirst, make sure to create the directory, because it won‚Äôt exist on a fresh boot. The, from your client computer, a MacBook in my case, you‚Äôll run the rsync commands to move the master backup and scripts. Notice that you can use either the ~ shortcut to denote your home directory, or write the path explicitly. We‚Äôll also need to move the master backup to the proper backup directory, so the restore script works properly.\n# On the Raspberry Pi, create the backup directory structure\nsudo mkdir -p /mnt/backups/configs/\n\n# From your MacBook, copy your backup files to the Pi\nrsync -avz ~/path/to/backups/configs/master chris@ubuntu-pi-server:/home/chris/\n\n# Copy your restore script to the Pi\nrsync -avz ~/path/to/scripts/config_restore.sh chris@ubuntu-pi-server:~/scripts/\n\n# Move the master backup directory to the correct location\nsudo mv /home/chris/master /mnt/backups/configs/\n\nRun the Configuration Restore Script\n\n# Make sure the script is executable\nchmod +x ~/scripts/config_restore.sh\n\n# Run the restore script\nsudo ./scripts/config_restore.sh\n\nVerify the System\n\n# Verify the root filesystem device\ndf -h /\n\n# Verify the microSD card is mounted properly\ndf -h /mnt/backups\n\n# Check that your backup files are accessible\nls -la /mnt/backups/configs/\n\n# Verify your network settings\nip addr show\n\n# Check System configurations\nsudo systemctl status ssh\nsudo systemctl status ufw\nsudo systemctl status fail2ban\nsudo systemctl status systemd-networkd\nsudo systemctl status wpa_supplicant@wlan0.service\nThe df -h / command shows disk usage statistics for the root filesystem, including which device it‚Äôs mounted from. You should see /dev/sdb2 (or similar) listed as the root device, and /dev/sdb1 as the boot device, not the thumb drive identifier you used before. You‚Äôll see a similar output, just focused on your /backups directory when running the second command. The ls -la command shows you all of the contents of a directory, as well as the permissions. The other commands you should be familiar with by now.\n\n\nFinal Thoughts\nOur Ubuntu Raspberry Pi Server is now booting from the SSD, and all your previous configurations have been restored. You may notice that on reboot, running sudo lsblk will show your SSD under a different This configuration provides a solid foundation for the more advanced server features we‚Äôll implement in the next sections. By moving to SSD boot, we‚Äôve significantly improved our server‚Äôs performance profile. SSDs offer several advantages over traditional storage media:\n\nFaster boot times: Your Raspberry Pi will start much more quickly\nImproved I/O performance: Database operations, file access, and application loading will be noticeably faster\nBetter reliability: SSDs have no moving parts, making them more resilient to physical impacts\nLower power consumption: SSDs typically use less power than traditional hard drives\n\nThe transition to SSD boot also aligns with modern server practices, where solid-state storage is becoming the standard for production environments. This configuration will serve us well as we expand into containerization with Docker and orchestration with Kubernetes."
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-backups",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-backups",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Backups and Basic Automation",
    "text": "Backups and Basic Automation\nNow that we‚Äôve configured the basics, from permissions to networking and ssh to partitions, we‚Äôll want to save those changes in case something happens and to ensure a seamless transition to the SSD for boot media. You‚Äôve already seen some basic backups. The process is the same, essentially creating a folder and then putting a copy of the current file into it and maybe adding a .bak extension to make it clear this is a previous version. That being said, to go through and do this for each and every folder we‚Äôve made changes in is impractical now, let alone in the future when more complex configurations are done. So, in this section, we‚Äôll go over creating a basic script to backup all of our configs and automating the backups.\nFor this section, we‚Äôll use rsync because it provides several important advantages over simple copy commands, that you may remember from the section on ssh:\n\nIncremental backups that only transfer changed files\nPreservation of file permissions, ownership, and timestamps\nBuilt-in compression for efficient transfers\nDetailed progress information and logging\nThe ability to resume interrupted transfers\n\nBefore we start, make sure you have:\n\nA mounted backup drive at /mnt/backups/\n\n\nSetting Up the Backup Directory\nFirst, in case you didn‚Äôt do this earlier, we‚Äôll prepare the backup directory structure and set appropriate permissions:\n# Create backup directories if they don't exist\nsudo mkdir -p /mnt/backups/configs\nsudo mkdir -p /mnt/backups/system\n\n# Change ownership to your user (replace 'chris' with your username)\nsudo chown -R chris:chris /mnt/backups\n\n# Set appropriate permissions\nsudo chmod -R 700 /mnt/backups  # Only owner can read/write/execute\n\n\nConfiguration Files Backup\nWhile it‚Äôs definitely beneficial to have a local copy of your backups to easily roll back changes, it isn‚Äôt the most secure solution to have all of your information in one place. Furthermore, the SSD is partitioned, but it doesn‚Äôt currently have an OS or any files stored. So, now it‚Äôs time to take advantage of the microSD card we formatted earlier.\nFor the purpose of this guide, I‚Äôll be showing you how to use rsync for a remote transfer to your client machine and how to automatically store backups on the SD card. The script we‚Äôll use saves all of the key user and system information (things like passwords), as well as the configuration changes we made. Additionally, as long as your SD card is mounted to /mnt/backups the backup will automatically be saved to the external memory. The following script demonstrates how to perform the backup while preserving all file attributes:\n#!/bin/bash\n# Using the {} around DATEYMD in the file path ensure it's specified as the variable's value, and the subsequent parts are not included\n\nDATEYMD=$(date +%Y%m%d)\nBACKUP_DIR=\"/mnt/backups/configs/$DATEYMD\"\nLOG_DIR=\"/mnt/backups/logs\"\nLOG_FILE=\"$LOG_DIR/${DATEYMD}_config_backup.log\"\n\n# Check if script is run as root\nif [ \"$(id -u)\" -ne 0 ]; then\n    echo \"Error: This script must be run as root. Try using sudo.\"\n    exit 1\nfi\n\n# Create backup directory\nmkdir -p \"$BACKUP_DIR\"\n\n{\n    # 1. User and Group Information\n    echo \"Backing up User and Group configuration...\"\n    sudo rsync -aAXv /etc/passwd \"$BACKUP_DIR/passwd.bak\"\n    sudo rsync -aAXv /etc/group \"$BACKUP_DIR/group.bak\"\n    sudo rsync -aAXv /etc/shadow \"$BACKUP_DIR/shadow.bak\"\n    sudo rsync -aAXv /etc/gshadow \"$BACKUP_DIR/gshadow.bak\"\n\n    # 2. Crontab Configurations\n    echo \"Backing up Crontab configuration...\"\n    sudo rsync -aAXv /etc/crontab \"$BACKUP_DIR/\"\n    sudo rsync -aAXv /var/spool/cron/crontabs/. \"$BACKUP_DIR/crontabs/\"\n\n     # 3. SSH Configuration\n    echo \"Backing up SSH configuration...\"\n    sudo rsync -aAXv /etc/ssh/. \"$BACKUP_DIR/ssh/\"\n    \n    # Create user_ssh directory\n    mkdir -p \"$BACKUP_DIR/user_ssh\"\n    \n    # Copy SSH user configuration with explicit handling of authorized_keys\n    rsync -aAXv /home/chris/.ssh/config \"$BACKUP_DIR/user_ssh/\" 2&gt;/dev/null || true\n    rsync -aAXv /home/chris/.ssh/id_* \"$BACKUP_DIR/user_ssh/\" 2&gt;/dev/null || true\n    rsync -aAXv /home/chris/.ssh/known_hosts \"$BACKUP_DIR/user_ssh/\" 2&gt;/dev/null || true\n    \n    # Explicitly backup authorized_keys if it exists\n    if [ -f /home/chris/.ssh/authorized_keys ]; then\n        echo \"Backing up authorized_keys file...\"\n        rsync -aAXv /home/chris/.ssh/authorized_keys \"$BACKUP_DIR/user_ssh/\"\n    else\n        echo \"No authorized_keys file found in /home/chris/.ssh/\"\n    fi\n\n    # 4. UFW (Uncomplicated Firewall) Configuration\n    echo \"Backing up ufw configuration...\"\n    sudo rsync -aAXv /etc/ufw/. \"$BACKUP_DIR/ufw/\"\n    sudo ufw status verbose &gt; \"$BACKUP_DIR/ufw_rules.txt\"\n\n    # 5. Fail2Ban Configuration\n    echo \"Backing up fail2ban configuration...\"\n    sudo rsync -aAXv /etc/fail2ban/. \"$BACKUP_DIR/fail2ban/\"\n\n    # 6. Network Configuration\n    echo \"Backing up Network configuration...\"\n    sudo rsync -aAXv /etc/network/. \"$BACKUP_DIR/network/\"\n    sudo rsync -aAXv /etc/systemd/network/. \"$BACKUP_DIR/systemd/network/\"\n    sudo rsync -aAXv /etc/netplan/. \"$BACKUP_DIR/netplan/\"\n    sudo rsync -aAXv /etc/hosts \"$BACKUP_DIR/hosts.bak\"\n    sudo rsync -aAXv /etc/hostname \"$BACKUP_DIR/hostname.bak\"\n    sudo rsync -aAXv /etc/resolv.conf \"$BACKUP_DIR/resolv.conf.bak\"\n    sudo rsync -aAXv /etc/wpa_supplicant/. \"$BACKUP_DIR/wpa_supplicant/\"\n\n    # 7. Systemd Services and Timers\n    echo \"Backing up Systemd Timers configuration...\"\n    sudo rsync -aAXv /etc/systemd/system/. \"$BACKUP_DIR/systemd/\"\n\n    # 8. Logrotate Configuration\n    echo \"Backing up Logrotate configuration...\"\n    sudo rsync -aAXv /etc/logrotate.conf \"$BACKUP_DIR/logrotate.conf.bak\"\n    sudo rsync -aAXv /etc/logrotate.d/. \"$BACKUP_DIR/logrotate.d/\"\n\n    # 9. Timezone and Locale\n    echo \"Backing up Timezone and Locale configuration...\"\n    sudo rsync -aAXv /etc/timezone \"$BACKUP_DIR/timezone.bak\"\n    sudo rsync -aAXv /etc/localtime \"$BACKUP_DIR/localtime.bak\"\n    sudo rsync -aAXv /etc/default/locale \"$BACKUP_DIR/locale.bak\"\n\n    # 10. Keyboard Configuration\n    echo \"Backing up Keyboard configuration...\"\n    sudo rsync -aAXv /etc/default/keyboard \"$BACKUP_DIR/keyboard.bak\"\n\n    # 11. Filesystem Table (fstab)\n    echo \"Backing up filesystem table (fstab)...\"\n    sudo rsync -aAXv /etc/fstab \"$BACKUP_DIR/fstab.bak\"\n\n    # Set appropriate permissions\n    echo \"Configuring backup directory permissions...\"\n    sudo chown -R chris:chris \"$BACKUP_DIR\"\n    sudo chmod -R 600 \"$BACKUP_DIR\"\n\n    echo \"Configuration backup completed at: $BACKUP_DIR\"\n\n} &gt; \"$LOG_FILE\" 2&gt;&1\n\necho \"Logs available at: $LOG_FILE\"\n# Make the script executable\nchmod +x /scripts/config_backup.sh\n\n# Run the script\n./scripts/config_backup.sh\nThe rsync commands use several important options:\n\n-a: Archive mode, preserves almost everything\n-A: Preserve ACLs (Access Control Lists)\n-X: Preserve extended attributes\n-v: Verbose output\n--one-file-system: Don‚Äôt cross filesystem boundaries\n--hard-links: Preserve hard links\n--exclude: Skip specified directories\n\n\nRemote Transfers of Backups\nWe covered rsync vs.¬†scp earlier, so remember that rsync is specifically designed for copying and transferring files, so it offers more sophisticated file synchronization capabilities than basic tools like SCP. If you need a refresher, run the following command from your client machine (laptop), just change the paths to match what your system uses.\nrsync -avz --partial --progress --update chris@ubuntu-pi-server:/mnt/backups/configs/master/ ~/Documents/raspberry_pi_server/backups/configs/master\nThe flags do the following:\n\n-a: Archive mode, which preserves permissions, timestamps, symbolic links, etc.\n-v: Verbose output, showing what files are being transferred\n-z: Compress data during transfer for faster transmission\n--partial: Keep partially transferred files, allowing you to resume interrupted transfers\n--progress: Show progress during transfer\n--update: Skip files that are newer on the receiver (only transfer if source is newer)\n\n\n\n\nRestoring from Backup\nNow that we‚Äôve backed up all of the configurations we‚Äôve made so far, it‚Äôs time to create a script that restores that backup. At the time of writing this, I‚Äôve probably had to reflash a fresh image and reconfigure things between 10 and 20 times. I‚Äôm so good at it, that I can now do it all in under 20 minutes. That being said, it‚Äôs much easier to do when you can just run a script that takes all of the configurations from your Master backup and overwrites the defaults.\nFirst, here are some important things to remember:\n\nThe --delete option during restore will remove files at the destination that don‚Äôt exist in the backup. Use with caution.\nConsider using rsync‚Äôs --dry-run option to test backups and restores without making changes.\nThe backup includes sensitive system files. Store it securely and restrict access.\nConsider encrypting the backup directory for additional security.\nTest the restore process in a safe environment before using in production.\n\nAfter writing this, you can test the script by running it on your server with the boot media you‚Äôve been using (not the SSD)‚Äì just make sure you save the master/ backup and any scripts/configs externally first. You‚Äôll know this succeeds, if nothing changes after the reboot. When you‚Äôve verified that‚Äôs done, we‚Äôll shutdown the server and make the SSD the boot media. For now, let‚Äôs write the config_restore script.\n#!/bin/bash\n\n# Simple Configuration Restoration Script for Ubuntu Pi Server\nBACKUP_DIR=${1:-\"/mnt/backups/configs/master\"}\n\n# Check if script is run as root\nif [ \"$(id -u)\" -ne 0 ]; then\n    echo \"Error: This script must be run as root. Try using sudo.\"\n    exit 1\nfi\n\n# Check if the backup directory exists\nif [ ! -d \"$BACKUP_DIR\" ]; then\n    echo \"Error: Backup directory not found: $BACKUP_DIR\"\n    echo \"Usage: $0 [backup_directory_path]\"\n    exit 1\nfi\n\n# Begin restoration process\necho \"Starting configuration restoration from $BACKUP_DIR...\"\necho \"This will overwrite current system configurations with those from the backup.\"\nread -p \"Continue with restoration? (y/n): \" CONFIRM\nif [[ \"$CONFIRM\" != \"y\" && \"$CONFIRM\" != \"Y\" ]]; then\n    echo \"Restoration aborted by user.\"\n    exit 0\nfi\n\n# 1. Restore User and Group Information\necho \"Restoring user and group information...\"\n[ -f \"$BACKUP_DIR/passwd.bak\" ] && rsync -a \"$BACKUP_DIR/passwd.bak\" /etc/passwd\n[ -f \"$BACKUP_DIR/group.bak\" ] && rsync -a \"$BACKUP_DIR/group.bak\" /etc/group\n[ -f \"$BACKUP_DIR/shadow.bak\" ] && rsync -a \"$BACKUP_DIR/shadow.bak\" /etc/shadow\n[ -f \"$BACKUP_DIR/gshadow.bak\" ] && rsync -a \"$BACKUP_DIR/gshadow.bak\" /etc/gshadow\n\n# Explicitly Set Permissions for Critical System Files\necho \"Fixing critical system file permissions...\"\nchmod 644 /etc/passwd   # Read-write for root, read-only for everyone else\nchmod 644 /etc/group    # Read-write for root, read-only for everyone else  \nchmod 640 /etc/shadow   # Read-write for root, read-only for shadow group\nchmod 640 /etc/gshadow  # Read-write for root, read-only for shadow group\n\n# 2. Restore SSH Configuration\necho \"Restoring SSH configuration...\"\n[ -d \"$BACKUP_DIR/ssh\" ] && rsync -a \"$BACKUP_DIR/ssh/\" /etc/ssh/\nchmod 600 /etc/ssh/ssh_host_*_key 2&gt;/dev/null || true\nchmod 644 /etc/ssh/ssh_host_*_key.pub 2&gt;/dev/null || true\n\n# 3. Restore UFW Configuration\necho \"Restoring UFW configuration...\"\nif [ -d \"$BACKUP_DIR/ufw\" ]; then\n    apt-get install -y ufw &gt;/dev/null\n    rsync -a \"$BACKUP_DIR/ufw/\" /etc/ufw/\nfi\n\n# 4. Restore Fail2Ban Configuration\necho \"Restoring Fail2Ban configuration...\"\nif [ -d \"$BACKUP_DIR/fail2ban\" ]; then\n    apt-get install -y fail2ban &gt;/dev/null\n    rsync -a \"$BACKUP_DIR/fail2ban/\" /etc/fail2ban/\nfi\n\n# 5. Restore Network Configuration\necho \"Restoring network configuration...\"\n[ -d \"$BACKUP_DIR/network\" ] && rsync -a \"$BACKUP_DIR/network/\" /etc/network/\n[ -d \"$BACKUP_DIR/systemd/network\" ] && rsync -a \"$BACKUP_DIR/systemd/network/\" /etc/systemd/network/\n[ -d \"$BACKUP_DIR/netplan\" ] && rsync -a \"$BACKUP_DIR/netplan/\" /etc/netplan/\n[ -f \"$BACKUP_DIR/hosts.bak\" ] && rsync -a \"$BACKUP_DIR/hosts.bak\" /etc/hosts\n[ -f \"$BACKUP_DIR/hostname.bak\" ] && rsync -a \"$BACKUP_DIR/hostname.bak\" /etc/hostname\n[ -f \"$BACKUP_DIR/resolv.conf.bak\" ] && rsync -a \"$BACKUP_DIR/resolv.conf.bak\" /etc/resolv.conf\n[ -d \"$BACKUP_DIR/wpa_supplicant\" ] && rsync -a \"$BACKUP_DIR/wpa_supplicant/\" /etc/wpa_supplicant/\n\n# 6. Restore Filesystem Table (fstab)\necho \"Restoring filesystem table (fstab)...\"\n[ -f \"$BACKUP_DIR/fstab.bak\" ] && rsync -a \"$BACKUP_DIR/fstab.bak\" /etc/fstab\n\n# 7. Restore Package List\n#echo \"Reinstalling packages from backup...\"\n#if [ -f \"$BACKUP_DIR/package_list.txt\" ]; then\n#    apt-get update && apt-get install -y dselect\n#    dpkg --set-selections &lt; \"$BACKUP_DIR/package_list.txt\"\n#    apt-get dselect-upgrade -y\n#fi\n\n# Restart services\nsystemctl restart systemd-networkd wpa_supplicant@wlan0.service ssh ufw fail2ban \n\necho \"Configuration restoration completed. A system reboot is recommended.\"\nread -p \"Would you like to reboot now? (y/n): \" REBOOT\n[[ \"$REBOOT\" == \"y\" || \"$REBOOT\" == \"Y\" ]] && reboot\n\nexit 0\nYou probably have some questions about the script let me explain some of the decisions I made while doing some trial and error testing.\n\nOriginally, I had the backup directory as a value in the script call itself, now it just defaults to the master/ backup\n\nThis backup is one I know that works and is in the format I‚Äôm hoping\nEasier to have a standard version to reference than relying on monthly backups\n\nI had a lot of issues with incorrect permissions after restoring backups previously, so it needs to be run with sudo\nFirst, the user and group information is important, a lot of processes behind the scenes rely on these configurations\n\nPart of this, I added an explicit chmod call because after the reboot, I was getting an error with whoami\n\nThe command whoami returns which user you are/currently running commands as\nThe user and group info wasn‚Äôt exactly the same, it was leaving my user chris as UID 1000, but changing the group to 1003\nThe chmod call fixes that\nYou can use getent group | grep 'chris' to view all group IDs and assignments\n\n\nSecond, restoring the ssh configurations ensures security and remote connectivity\nThird, ufw increases your system security\nFourth, fail2ban does the same by improving security\nFifth, restoring the network configurations, originally, I had issues because networkd wasn‚Äôt included\n\nThis block ensures all of the systemd configurations are included\n\nSixth Package List TBD\nThen, the specific services we modified are all explicitly started\n\nWhile developing this, some of the services wouldn‚Äôt necessarily start, so I would run into network or ssh issues post-reboot\n\nFinally, the script asks you to reboot your system so all of the changes take affect\n\n# Make the script executable\nchmod +x /scripts/config_restore.sh\n\n# Run the script\nsudo ./scripts/config_restore.sh\n\n\nFinal Thoughts\nAt the time of writing this, I‚Äôve tested and revised everything up until this point. There are a few things I haven‚Äôt added yet, that I want to later, once it‚Äôs necessary.\n\nSaving the installed packages and their requirements, similar to a uv lockfile or .venv requirements.txt file.\n\nModify the restore script to restore packages to exactly where they should be\nNot necessary now, because only Traceroute, ufw, and fail2ban were installed; however, when other apps are added this will be helpful for tracking dependencies and versioning issues\n\nCreating a private repoistory for storing configuration files, scripts, etc. and then using that, rather than rsync with my laptop\nDo some more granular network configurations, inlcuding subnets and static IPs"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-partitions",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-partitions",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Partitions",
    "text": "Partitions\n\nPartitioning Basics\nPartitions are logical divisions of a physical storage device. Think of a storage device like a large piece of land, and partitions as fenced areas within that land dedicated to different purposes. Each partition appears to the operating system as a separate disk, even though physically they‚Äôre on the same device. Remember from the beginning of this guide, I‚Äôm currently using a Flash Drive for my primary memory and a microSD card for backups; however, the SSD is what I want to serve as the boot device. Once we complete the partitioning, we can flash the base image from RPi onto the SSD and then reboot, with the SSD as the boot device.\n\nSeparation of concerns: Isolate the operating system from user data, which improves security and simplifies backups\nPerformance optimization: Different filesystems can be used for different workloads\nMulti-boot capability: Install multiple operating systems on the same physical device\nData protection: Limiting the scope of filesystem corruption to a single partition\nResource management: Setting size limits for specific system functions\n\nFor our Raspberry Pi server, proper partitioning creates a solid foundation for everything else you‚Äôll build. We‚Äôll primarily use ext4 for Linux partitions and FAT32 for the microSD card that needs broader compatibility.\n\n\n\n\n\n\n\n\nFilesystem\nBest For\nFeatures\n\n\n\n\next4\nLinux\n\nJournaling\nLarge file support\nBackwards compatible\n\n\n\nFAT32\nCross-platform compatibility\n\nWorks with virtually all operating systems\nLimited to 4GB Files\n\n\n\nexFAT\nModern cross-platform\n\nSupports large files\nNo built-in journaling\n\n\n\nNTFS\nWindows compatibility\n\nJournaling\nPermissions\nCompression\n\n\n\nBtrfs\nAdvanced Linux systems\n\nSnapshots\nChecksums\nCompression\n\n\n\n\nFinally, let‚Äôs cover some important terms:\n\nPartition Table: A data structure on a disk that describes how the disk is divided\n\nMBR (Master Boot Record): Traditional partition scheme limited to 2TB drives and 4 primary partitions\nGPT (GUID Partition Table): Modern scheme supporting larger drives and more partitions\n\nPartition Types:\n\nPrimary: Can be bootable and hold an operating system\nExtended: Acts as a container for logical partitions (MBR only)\nLogical: Created within an extended partition (MBR only)\n\nFilesystem: The method used to organize and store data within a partition\n\nCommon Linux filesystems: ext4, Btrfs\nCross-platform filesystems: FAT32, exFAT\n\n\n\n\nPartitioning Tools\nSeveral command-line tools are available for disk partitioning on Linux. Each has strengths for different scenarios:\n\n\n\n\n\n\n\n\n\nTool\nStrengths\nLimitations\nBest For\n\n\nfdisk\n\nSimple interface\nWidely available\n\n\nLimited GPT support in older versions\n\n\nBasic partitioning tasks\n\n\n\nparted\n\nFull GPT support\nHandles large drives\n\n\nMore complex syntax\n\n\nAdvanced partitioning needs\n\n\n\ngdisk\n\nGPT focused\nSimilar to fdisk\n\n\nLess common on minimal installations\n\n\nGPT-specific operations\n\n\n\nsfdisk\n\nScriptable for automation\n\n\nLess user-friendly\n\n\nAutomated deployments\n\n\n\n\nFor this project, and after doing some research, I chose parted for both the microSD card and SSD partitioning because:\n\nIt fully supports both MBR and GPT partition tables\nIt can handle drives larger than 2TB (relevant for the SSD)\nIt provides a more consistent interface across different partition table types\nIt supports both interactive and command-line usage\nIt‚Äôs included in most Ubuntu installations\n\n\n\nPartitioning a MicroSD Card for Backups\nLet‚Äôs partition our microSD card to serve as backup media. You can get great quality cards from Amazon Basics that are perfect for this use case. We‚Äôll use a simple, effective partition scheme. Before we dive into the actual commands, it‚Äôs important to remember that you can‚Äôt modify the memory of the active primary drive. Meaning, that you‚Äôll need to use an SSD or thumb drive as the boot media while you modify the SD card. Similarly, you‚Äôll need to use a different piece of boot media (you could use the micro SD) when partitioning the SSD.\nNow, let‚Äôs walk through this step-by-step:\n\nIdentify the device name of the microSD. Your microSD card will typically appear as something like /dev/mmcblk0 (what mine showed as) or /dev/sdX (where X is a letter like a, b, c). This command lists block devices with key information:\n\nsudo lsblk -o NAME,SIZE,FSTYPE,TYPE,MOUNTPOINT\nThe lsblk command lists all block devices, which includes your storage devices. The -o flag specifies which columns to display in the output. This final check ensures you‚Äôre working with the correct device and helps you confirm the partition structure you just created.\n\nNAME: Device identifier\nSIZE: Storage capacity\nFSTYPE: Current filesystem type\nTYPE: Whether it‚Äôs a disk or partition\nMOUNTPOINT: Where it‚Äôs currently mounted (if applicable)\n\n\nFor a backup microSD card, we‚Äôll use a simple partition layout with a single partition using ext4 filesystem, which provides good performance and Linux compatibility.\n\n# Start parted on the microSD card (replace /dev/mmcblk0 with your device)\nsudo parted /dev/mmcblk0\n\n# View the partition table for a specific device, or all\nprint mmcblk0\nprint all\n\n# Inside parted, create a new GPT partition table\n&gt; (parted) mklabel gpt\nWarning: The existing disk label on /dev/mmcblk0 will be destroyed and all data on this disk will be lost. Do you want to continue?\nYes/No? Yes\n\n# Create a single partition using the entire card\n&gt; (parted) mkpart primary ext4 0% 100%\n\n# Set a name for easy identification\n&gt; (parted) name 1 backups\n\n# Verify the partition layout\n&gt; (parted) print\n\n# Exit parted\n&gt; (parted) quit\n\nmklabel gpt: Creates a new GPT partition table (preferred over MBR for modern systems)\nmkpart primary ext4 0% 100%: Creates a primary partition using the ext4 filesystem that spans the entire device\nname 1 backup: Names the first partition ‚Äúbackup‚Äù for easy identification\nprint: Shows the current partition layout\nquit: Exits the parted utility\n\n\nAfter creating the partition, we need to format it with the ext4 filesystem. Double check the current layout of memory on your system with sudo lsblk -o NAME,SIZE,FSTYPE,TYPE,MOUNTPOINT before formatting the filesystem, to get the specific SD Card partitions device name:\n\n# Format the partition (adjust if your device/partition is different)\nsudo mkfs.ext4 -L backups /dev/mmcblk0p1\n\n-L backup: Sets the filesystem label to ‚Äúbackup‚Äù\n/dev/mmcblk0p1: The partition we just created (p1 indicates the first partition)\nYou‚Äôll see an output similar to this:\n\n\n\nNow, we need to prepare the SD card for backups. You can make those changes with the following commands:\n\n# Create a mount point\nsudo mkdir -p /mnt/backups\n\n# Add an entry to /etc/fstab for automatic mounting\necho \"UUID=$(sudo blkid -s UUID -o value /dev/mmcblk0p1) /mnt/backups ext4 defaults,noatime 0 2\" | sudo tee -a /etc/fstab\n\n# Restart the systemd daemon to get the changes made to fstab\nsudo systemctl daemon-reload\n\n# Mount the filesystem from fstab\nsudo mount /dev/mmcblk0\n\n# Create backup directories\nsudo mkdir -p /mnt/backups/{configs,logs}\n\n# Set ownership (replace 'chris' with your username)\nsudo chown -R chris:chris /mnt/backups/\n\n# Set secure permissions\nsudo chmod -R 700 /mnt/backups/\n\nmkdir -p: Creates directories and parent directories if they don‚Äôt exist\nblkid -s UUID -o value: Gets the UUID (unique identifier) of the partition\ndefaults,noatime: Mount options for good performance (noatime disables recording access times)\n0 2: The fifth field (0) disables dumping, the sixth field (2) enables filesystem checks\nmount -a: Mounts all filesystems specified in fstab\nchmod -R 700: Sets permissions so only the owner can read/write/execute\n\n\n\nPartitioning your SSD\nFor a Raspberry Pi server, a two-partition scheme offers the perfect balance of simplicity and functionality. This approach mirrors what RPi Imager creates automatically, but gives us control over the sizes:\n\nA small FAT32 boot partition for firmware and boot files\nA large ext4 root partition for the entire operating system and data\n\nThis simplified structure eliminates the complexity of separate swap and data partitions while maintaining full functionality. The Raspberry Pi can use swap files instead of dedicated partitions, which provides more flexibility for managing memory as your needs change.\n\nFor the Samsung T7 SSD, we‚Äôll follow a similar workflow. The Samsung T7 SSD will likely appear as /dev/sdX (where X is a letter like a, b, c), mine is /dev/sdb.\n\nsudo lsblk -o NAME,SIZE,FSTYPE,TYPE,MOUNTPOINT\n\nIn this code block, I‚Äôll show you a way to use parted without entering the interactive mode.\n\n# Create a new GPT partition table\nsudo parted /dev/sdb mklabel gpt\n\n# Create the EFI System Partition (ESP)\nsudo parted /dev/sdb mkpart boot fat32 1MiB 513MiB\nsudo parted /dev/sdb set 1 esp on\nsudo parted /dev/sdb set 1 boot on\n\n# Create the root partition\nsudo parted /dev/sdb mkpart ubuntu-root ext4 513MiB 100%\n\n# Verify the partition layout\nsudo parted /dev/sdb print\n\n/dev/sdc1: 512MB FAT32 partition for boot files.\n/dev/sdc2: Remaining space (about 931GB) ext4 partition for the entire system.\nThe set 1 boot on command marks the partition as bootable.\nThe set 1 esp on marks it as an EFI System Partition, ensuring compatibility with both legacy and UEFI boot methods.\n\n\nNow we need to format each partition.\n\n# Format the ESP partition\nsudo mkfs.fat -F32 -n BOOT /dev/sdb1\n\n# Format the root partition\nsudo mkfs.ext4 -L ubuntu-root /dev/sdb2\n\nmkfs.fat -F32: Creates a FAT32 filesystem\n-n ESP: Sets the volume label to ‚ÄúESP‚Äù\nmkfs.ext4: Creates an ext4 filesystem\n-L ubuntu-root: Sets the filesystem label\n/dev/sdb1, /dev/sdb2, etc.: The specific partitions we created\n\n\nNow, we will verify that the partitions went as we hoped\n\n# Check the partition layout\nsudo lsblk -o NAME,SIZE,FSTYPE,LABEL,TYPE,MOUNTPOINT\n\n# Verify the filesystem types and labels\nsudo blkid | grep sdc\nThis approach to partitioning offers several advantages:\n\nMatches RPi Imager default: Aligns with what users expect from standard Raspberry Pi installations\nEasier to manage: Fewer partitions mean simpler maintenance and troubleshooting\nThe Raspberry Pi firmware requires a FAT32 boot partition to find and load the kernel.\nThe 512MB size ensures plenty of space for kernel updates and multiple kernel versions if needed.\n\nNow you‚Äôre ready to flash Ubuntu Server to these properly prepared partitions! The RPi Imager will use this partition structure and write the system files to the correct locations.\n\nNow we‚Äôll need to mount the partitions by setting up mount points and telling the system to use them.\n\n# Check the partition layout\nsudo lsblk -o NAME,SIZE,FSTYPE,LABEL,TYPE,MOUNTPOINT\n\n# Verify the filesystem types and labels\nsudo blkid | grep sdc\n\n\nAdvanced Partitioning\nAs you begin to utilize your server more, you‚Äôre bound to use up more memory. So, it‚Äôs important to monitor your partition space usage.\n# View disk usage\ndf -h\n\n# View inode usage (for number of files)\ndf -i\n\n# View detailed filesystem information\nsudo tune2fs -l /dev/sda2 | grep -E 'Block count|Block size|Inode count|Inode size'\n\ndf -h: Shows disk usage in human-readable format\ndf -i: Shows inode usage (inode = index node, representing a file)\ntune2fs -l: Lists filesystem information for ext2/3/4 filesystems\ngrep -E: Filters output for specified patterns\n\nFurthermore, you may realize that you want to reformat your SSD at some point because your storage needs changed. You can reformat the partitions using the following code.\n# For online resizing of ext4 (unmounting not required)\nsudo parted /dev/sda\n(parted) resizepart 4 100%  # Resize partition 4 to use all available space\n(parted) quit\n\n# After resizing the partition, expand the filesystem\nsudo resize2fs /dev/sda4\n\nresizepart 4 100%: Resizes partition 4 to use 100% of the remaining available space\nresize2fs: Resizes an ext2/3/4 filesystem to match the partition size"
  },
  {
    "objectID": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-network",
    "href": "pages/projects/data_engineering/posts/raspberry_pi_server.html#sec-network",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Computer Networking",
    "text": "Computer Networking\nThis section provides a brief example of how to connect your server to WiFi. It assumes you are already connected using the wireless network settings you configured in the Requirements Section. That being said, I‚Äôll also go over some basic networking concepts and background information. As a result, some of the decisiions and terminology in this guide will make more sense (it also helps me remember what I‚Äôm doing and why).\n\nFundamental Concepts of Computer Networking\nSimply put, a computer network is a collection of interconnected devices that can communicate with each other using a set of rules called protocols. Networking allows devices to share resources, exchange data, and collaborate on tasks. On a deeper level, it helps to understand the conceptual models that describes how data moves through a network. Before we dive in, let‚Äôs go over some basic terminology.\n\nKey Terms\nBasic Networking Concepts:\n\nProtocol: A set of rules that determine how data is transmitted between devices on a network. Examples include TCP, UDP, and HTTP.\nMAC Address: Media Access Control address; a unique hardware identifier assigned to network interfaces. It‚Äôs a 48-bit address (e.g., 00:1A:2B:3C:4D:5E) permanently assigned to a network adapter.\nIP Address: A numerical label assigned to each device on a network that uses the Internet Protocol. Functions like a postal address for devices.\nPacket: A unit of data transmitted over a network. Includes both the data payload and header information for routing.\nSubnet: A logical subdivision of an IP network that allows for more efficient routing and security segmentation.\nGateway: A network node that serves as an access point to another network, typically connecting a local network to the wider internet.\nDNS: Domain Name System; translates human-readable domain names (like google.com) into IP addresses computers can understand.\nDHCP: Dynamic Host Configuration Protocol; automatically assigns IP addresses and other network configuration parameters to devices.\n\nNetwork Types and Components:\n\nLAN: Local Area Network; a network confined to a small geographic area, like a home or office.\nWAN: Wide Area Network; connects multiple LANs across large geographic distances.\nRouter: A device that forwards data packets between computer networks, determining the best path for data transmission.\nSwitch: A networking device that connects devices within a single network and uses MAC addresses to forward data to the correct destination.\nBandwidth: The maximum data transfer rate of a network connection, measured in bits per second (bps).\nLatency: The delay between sending and receiving data, typically measured in milliseconds.\n\nLinux Networking Terminology:\n\nInterface: A connection between a device and a network. In Linux, these have names like eth0 (Ethernet) or wlan0 (wireless).\nNetplan: Ubuntu‚Äôs default network configuration tool that uses YAML files to define network settings.\nsystemd-networkd: A system daemon that manages network configurations in modern Linux distributions.\nNetworkManager: An alternative network management daemon that provides detection and configuration for automatic network connectivity.\nSocket: An endpoint for sending or receiving data across a network, defined by an IP address and port number.\n\nSecurity Concepts:\n\nFirewall: Software or hardware that monitors and filters incoming and outgoing network traffic based on predetermined security rules.\nSSH: Secure Shell; a cryptographic network protocol for secure data communication and remote command execution.\nEncryption: The process of encoding information to prevent unauthorized access.\nPort: A virtual point where network connections start and end. Ports are identified by numbers (0-65535).\nNAT: Network Address Translation; allows multiple devices on a local network to share a single public IP address.\nVPN: Virtual Private Network; extends a private network across a public network, enabling secure data transmission.\n\n\n\nThe OSI Model\nNow that you understand some common terms and concepts, we can dive into the conceptual models. The Open Systems Interconnection (OSI) Model divides networking into seven layers, each handling specific aspects of network communication.\n\nPhysical Layer: Physical medium, electrical signals, cables, and hardware\nData Link Layer: Physical addressing (MAC addresses), error detection\nNetwork Layer: Logical addressing (IP addresses), routing\nTransport Layer: End-to-end connections, reliability (TCP/UDP)\nSession Layer: Session establishment, management, and termination\nPresentation Layer: Data translation, encryption, compression\nApplication Layer: User interfaces and services (HTTP, SMTP, etc.)\n\n\n\nThe TCP/IP Model\nThe OSI Model is conceptual, but the TCP/IP Model is more practical and has four layers.\n\nNetwork Access Layer:Combines OSI‚Äôs Physical and Data Link layers\nInternet Layer: Similar to OSI‚Äôs Network layer (IP)\nTransport Layer: Same as OSI‚Äôs Transport layer (TCP/UDP)\nApplication Layer: Combines OSI‚Äôs Session, Presentation, and Application layers\n\n\n\nNetwork Protocols\nRemember, a protocol is a set of rules that determine how data is transmitted between devices on a network. You can think of protocols in one of two camps, Connection-Oriented and Connectionless. Within these camps, two protocols stand out as the backbone of the internet‚Äôs data transfers: TCP and UDP.\nTCP (Transmission Control Protocol) is a connection-oriented protocol that establishes a dedicated end-to-end connection before transmitting data. TCP is used when reliability is more important than speed (e.g., web browsing, email, file transfers). It has four defining traits:\n\nReliability: Guarantees delivery of packets in the correct order\nFlow Control: Prevents overwhelming receivers with too much data\nError Detection: Identifies and retransmits lost or corrupted packets\nHandshake Process: Three-way handshake establishes connections\n\nUDP (User Datagram Protocol) is a connectionless protocol that sends data without establishing a dedicated connection. UDP is used for real-time applications (e.g., video streaming, VoIP, online gaming). It also has four defining traits:\n\nSimplicity: No connection setup or maintenance overhead\nSpeed: Faster than TCP due to fewer checks and guarantees\nLower Reliability: No guarantee of delivery or correct ordering\nEfficiency: Better for real-time applications where occasional data loss is acceptable\n\nBeyond those, there are some other important protocols to know, because they provide the foundation for most of the user friendly features we are used to today.\n\nIP (Internet Protocol)\n\nIP handles addressing and routing of packets across networks. There are two versions in common use:\nIPv4: 32-bit addresses (e.g., 192.168.1.1)\nIPv6: 128-bit addresses (e.g., 2001:0db8:85a3:0000:0000:8a2e:0370:7334)\n\nICMP (Internet Control Message Protocol)\n\nICMP helps diagnose network issues by sending error messages and operational information. The ping command uses ICMP to test connectivity.\n\nHTTP/HTTPS (Hypertext Transfer Protocol)\n\nHTTP and its secure variant HTTPS are application-layer protocols used for web browsing.\n\nDNS (Domain Name System)\n\nDNS translates human-readable domain names (like google.com) into IP addresses.\n\n\n\n\n\nNetwork Connections\nThere are two ways for systems to connect to the internet: wired and wireless.\n\nWired Connections\nEthernet is the most common wired networking technology. Its name comes from the term ether referring to a theoretical medium that was believed to carry light waves through space. It was developed by Robert Metcalf and David Boggs at Xerox‚Äôs PARC facility in the 1970s. The goal was to provide a more stable LAN which could facilitate high speed transfers between computers and laser printers. They succeeded, and had improved on a precursor‚Äôs, ALOHAnet, design by creating a system that could detect collisions‚Äì when two devices try to transmit at the same time. Here are some key traits:\n\nReliability: Less susceptible to interference\nSpeed: Typically faster and more stable than wireless\nSecurity: Harder to intercept without physical access\nConnectors: RJ45 connectors on Ethernet cables\nStandards: 10/100/1000 Mbps (Gigabit) are common speeds\n\n\n\nWireless Connections\nWi-Fi allows devices to connect to networks without physical cables. Its name is not short for Wireless Fidelity, but actually a marketing choice by the brand-consulting firm Interbrand. They chose the name because it sounded similar to Hi-Fi. Wi-Fi was developed by numerous researchers and engineers, but the key breakthrough was by Dr.¬†John O‚ÄôSullivan from CSIRO in Australia. His work focused on a wireless LAN, which would eventually become the IEEE (Institute of Electrical and Electronics Engineers) 802.11 standard in 1997. Eventually, Apple would help with widespread adoption by including the AirPort feature on its laptops, enabling W-Fi connectivity out of the box. Here are some key traits:\n\nConvenience: No cables required, more flexible placement\nStandards: 802.11a/b/g/n/ac/ax (Wi-Fi 6) with varying speeds and ranges\nSecurity: WEP, WPA, WPA2, and WPA3 encryption standards (WPA2/WPA3 recommended)\n\n\n\nNetwork Interface Names in Linux\nIn Ubuntu Server, network interfaces follow a predictable naming convention:\n\neth0, eth1: Traditional Ethernet interface names\nwlan0, wlan1: Traditional wireless interface names\nenp2s0, wlp3s0: Modern predictable interface names (based on device location)\n\n\n\nIP Addressing\nAn IP (Internet Protocol) Address, is a unique identifier for a device on the internet, or a LAN. There are two different kinds of addresses: IPv4 and IPv6.\nIPv4 uses 32-bit addresses, providing approximately 4.3 billion unique addresses (now largely exhausted):\n\nFormat: Four octets (numbers 0-255) separated by dots (e.g., 192.168.1.1)\nClasses: Traditionally divided into classes A, B, C, D, and E\nPrivate Ranges:\n\n10.0.0.0 to 10.255.255.255 (10.0.0.0/8)\n172.16.0.0 to 172.31.255.255 (172.16.0.0/12)\n192.168.0.0 to 192.168.255.255 (192.168.0.0/16)\n\nSubnet Masks: Used to divide networks (e.g., 255.255.255.0 or /24)\nIssues: IPv4 address exhaustion due to limited capacity\n\nIPv6 uses 128-bit addresses, providing approximately 3.4√ó10^38 unique addresses:\n\nFormat: Eight groups of four hexadecimal digits separated by colons (e.g., 2001:0db8:85a3:0000:0000:8a2e:0370:7334)\nShorthand: Leading zeros in a group can be omitted, and consecutive groups of zeros can be replaced with :: (only once)\n\nExample: 2001:db8:85a3::8a2e:370:7334\n\nAddress Types:\n\nUnicast: Single interface\nAnycast: Multiple interfaces (closest responds)\nMulticast: Multiple interfaces (all respond)\n\nBenefits: More addresses, improved security, simplified headers, no need for NAT\n\nOne final note, CIDR (Classless Inter-Domain Routing) notation represents IP addresses and their associated routing prefix:\n\nFormat: IP address followed by ‚Äú/‚Äù and prefix length (e.g., 192.168.1.0/24)\nCalculation: A prefix of /24 means the first 24 bits are the network portion, leaving 8 bits for hosts (allowing 2^8 = 256 addresses)\n\n\n\n\nUbuntu Server Networking Tools\nNow that we‚Äôve covered the basic concepts, it‚Äôs time to dive into the actual commands and tools that will let you configure and manage your server‚Äôs network. To start, you can view network interfaces and their statuses using the command ip link show, or ip addr show for your IP Address configuration. You can view only the IPv4 or IPv6 addresses using ip -4 addr or ip -6 addr, respectively.\n\nTesting Connectivity\nAlthough it seems redundant if you already viewed your IP addresses, you can also test connectivity using the ping and traceroute commands. These will be more useful for checking your servers network status from your desktop or laptop.\nTest basic connectivity to a host:\nping -c 4 google.com\n\nTrace the route to a destination:\n# First update and install your packages\nsudo apt update && sudo apt upgrade -y\n\n# Install traceroute\nsudo apt install traceroute -y\n\n# Run traceroute\ntraceroute google.com\n\nCheck the DNS resolution:\nnslookup google.com\n# dig google.com\n\n\n\nViewing Network Statistics\nYou can view more specific network information with the ss command. This command‚Äôs name is an acronym for socket statistics and is used as a replacement for the older netstat plan because it offers faster performance and a more detailed output. Additionally, you can filter by specific protocol.\nss -tuln\nThe tuln flag is made up of four separate flags:\n\n-t, displays only TCP sockets\n-u, displays only UDP sockets\n-l, displays listening sockets\n-n, displays address numerically, instead of resolving them\n\n\n\nConfiguration Files\nFinally, there are a few crucial configuration files that will handle the bulk of your networking. In Ubuntu Server, network interfaces and DNS configurations are configured and stored in the /etc/ directory.\nNetwork Interfaces:\n\n/etc/netplan/: Contains YAML configuration files for Netplan\n/etc/network/interfaces: Configuration method (if NetworkManager is used)\n\nDNS Configuration:\n\n/etc/resolv.conf: DNS resolver configuration\n/etc/hosts: Static hostname to IP mappings\n/etc/hostname: System hostname\n\n\n\n\nsystemd-networkd\nsystemd-networkd is a system daemon that manages network configurations in modern Linux distributions. It‚Äôs part of the systemd suite and provides network configuration capabilities through simple configuration files.\nIt generally works using three key components:\n\nConfiguration Files: You define network settings in .network files located in /etc/systemd/network/\nService Management: systemd-networkd runs as a system service to apply and maintain network configurations\nIntegration: Works closely with other systemd components for DNS resolution and networking\n\n\nBasic Wired Configuration\nsystemd-networkd uses configuration files with .network extension. Each file consists of sections with key-value pairs. A basic configuration for a static IP would look like this:\n# /etc/systemd/network/20-wired.network\n[Match]\nName=eth0\n\n[Network]\nAddress=192.168.1.100/24\nGateway=192.168.1.1\nDNS=8.8.8.8\nDNS=8.8.4.4\nLet‚Äôs walk through the configuration file‚Äôs structure:\n\nFile Naming Convention:\n\nThe file is named 20-wired.network.\nThe number prefix (20-) determines the processing order (lower numbers processed first), allowing you to create prioritized configurations.\nThe suffix .network tells systemd-networkd that this is a network interface configuration file.\n\n[Match] Section:\n\nThis critical section determines which network interfaces the configuration applies to.\nName=eth0: This specifies that the configuration should apply to the eth0 interface only.\nYou can use wildcards (e.g., eth* would match all Ethernet interfaces) or match by other properties such as MAC address using MACAddress=xx:xx:xx:xx:xx:xx.\nBehind the scenes:\n\nsystemd-networkd scans all available network interfaces.\nCompares their properties against those specified in the [Match] section.\nIf all properties match, the configuration is applied to that interface.\n\n\n[Network] Section:\n\nThis section defines the network configuration parameters.\nAddress=192.168.1.100/24: Sets a static IPv4 address with CIDR notation. The /24 represents the subnet mask (equivalent to 255.255.255.0) and defines the network boundary.\nGateway=192.168.1.1: Specifies the default gateway for routing traffic outside the local network. All traffic not destined for the local subnet (192.168.1.0/24) will be sent to this IP address.\nDNS=8.8.8.8 and DNS=8.8.4.4: These are Google‚Äôs public DNS servers. When specified, systemd-networkd will automatically configure /etc/resolv.conf through systemd-resolved. You can specify multiple DNS servers, and they will be tried in order.\n\nBehind the scenes:\n\nsystemd-networkd identifies the eth0 interface\nAssigns the static IP address using kernel netlink sockets\nSets up the routing table to use the specified gateway\nCommunicates with systemd-resolved to configure DNS settings\nMaintains this configuration and reapplies it if the interface goes down and back up\n\n\nThis configuration example works well for server environments where static, predictable networking is preferable. This is a declarative configuration, it describes the desired state, rather than the steps to achieve it, so repeated application produces the same result.\n\n\nDHCP with a Wired Connection\nIf you want to add DHCP, you can use the following:\n# /etc/systemd/network/20-wired.network\n[Match]\nName=eth0\n\n[Network]\nDHCP=yes\nLet‚Äôs walk through the differences between a dynamic and static host configuration file structure:\n\nDHCP=yes: This single line replaces all the static configuration parameters from the previous example.\n\nIt instructs systemd-networkd to obtain IP address, subnet mask, gateway, DNS servers, and other network parameters automatically from a DHCP server.\nYou can also use DHCP=ipv4 to enable only IPv4 DHCP, or DHCP=ipv6 for only IPv6 DHCP, or DHCP=yes for both.\n\nBehind the scenes:\n\nsystemd-networkd identifies the eth0 interface\nInitiates the DHCP client process, which follows the DHCP protocol‚Äôs Discover-Offer-Request-Acknowledge (DORA) sequence:\n\nThe client broadcasts a DISCOVER message\nAvailable DHCP servers respond with OFFER messages\nThe client selects an offer and sends a REQUEST\nThe selected server sends an ACKNOWLEDGE\n\nApplies all the received network parameters (IP, subnet, gateway, DNS)\nSets up a lease timer to manage when the configuration needs renewal\nHandles DHCP lease renewals automatically\n\nAdvantages:\n\nSimplified configuration maintenance - no need to update parameters when network details change\nWorks well in networks where IP assignments are centrally managed\nAutomatically adapts to network changes\n\n\nThis configuration works well for environments where network parameters are dynamic or managed by a network admin through DHCP.\n\n\nWireless Configurations and wpa_supplicant\nWhile wired connections are a basic part of networking, wireless connections require some extra work. More specifically, with systemd-networkd, you‚Äôll need a tool like WPA. Wi-Fi Protected Access (WPA) emerged as a response to weaknesses in the original Wired Equivalent Privacy (WEP) security protocol. As wireless networks became ubiquitous, secure authentication and encryption mechanisms became essential. The Linux ecosystem offers several powerful tools for managing these connections:\n\nwpa_supplicant: The core daemon that handles wireless connections\nwpa_cli: A command-line interface for controlling wpa_supplicant dynamically\nwpa_passphrase: A utility for generating secure password hashes\n\nOn the systemd-networkd side of things, the configuration is simple, broken down in detail below.\n# /etc/systemd/network/25-wireless.network\n[Match]\nName=wlan0\n\n[Network]\nDHCP=yes\n\nWireless Interface:\n\nThe configuration targets wlan0, which is the traditional name for the first wireless network interface in Linux.\n\nMinimal Configuration:\n\nThe file only has the information needed by systemd-networkd to manage the IP addressing aspect of the wireless connection. Note what‚Äôs missing: there‚Äôs no SSID, password, or security protocol information. This is because:\nsystemd-networkd isn‚Äôt designed to handle wireless authentication and association\nThis separation of concerns is intentional in the systemd design philosophy - specialized tools should handle specialized tasks\n\nIntegration with wpa_supplicant:\n\nwpa_supplicant is the standard Linux utility for managing wireless connections\nsystemd-networkd handles the network layer (Layer 3) configuration once wpa_supplicant establishes the data link layer (Layer 2) connection\nThis division follows the OSI model‚Äôs separation of network layers\n\nBehind the scenes:\n\nwpa_supplicant handles wireless scanning, authentication, and association\nOnce a wireless link is established, it notifies the system\nsystemd-networkd detects the active interface that matches wlan0\nIt then initiates the DHCP client process to configure the network parameters\n\nThis separation provides flexibility and security\n\nThe wireless security operations are handled by a dedicated, well-tested component\nNetworking remains under systemd-networkd‚Äôs control for consistency with other interfaces\n\n\nWhile the systemd-networkd configuration is straightforward, things get more complicated with WPA. In standard wpa_supplicant configuration files, wireless passwords are often stored in plaintext. This creates a security vulnerability - anyone with access to the configuration file can view the password.\n# /etc/wpa_supplicant/wpa_supplicant-wlan0.conf\nctrl_interface=/run/wpa_supplicant\nupdate_config=1\n\nnetwork={\n    ssid=\"YourNetworkSSID\"\n    psk=\"YourWiFiPassword\"\n}\nThe wpa_passphrase tool solves this problem by generating a pre-computed hash of the password. Running this is straightforward as the basic syntax is wpa_passphrase [SSID] [passphrase]. Then, WPA outputs a hashed version of your password.\n# Generate a hashed passphrase\nwpa_passphrase \"MyHomeNetwork\" \"MySecurePassword123\"\nTo then use the hashed password in your configuration, you can run the following command, just make sure to remove the line with the plaintext password from the config file after runtime:\n# Generate the hash and save directly to the configuration file\nwpa_passphrase \"MyHomeNetwork\" \"MySecurePassword123\" | sudo tee -a /etc/wpa_supplicant/wpa_supplicant-wlan0.conf\nWhen you use wpa_passphrase: - It combines the SSID and password using the PBKDF2 (Password-Based Key Derivation Function 2) algorithm - It applies 4096 iterations of HMAC-SHA1 for key strengthening - The result is a 256-bit (32-byte) hash represented in hexadecimal format - This hash is what‚Äôs actually used for the authentication process, not the original password\nThis approach makes it virtually impossible to reverse-engineer the original password from the hash.\n# /etc/wpa_supplicant/wpa_supplicant-wlan0.conf\nctrl_interface=/run/wpa_supplicant\nupdate_config=1\n\nnetwork={\n    ssid=\"MyHomeNetwork\"\n    #psk=\"MySecurePassword123\"\n    psk=a8e665b82929d810746c5a1208c472f9d2a25db67a6bc32a99fa4158aea02175\n}\nNow that you have an idea about the basic structure of this file, lets go over some key points:\n\nFile Naming Convention:\n\nThe file wpa_supplicant-wlan0.conf is specifically named to associate with the wlan0 interface.\nThis naming allows different wireless interfaces to have different configurations.\n\nConfiguration Directives:\n\nctrl_interface=/run/wpa_supplicant: This specifies the control interface path, which is a socket that allows programs to communicate with wpa_supplicant. This enables tools like wpa_cli to connect and control wpa_supplicant dynamically.\nupdate_config=1: Allows wpa_supplicant to update the configuration file automatically, useful when network details change or when using wpa_cli to add networks interactively.\n\nNetwork Block:\n\nThe network={} block defines a single wireless network configuration.\nssid=\"YourNetworkSSID\": The Service Set Identifier - the name of the wireless network to connect to.\npsk=\"YourWiFiPassword\": The Pre-Shared Key - the password for the wireless network in plaintext.\n\nSecurity Considerations:\n\nWhen you enter the password in plaintext as shown, wpa_supplicant will automatically convert it to a hash during processing.\nFor better security, you can pre-hash the password using: wpa_passphrase ‚ÄúYourNetworkSSID‚Äù ‚ÄúYourWiFiPassword‚Äù and use the generated hash.\nThe configuration file should have restricted permissions (600) to prevent other users from reading the passwords.\n\nBehind the scenes:\n\nwpa_supplicant reads this configuration at startup\nIt scans for available wireless networks\nWhen it finds the specified SSID, it attempts to authenticate using the provided credentials\nIt handles all the wireless protocol handshakes, including:\n\nAuthentication and association with the access point\nNegotiation of encryption parameters\nEstablishment of the encrypted channel\n\n\n\nOnce connected, it maintains the connection and handles roaming between access points with the same SSID. This configuration represents the minimum needed for a WPA/WPA2 Personal network connection. For more complex scenarios like enterprise authentication (WPA-EAP), additional parameters would be needed in the network block.\nWhile the wpa_supplicant configuration files provide static configuration that saves when you write out, wpa_cli offers interactive, dynamic control over wireless connections. First ensure wpa_supplicant is running with a control interface by using ps aux | grep wpa_supplicant. If it‚Äôs running with the -c flag pointing to a config file that contains the ctrl_interface=/run/wpa_supplicant line, you can connect to it.\n\n\n\n\n\n\nImportant\n\n\n\nAs a heads up, because we already created the wlan0 configuration file manually, the following steps are just for your knowledge. You‚Äôll probably get some messages saying FAIL if you try to run some of the commands, but I think it‚Äôs good to learn them anyways‚Äì even though they aren‚Äôt necessarilly important right now.\n\n\nFirst, start the interactive mode with sudo wpa_cli, or specify the interface with sudo wpa_cli -i wlan0. Let‚Äôs go over some essential commands:\n# Show help\nhelp\n\n# List all available commands\nhelp all\n\n# List available networks\nscan\nscan_results\n\n# Show current status\nstatus\n\n# List configured networks\nlist_networks\n\n# Add a new network\nadd_network\nStep-By-Step: Adding a Network\n&gt; add_network\n0\n&gt; set_network 0 ssid \"MyNetwork\"\nOK\n&gt; set_network 0 psk \"MyPassword\"\nOK\n&gt; set_network 0 priority 5 \nOK\n&gt; enable_network 0\nOK\n&gt; save_config\nOK\n\n# For networks with hashed passwords\n&gt; add_network\n0\n&gt; set_network 1 ssid \"MyNetwork\"\nOK\n&gt; set_network 1 psk 0a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0u1v2w3x4y5z\nOK\n&gt; set_network 1 priority 10\nOK\n&gt; enable_network 1\nOK\n&gt; save_config\nOK\nIt‚Äôs good to know that higher priority values (like 10), are preferred over lower ones (like 5). Now, you can also use wpa by running one off commands or writing scripts with non-interactive mode. Additionally, you should know that when you first boot up your Raspberry Pi, the internet will be managed by Netplan (more on that later in this section). So, if you try to use wpa_cli save_config after creating the config files, that will return FAIL. Instead, once you write the files in the proper directories, run the reconfigure command.\n# Scan for networks\nsudo wpa_cli scan\nsudo wpa_cli scan_results\n\n# Save the current configuration\nsudo wpa_cli save_config\n\n# Reconnect to the network\nsudo wpa_cli reconfigure\nFinally, you can monitor signal quality and connection status by using the signal_poll command. The RSSI (Received Signal Strength Indicator) shows connection quality in dBm. Values closer to 0 indicate stronger signals. Additionally, you can debug connection issues using status.\n&gt; signal_poll\nRSSI=-67\nLINKSPEED=65\nNOISE=9999\nFREQUENCY=5220\n\n&gt; status\nbssid=00:11:22:33:44:55\nfreq=5220\nssid=MyNetwork\nid=0\nmode=station\npairwise_cipher=CCMP\ngroup_cipher=CCMP\nkey_mgmt=WPA2-PSK\nwpa_state=COMPLETED\nip_address=192.168.1.100\nNow that we‚Äôve covered a lot of the great features available with wpa_cli, it‚Äôs time to continue configuring our server. You may remember me mentioning that the Raspberry Pi default networking tool is Netplan. Before we can enable and start the wlan0 service (meaning your primary wifi is on), we need to safely shut down Netplan.\n\n\n\nConverting Netplan to networkd\nIt‚Äôs no surprise that Raspberry Pi uses Netplan as the default network manager because it provides a consistent interface for network configuration; however, there are several reasons you might want to use systemd-networkd directly:\n\nSimplicity: Direct systemd-networkd configuration eliminates a layer of abstraction\nControl: Direct access to all of systemd-networkd‚Äôs features without Netplan‚Äôs limitations\nIntegration: Better alignment with other systemd components\nLearning: Understanding the underlying network configuration system\nPerformance: Potentially faster setup without the translation layer\n\nUbuntu Server uses a layered approach to network configuration:\n\nUser configuration layer: YAML files in /etc/netplan/\nTranslation layer: Netplan reads YAML files and generates configurations for a backend\nBackend layer: Either systemd-networkd or NetworkManager applies the actual configuration\n\nBy removing the middle layer (Netplan), we‚Äôre configuring the backend directly. As you can see from the previous parts of this Networking section in the guide, I like the learning value and longterm potential of systemd, which is why I went with it over Netplan.\n\nStep-by-step Migration\n\nBegin by creating backups of your current network configuration\n\n# Create a backup directory\nsudo mkdir -p /etc/netplan/backups\n\n# Copy all netplan config files\nsudo cp /etc/netplan/*.yaml /etc/netplan/backups/\n\n# Document the current network state\nsudo ip -c addr | sudo tee /etc/netplan/backups/current-ip-addr.txt\nsudo ip -c route | sudo tee /etc/netplan/backups/current-ip-route.txt\n\nReview your existing Netplan so you know what to recreate\n\n# View your current netplan configs\ncat /etc/netplan/*.yaml\n\nNow, create the corresponding systemd-networkd configuration files in /etc/systemd/network/.\n\nFor each interface (wired, wireless, etc.) in your Netplan configuration, create a corresponding .network file, with the appropriate configurations (i.e.¬†static vs.¬†DHCP).\nRemember: For wireless connections, you need both a systemd and a wpasupplicant configuration.\n\n\n# Create the directory if it doesn't exist\nsudo mkdir -p /etc/systemd/network/\n\n# For an Ethernet configuration\nsudo nano /etc/systemd/network/20-wired.network\n\n# For a Wireless configuration\nsudo nano /etc/systemd/network/25-wireless.network\nsudo nano /etc/wpa_supplicant/wpa_supplicant-wlan0.conf\n\nNow that your networkd configuration is in place, disable Netplan.\n\n# Ensure systemd-networkd is enabled\nsudo systemctl enable systemd-networkd\nsudo systemctl enable systemd-resolved\nsudo systemctl start systemd-networkd\n\n# Move the Netplan configurations to a disabled state\nsudo mkdir -p /etc/netplan/disabled\nsudo mv /etc/netplan/*.yaml /etc/netplan/disabled/\n\n# Create a minimal netplan configuration that defers to systemd-networkd\nsudo tee /etc/netplan/01-network-manager-all.yaml &gt; /dev/null &lt;&lt; EOF\nnetwork:\n  version: 2\n  renderer: networkd\nEOF\nLet‚Äôs take a look at the systemctl enable and start commands, because without them we will lose connectivity when turning off netplan. Before we do that, however, what the minimal netplan configuration file does is essentially tell Netplan to just use networkd. We‚Äôll remove it once we are sure everything is up and running.\n\nEnable Command:\n\nsudo systemctl enable systemd-networkd: This configures systemd-networkd to start automatically when the system boots.\nBehind the scenes:\n\nThis command creates the necessary symbolic links in systemd‚Äôs unit directories so that the network daemon will be started by systemd during the boot process.\nIt integrates the service into systemd‚Äôs dependency tree.\nWithout this step, you would need to manually start networkd after each reboot, which is impractical for a server environment.\n\n\nStart Command:\n\nsudo systemctl start systemd-networkd: This launches the systemd-networkd daemon immediately.\nBehind the scenes:\n\nsystemd spawns the networkd process, which then:\n\nReads all .network, .netdev, and .link configuration files in /etc/systemd/network/ and /usr/lib/systemd/network/\nApplies the configurations to matching interfaces\nSets up monitoring for network changes\n\n\n\nRestart Command:\n\nsudo systemctl restart systemd-networkd: This stops and then starts the daemon again, ensuring all configuration changes are applied.\nBehind the scenes:\n\nsystemd sends a termination signal to the running networkd process, waits for it to exit cleanly, and then starts a new instance.\nThe new instance repeats the initialization process, reading all configuration files again.\nThis is the command you‚Äôll use most frequently when making changes to network configurations.\n\n\nWhy Restart Is Necessary:\n\nWhile systemd-networkd does monitor for some changes, editing configuration files doesn‚Äôt automatically trigger a reconfiguration.\nThe restart ensures that:\n\nAll new or modified configuration files are re-read\nAny removed configurations are no longer applied\nAll interface configurations are freshly evaluated against the current state\n\n\nImpact on Network Connectivity:\n\nA restart will temporarily disrupt network connectivity as interfaces are reconfigured\nFor remote servers, use caution when restarting network services to avoid losing your connection\nFor critical remote systems, consider using a command pipeline, like:\n\nsudo systemctl restart systemd-networkd.service || (sleep 30 && sudo systemctl start systemd-networkd.service)\nWhich attempts to restart and then tries to start the service again after 30 seconds if connectivity is lost\n\n\n\n\nApply the systemd-networkd network configuration\n\n# Apply Netplan changes (this will do nothing as we now have a minimal config)\nsudo netplan apply\n\n# Restart systemd-networkd to apply our direct configuration\nsudo systemctl restart systemd-networkd\n\n# The next step is to enable and start the wpa_supplicant service.\n\nsudo systemctl enable wpa_supplicant@wlan0.service\nsudo systemctl start wpa_supplicant@wlan0.service\nThese commands are crucial for integrating wpa_supplicant with systemd, let‚Äôs break them down:\n\nService Template:\n\nThe wpa_supplicant@wlan0.service syntax uses systemd‚Äôs template unit feature.\nThe @ symbol indicates a template service, and wlan0 is the instance name that gets passed to the template.\nThis allows the same service definition to be used for different wireless interfaces.\n\nEnable Command:\n\nsudo systemctl enable wpa_supplicant@wlan0.service: This creates symbolic links from the system‚Äôs service definition directory to systemd‚Äôs active service directory, ensuring the service starts automatically at boot.\nBehind the scenes:\n\nThis modifies systemd‚Äôs startup configuration by adding the service to the correct target units. Typically multi-user.target.\nThe symbolic links created point to the wpa_supplicant service template file.\n\n\nStart Command:\n\nsudo systemctl start wpa_supplicant@wlan0.service: This immediately starts the service without waiting for a reboot.\nBehind the scenes:\n\nsystemd executes the wpa_supplicant binary with appropriate arguments\nDerived from the service template and the instance name (wlan0).\nThe command effectively executed is similar to: /usr/sbin/wpa_supplicant -c /etc/wpa_supplicant/wpa_supplicant-wlan0.conf -i wlan0\n\n\nIntegration with systemd-networkd:\n\nWhen wpa_supplicant successfully connects to a wireless network, it brings the interface up\nsystemd-networkd detects this state change through kernel events\nsystemd-networkd then applies the matching network configuration (our earlier 25-wireless.network file)\nIf DHCP is enabled, the DHCP client process begins\n\nBenefits of this systemd configuration:\n\nDependency management (services can start in the correct order)\nAutomatic restart if the service fails\nStandardized logging through journald\nConsistent management interface alongside other system services\nThe template approach allows for modular configuration that can be easily expanded if you add more wireless interfaces to your Raspberry Pi.\n\n\n\nVerify the new configuration by checking the systemctl status and running simple network check commands\n\n# Check systemd-networkd status\nsystemctl status systemd-networkd\n\n# Check interface status\nip addr show\n\n# Test connectivity\nping -c 4 google.com\nYou should see outputs that look like this: Note, I didn‚Äôt show the output of id addr because I don‚Äôt want to accidentally post my actual IP address online.\n\n\n\nMake the change permanent\n\n# Remove the minimal Netplan configuration\nsudo rm /etc/netplan/01-network-manager-all.yaml\n\n# Mask the Netplan service to prevent it from running\nsudo systemctl mask netplan-wpa@.service\nsudo systemctl mask netplan-ovs-cleanup.service\nsudo systemctl mask netplan-wpa-wlan0.service\nOne final note before moving on, by the time I removed the generic netplan configuration, my system did not have netplan-wpa.service or netplan-wpa-wlan0.service. I forgot to look before I tested the previous steps, so I‚Äôm not sure if I did, but I‚Äôll leave them here just in case someone needs them. That being said, I was able to mask netplan-ovs-cleanup.service successfully.\n\n\nTroubleshooting\nOnce you‚Äôve finished making changes and applying them, verify that everything is up, running, and as you expect.\n# Check systemd-networkd status\nsystemctl status systemd-networkd\n\n# View network status\nnetworkctl status\n\n# List all network links\nnetworkctl list\nThese are crucial commands for troubleshooting and confirming your network configuration, let‚Äôs break them down:\n\nsystemd-networkd Status Check:\n\nsystemctl status systemd-networkd: This displays the current status of the systemd-networkd service. The output includes:\n\nWhether the service is active, inactive, or failed\nWhen it was started and how long it‚Äôs been running\nThe process ID and memory usage\nRecent log entries directly related to the service\n\nBehind the scenes:\n\nThis queries systemd‚Äôs internal service management database and pulls relevant information from the journal logging system.\nUseful pattern: Look for ‚ÄúActive: active (running)‚Äù to confirm the service is working properly and check the logs for any warning or error messages.\n\n\nNetwork Status Overview:\n\nnetworkctl status: This command provides a comprehensive overview of your system‚Äôs network state.\n\nThe output includes:\n\nHostname and domain information\nGateway and DNS server configurations\nCurrent network interfaces and their states\nNetwork addresses (IPv4 and IPv6)\n\n\nBehind the scenes:\n\nThis tool directly communicates with systemd-networkd using its D-Bus API to retrieve the current network state.\nThis command is particularly useful because it aggregates information that would otherwise require multiple different commands to collect.\n\n\nNetwork Links Enumeration:\n\nnetworkctl list: This lists all network interfaces known to systemd-networkd.\n\nThe output shows:\n\nInterface index numbers\nInterface names\nInterface types (ether, wlan, loopback, etc.)\nOperational state (up, down, dormant, etc.)\nSetup state (configured, configuring, unmanaged)\n\n\nBehind the scenes:\n\nLike the status command, this uses systemd-networkd‚Äôs D-Bus API to enumerate all network links and their current states.\nThis provides a quick way to verify which interfaces systemd-networkd is managing and their current status.\n\n\nTroubleshooting with These Commands:\n\nStart with systemctl status systemd-networkd to ensure the service is running\nUse networkctl list to see which interfaces are detected and their states\nIf an interface shows ‚Äúconfiguring‚Äù instead of ‚Äúconfigured,‚Äù check for configuration errors\nUse networkctl status to verify DNS settings and addressing\nFor more detailed logs: journalctl -u systemd-networkd shows all logs from the networkd service\n\n\nThese commands represent the primary diagnostic tools when working with systemd-networkd. They provide a layered approach to troubleshooting - from service-level status to detailed interface information - that helps pinpoint issues in your network configuration. If you need more:\n\nNetwork Connectivity Loss:\n\nConnect directly to the device via console or keyboard/monitor\nCheck logs with journalctl -u systemd-networkd\nRestore the Netplan configuration from your backup if needed\n\nDNS Resolution Issues:\n\nEnsure systemd-resolved is running: systemctl status systemd-resolved\nCheck /etc/resolv.conf is a symlink to /run/systemd/resolve/stub-resolv.conf\nIf not, create it: sudo ln -sf /run/systemd/resolve/stub-resolv.conf /etc/resolv.conf\n\nConfiguration Errors:\n\nVerify syntax with networkctl list to see if interfaces are ‚Äúconfigured‚Äù or ‚Äúconfiguring‚Äù\nCheck for errors with journalctl -u systemd-networkd -n 50\n\n\n\n\n\nAdvanced Networking\n\nSubnets\nA subnet is a logical subdivision of an IP network. Subnetting allows network administrators to partition a large network into smaller, more manageable segments. Subnetting serves several important functions:\n\nAddress Conservation: More efficient allocation of limited IPv4 address space\nSecurity Segmentation: Isolating sensitive systems from general network traffic\nBroadcast Domain Control: Reducing broadcast traffic by limiting its scope\nHierarchical Addressing: Simplifying routing tables and network management\nTraffic Optimization: Improving network performance by segregating traffic types\n\nA subnet mask determines which portion of an IP address refers to the network and which portion refers to hosts within that network. Consider an IPv4 address: 192.168.1.10 with subnet mask 255.255.255.0 (/24)\n\nIn binary:\n\nIP: 11000000.10101000.00000001.00001010\nMask: 11111111.11111111.11111111.00000000\nThe 1s in the mask represent the network portion, while the 0s represent the host portion.\n\nIn CIDR Notation:\n\n/24 means the first 24 bits identify the network (equivalent to 255.255.255.0)\n/16 means the first 16 bits identify the network (equivalent to 255.255.0.0)\n\nSubnet Calculations: For a /24 network\n\nNetwork address: First address in range (e.g., 192.168.1.0)\nBroadcast address: Last address in range (e.g., 192.168.1.255)\nAvailable host addresses: 2^(32-prefix) - 2 = 2^8 - 2 = 254 usable addresses\n\n\nCreating subnets involves both network design and interface configuration. Here‚Äôs how to implement subnetting on a Linux server using systemd-networkd:\n\nScenario 1: Simple Subnet Isolation\n\nThis configuration matches the eth1 interface\nAssigns it the IP 10.0.1.1 within a /24 subnet (255.255.255.0)\nEnables IP forwarding to allow traffic between this subnet and others\nWhen applied, this creates a subnet with 254 usable addresses (10.0.1.1 through 10.0.1.254, excluding the network address 10.0.1.0 and broadcast address 10.0.1.255).\n\n\n# /etc/systemd/network/25-subnet.network\n[Match]\nName=eth1\n\n[Network]\nAddress=10.0.1.1/24\nIPForward=yes\n\nScenario 2: Multiple Subnets on a Single Interface\n\nThis configuration creates three separate subnets accessed through the same physical interface\n\nA /24 subnet (256 addresses) in the 192.168.1.x range\nA /24 subnet (256 addresses) in the 10.10.10.x range\nA /16 subnet (65,536 addresses) in the 172.16.x.x range\n\n\n\nThe system serves as a router/gateway for all three networks simultaneously.\n# /etc/systemd/network/30-multi-subnet.network\n[Match]\nName=eth0\n\n[Network]\nAddress=192.168.1.10/24\nAddress=10.10.10.1/24\nAddress=172.16.1.1/16\n\nDHCP Server Configuration for Subnets\n\nThis configuration, creates a subnet (10.0.1.0/24) on eth1\nEnables a DHCP server\nAllocates IPs from 10.0.1.11 (base + offset of 10) through 10.0.1.210 (for 200 addresses)\nProvides DNS server information to DHCP clients\n\n\n# /etc/systemd/network/25-dhcp-server.network\n[Match]\nName=eth1\n\n[Network]\nAddress=10.0.1.1/24\nIPForward=yes\nDHCPServer=yes\n\n[DHCPServer]\nPoolOffset=10\nPoolSize=200\nEmitDNS=yes\nDNS=8.8.8.8\nAlthough more complicated than simple networking, subnetting can enhance security when configured properly. It improves isolation by putting separate sensitive services onto different subnets, segmentation by limiting broadcast domains to reduce the potential attack surface, and access control by implementing filters between subnets at the router level. You can see an example of a security-enhanced subnet configuration below, as well as a list of commands to troubleshoot your subnet with.\n# /etc/systemd/network/25-secure-subnet.network\n[Match]\nName=eth2\n\n[Network]\nAddress=10.0.3.1/24\nIPForward=yes\nIPMasquerade=yes  # NAT for outgoing connections\nConfigureWithoutCarrier=yes\n\n[DHCPServer]\nPoolOffset=50\nPoolSize=100\nEmitDNS=yes\nDNS=1.1.1.1\n\n# Restrict routes between subnets for this segment\n[Route]\nGateway=_ipv4gateway\nDestination=0.0.0.0/0\n# Check interface configuration\nip addr show\n\n# Verify routing tables\nip route show\nip route show table 200  # For custom route tables\n\n# Test connectivity between subnets\nping 10.0.1.1  # From another subnet\n\n# View ARP table to verify proxy ARP functionality\nip neigh show\n\n# Check systemd-networkd logs for issues\njournalctl -u systemd-networkd -n 50"
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html",
    "href": "pages/guides/posts/raspberry_pi_server.html",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "",
    "text": "This guide provides step-by-step instructions and explanations for configuring a Raspberry Pi 4 to learn about hardware, servers, containerization, and self-hosting principles. To be clear, this guide is not-exhaustive and I‚Äôm sure there were areas I made mistakes or misunderstood a topic. I‚Äôm inviting (encouraging) you to let me know! You can submit an issue via GitHub on the guide on my website. That being said, the primary purpose of this guide is so that I can go back and reference what I previously did, as well as understand the thought process, when I need to troubleshoot or recreate something. The secondary purpose is to provide a helpful resource for others in a similar situation to me, because I struggled to find the sort of comprehensive document I‚Äôm aiming to create.\nBack to this guide, eventually I‚Äôd like to setup an actual server cluster and self-host some interesting, more resource-intensive applications. Before I make that kind of commitment, I wanted to learn the basics and see if this is something I would enjoy‚Äì the good news, I learned I do. The great news, Raspberry Pi makes their hardware very affordable and easy to purchase. Here‚Äôs the official webpage for the exact computer I bought.\nIt‚Äôs worth adding, I bought the 8GB Raspberry Pi 4. The price difference isn‚Äôt that great, but the performance is, between the lesser 2GB and 4GB models. Additionally, because I‚Äôm planning to host and experiment with CI/CD, I also bought a case and cooling fan to help with longevity. All in, the base price for that (with the power supply and HDMI cable) is $107.30 before taxes, shipping, and other fees.\nFinally, you‚Äôll see an outline below, you can gloss over it to get a general idea of what we‚Äôll be doing and in what order. At the start of each section I‚Äôll include a key terms list that has all of the fundamental terms which are important for a given topic.\n\n\n1. Introduction\n\nPurpose and scope of the guide\nWhat you‚Äôll learn and build\nPrerequisites and assumptions\n\n2. Initial Setup\n\nHardware Requirements\n\nRaspberry Pi 4 specifications\nStorage devices (Thumbdrive, SSD, microSD cards)\nAccessories and peripherals (Keyboard, monitor, etc.)\n\nImage Requirements\n\nSelecting and downloading Ubuntu Server LTS\nUsing Raspberry Pi Imager\nInitial configuration options\n\nGet Started\n\nThe physical setup of the Raspberry Pi, what to plug in, where, etc.\nWhat to expect as things turn on\n\n\n3. Linux Server Basics\n\nFirst Boot Process\n\nConnection and startup\nUnderstanding initialization\n\nService Management with systemd\n\nUnderstanding systemd units and targets\nBasic service commands\n\nUnderstanding Your Home Directory\n\nShell configuration files\nHidden application directories\n\nThe Root Filesystem\n\nFilesystem Hierarchy Standard (FHS)\nKey directories and their purposes\n\nUser and Group Permissions\n\nBasic permission concepts\nchmod and chown usage\nUnderstanding advanced permissions\n\n\n4. Networking Basics\n\nComputer Networking Basics\n\nOSI and TCP/IP models\nKey networking protocols\n\nNetwork Connections\n\nWired vs wireless configurations\nUnderstanding IP addressing\n\nUbuntu Server Networking Tools\n\nTesting connectivity\nViewing network statistics\n\nsystemd-networkd\n\nConfiguration file structure\nWired and wireless setup\n\nConverting Netplan to networkd\n\nWhy and how to transition\nTroubleshooting network issues\n\nAdvanced Networking\n\nSubnets and routing\nSecurity considerations\n\n\n5. SSH (Secure Shell)\n\nSSH Basics\n\nClient vs server setup\nKey-based authentication\n\nKey-Based Authentication\n\nTypes of SSH key encryption\nGenerating keys\nInstalling the public key\n\nServer-Side SSH Configuration\n\nHost keys and security options\nOptimizing for security\n\nClient-Side Configuration\n\nSetting up SSH config\nManaging known hosts\n\nAdditional Security Measures\n\nFirewall configuration with UFW\nIntrusion prevention with Fail2Ban\n\nSecure File Transfers\n\nUsing SCP (Secure Copy Protocol)\nUsing rsync for efficient transfers\n\n\n6. Remote Development with VS Code\n\nSetting Up VS Code Remote SSH\nManaging Remote Projects\nDebugging and Terminal Integration\n\n7. Partitions\n\nPartitioning Basics\n\nUnderstanding partition tables and types\nFilesystem options and considerations\n\nPartitioning Tools\n\nUsing parted and other utilities\n\nPartitioning for Backups\n\nSetting up microSD cards\nMount points and fstab configuration\n\nPartitioning your SSD\n\nBoot and root partitions\nFormatting and preparation\n\nAdvanced Partitioning\n\nMonitoring usage\nResizing partitions\n\n\n8. Backups and Basic Automation\n\nBackup Basics\n\nDirectory structure and permissions\n\nConfiguration Files Backup\n\nUsing rsync for system configurations\nRemote transfers of backups\n\nRestoring from Backup\n\nCreating restoration scripts\nTesting recovery procedures\n\n\n9. Changing Your Boot Media Device\n\nBoot Configuration Transition\n\nFlashing OS to new media\nProper shutdown procedures\nPhysically changing boot devices\nTesting the new boot configuration\nRestoring configurations\n\n\n10. Monitoring and Maintenance\n\nMonitoring Basics\n\nsmartmon, vcgencmd\nResolving SSD health issues\n\nSecurity Updates and Patching\n\nPreventive Measures\nSystem patching schedules\n\nLog Management\n\nLog basics\nManagement tools and strategy\n\n\n11. Containerization with Docker\n\nContainerization and Virtualization Basics\nDocker Installation and Setup\nCreating Images with Dockerfile\nManaging Images and Containers\nDocker Compose for Multi-container Applications\nCI/CD Integration\n\n12. Container Orchestration with Kubernetes\n\nOrchestration Basics\nKubernetes Concepts, Installation, and Setup\nSetting Up a Kubernetes Cluster\nDeployment Strategies\nManaging Resources\nScaling Applications"
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-introduction",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-introduction",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "",
    "text": "This guide provides step-by-step instructions and explanations for configuring a Raspberry Pi 4 to learn about hardware, servers, containerization, and self-hosting principles. To be clear, this guide is not-exhaustive and I‚Äôm sure there were areas I made mistakes or misunderstood a topic. I‚Äôm inviting (encouraging) you to let me know! You can submit an issue via GitHub on the guide on my website. That being said, the primary purpose of this guide is so that I can go back and reference what I previously did, as well as understand the thought process, when I need to troubleshoot or recreate something. The secondary purpose is to provide a helpful resource for others in a similar situation to me, because I struggled to find the sort of comprehensive document I‚Äôm aiming to create.\nBack to this guide, eventually I‚Äôd like to setup an actual server cluster and self-host some interesting, more resource-intensive applications. Before I make that kind of commitment, I wanted to learn the basics and see if this is something I would enjoy‚Äì the good news, I learned I do. The great news, Raspberry Pi makes their hardware very affordable and easy to purchase. Here‚Äôs the official webpage for the exact computer I bought.\nIt‚Äôs worth adding, I bought the 8GB Raspberry Pi 4. The price difference isn‚Äôt that great, but the performance is, between the lesser 2GB and 4GB models. Additionally, because I‚Äôm planning to host and experiment with CI/CD, I also bought a case and cooling fan to help with longevity. All in, the base price for that (with the power supply and HDMI cable) is $107.30 before taxes, shipping, and other fees.\nFinally, you‚Äôll see an outline below, you can gloss over it to get a general idea of what we‚Äôll be doing and in what order. At the start of each section I‚Äôll include a key terms list that has all of the fundamental terms which are important for a given topic.\n\n\n1. Introduction\n\nPurpose and scope of the guide\nWhat you‚Äôll learn and build\nPrerequisites and assumptions\n\n2. Initial Setup\n\nHardware Requirements\n\nRaspberry Pi 4 specifications\nStorage devices (Thumbdrive, SSD, microSD cards)\nAccessories and peripherals (Keyboard, monitor, etc.)\n\nImage Requirements\n\nSelecting and downloading Ubuntu Server LTS\nUsing Raspberry Pi Imager\nInitial configuration options\n\nGet Started\n\nThe physical setup of the Raspberry Pi, what to plug in, where, etc.\nWhat to expect as things turn on\n\n\n3. Linux Server Basics\n\nFirst Boot Process\n\nConnection and startup\nUnderstanding initialization\n\nService Management with systemd\n\nUnderstanding systemd units and targets\nBasic service commands\n\nUnderstanding Your Home Directory\n\nShell configuration files\nHidden application directories\n\nThe Root Filesystem\n\nFilesystem Hierarchy Standard (FHS)\nKey directories and their purposes\n\nUser and Group Permissions\n\nBasic permission concepts\nchmod and chown usage\nUnderstanding advanced permissions\n\n\n4. Networking Basics\n\nComputer Networking Basics\n\nOSI and TCP/IP models\nKey networking protocols\n\nNetwork Connections\n\nWired vs wireless configurations\nUnderstanding IP addressing\n\nUbuntu Server Networking Tools\n\nTesting connectivity\nViewing network statistics\n\nsystemd-networkd\n\nConfiguration file structure\nWired and wireless setup\n\nConverting Netplan to networkd\n\nWhy and how to transition\nTroubleshooting network issues\n\nAdvanced Networking\n\nSubnets and routing\nSecurity considerations\n\n\n5. SSH (Secure Shell)\n\nSSH Basics\n\nClient vs server setup\nKey-based authentication\n\nKey-Based Authentication\n\nTypes of SSH key encryption\nGenerating keys\nInstalling the public key\n\nServer-Side SSH Configuration\n\nHost keys and security options\nOptimizing for security\n\nClient-Side Configuration\n\nSetting up SSH config\nManaging known hosts\n\nAdditional Security Measures\n\nFirewall configuration with UFW\nIntrusion prevention with Fail2Ban\n\nSecure File Transfers\n\nUsing SCP (Secure Copy Protocol)\nUsing rsync for efficient transfers\n\n\n6. Remote Development with VS Code\n\nSetting Up VS Code Remote SSH\nManaging Remote Projects\nDebugging and Terminal Integration\n\n7. Partitions\n\nPartitioning Basics\n\nUnderstanding partition tables and types\nFilesystem options and considerations\n\nPartitioning Tools\n\nUsing parted and other utilities\n\nPartitioning for Backups\n\nSetting up microSD cards\nMount points and fstab configuration\n\nPartitioning your SSD\n\nBoot and root partitions\nFormatting and preparation\n\nAdvanced Partitioning\n\nMonitoring usage\nResizing partitions\n\n\n8. Backups and Basic Automation\n\nBackup Basics\n\nDirectory structure and permissions\n\nConfiguration Files Backup\n\nUsing rsync for system configurations\nRemote transfers of backups\n\nRestoring from Backup\n\nCreating restoration scripts\nTesting recovery procedures\n\n\n9. Changing Your Boot Media Device\n\nBoot Configuration Transition\n\nFlashing OS to new media\nProper shutdown procedures\nPhysically changing boot devices\nTesting the new boot configuration\nRestoring configurations\n\n\n10. Monitoring and Maintenance\n\nMonitoring Basics\n\nsmartmon, vcgencmd\nResolving SSD health issues\n\nSecurity Updates and Patching\n\nPreventive Measures\nSystem patching schedules\n\nLog Management\n\nLog basics\nManagement tools and strategy\n\n\n11. Containerization with Docker\n\nContainerization and Virtualization Basics\nDocker Installation and Setup\nCreating Images with Dockerfile\nManaging Images and Containers\nDocker Compose for Multi-container Applications\nCI/CD Integration\n\n12. Container Orchestration with Kubernetes\n\nOrchestration Basics\nKubernetes Concepts, Installation, and Setup\nSetting Up a Kubernetes Cluster\nDeployment Strategies\nManaging Resources\nScaling Applications"
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-setup",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-setup",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Initial Setup",
    "text": "Initial Setup\n\nKey Terms\nHardware Terminology:\n\nRaspberry Pi 4: A single-board computer developed by the Raspberry Pi Foundation, featuring a Broadcom BCM2711 processor, various RAM options, USB 3.0 ports, and GPIO pins.\nSoC (System on Chip): An integrated circuit that combines the components of a computer or electronic system into a single chip\nGPIO (General Purpose Input/Output): Programmable pins on the Raspberry Pi that allow interaction with physical components and sensors.\nSSD (Solid State Drive): Storage device using flash memory that offers faster access times and better reliability than traditional hard drives.\nBoot Media: The storage device containing the operating system files from which the computer starts.\nUSB 3.0: A USB standard offering data transfer speeds up to 5 Gbps, significantly faster than previous versions.\nmicroSD Card: A small form factor removable flash memory card used as storage media.\neMMC (embedded MultiMediaCard): An integrated flash storage solution often found in compact devices.\n\nSoftware and Imaging Terminology:\n\nUbuntu Server LTS: A long-term support version of Ubuntu‚Äôs server operating system, maintaining security updates for 5 years.\nRaspberry Pi Imager: Official software tool for flashing operating system images to SD cards and other storage devices.\nImage: A file containing the complete contents and structure of a storage device or filesystem.\nFlashing: The process of writing an operating system image to a storage device.\nHeadless Setup: Configuring a device to operate without a monitor, keyboard, or mouse.\nPublic-key Authentication: An authentication method using cryptographic key pairs instead of passwords.\n\nFirst Boot Terminology:\n\nCloud-Init: A utility used by Ubuntu Server to handle early initialization when a server instance boots for the first time.\nFirst Boot Experience: The initial setup process that occurs the first time an operating system is booted.\nInitial RAM Disk (initrd): A temporary root filesystem loaded into memory during the boot process.\nBootloader: Software that loads the operating system kernel into memory.\n\n\n\nHardware Requirements\nThis section will provide basic setup instructions, so you‚Äôll have the same tools I do and can follow along with this guide, step-by-step.\n\nRaspberry Pi 4 8GB\n\nMicro HDMI to HDMI cord (for direct access)\nProtective case\nCooling fan\nAppropriate Power Supply\n\nKeyboard (connected via USB for direct access)\nMonitor (for direct access)\n1TB Samsung T7 SSD (connected via USB for boot media/core memory)\n64GB Generic Flash Drive (used as the boot media when partitioning the SSD)\nAmazon Basics 128GB microSD card (or other microSD cards for backups media)\nSSH-capable devices for headless access\n\nI‚Äôm using a MacBook Air\nI prefer the MacOS and Terminal for personal development, because I use Windows at my day job\n\n\n\n\nImage Requirements\nOnce you have your hardware ready to go, you can being setting up the software. I‚Äôm using Linux Server LTS because it‚Äôs a stable version of Linux intended for headless, server environments. LTS means long-term support, so unlike the more frequently updated versions, these OS versions are supported for 5 years. Additionally, you‚Äôll want to use public-key authentication for better security purposes, but more on that in the SSH Section.\n\nHave your Thumb Drive ready and able to connect to either a laptop or desktop (whichever you plan to use with SSH)\nDownload Raspberry Pi Imager from the official website\nRun the Imager and configure your installation of the most recent Linux Server LTS image\n\nSelect your Raspberry Pi device\n\n\n\nSelect the OS Image you want to flash\n\n\n\nSelect the media storage device for the image\n\n\n\nConfigure settings\n\n Here, you‚Äôll configure your primary user ID and password; network connection; locale and timezone; and your hostname (the nickname your computer remembers the IP address as).\n Here, you‚Äôll configure your SSH settings. You should probably use public-key authentication only when dealing with SSH in your leave, but for learning purposes, you don‚Äôt need to at this time. Later on in this guide, I‚Äôll walk you through the steps to manually configure SSH, if you are unfamiliar.\n These are more preference based, but it‚Äôs nice to have the storage device automatically eject once the flashing is complete. Then, you just need to unplug it and plug it into your Raspberry Pi to get going.\n\n\n\nGet Started\nIt‚Äôs time to get the actual Raspberry Pi device setup. For most of this guide, I recommend leaving the Pi outside of the case, because it‚Äôll be easier to plug and unplug some of the devices‚Äì the microSD slot is not accessible while the case is on. Later, once we‚Äôve got everything configured and setup as we like, we will attach the fan and case, so it‚Äôs a bit safer and able to run in an always-on state. I‚Äôll share a picture of what my server looks like during the early development, and then later I‚Äôll show what it looks like with everything connected and setup.\n\nNow you‚Äôre ready to plug your boot media device (the Thumb Drive) into your Raspberry Pi. You should also connect a keyboard, monitor, and power supply. Once all of this is connected, your Raspberry Pi will boot up. Connecting a monitor and keyboard will allow you to directly interact with the system‚Äôs terminal. Ideally, you‚Äôll use SSH, but it may be helpful to have direct access in case there are any network issues. Eventually, the SSD will serve as the boot media and primary storage device for the server; however, we can‚Äôt modify its partitions while it‚Äôs serving as the boot device. So, we‚Äôll use a thumb drive as the boot media device, until we complete the partitions.\nWhen first connecting from the wired keyboard and monitor, let all of the start up processes finish running (these will hopefully have brackets with the word Success in green). Then, type in the name of the User ID you wish to login with, in my case it‚Äôs chris. Then, enter the password (no characters will show up as you type it in) and hit enter. You‚Äôll see a plaintext message telling you the OS version, some system information (memory usage, temperature, etc.), and you‚Äôll see a line where you can enter commands (the CLI). In my case, it looks like this: chris@ubuntu-pi-server:~$\nNow you can run some basic commands to see where you are and what you have available to you. Spoiler alert, you‚Äôre in your home directory and have no files. In my case it‚Äôs /home/chris, where the /home directory is owned by root and /chris is owned by my user‚Äì UID 1000 (the default for new users on a fresh system/image). Right now your directory will be empty, outside of some hidden folders like .ssh. More on this later.\nNext we‚Äôll cover what happened during the boot process, the basic structure of the Linux Server OS, and some important information related to permissions, before we move on to basic networking concepts and configurations."
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-linux_basics",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-linux_basics",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Linux Server Basics",
    "text": "Linux Server Basics\n\nKey Terms\nSystem Initialization Concepts:\n\nInitialization: The process of starting up the operating system and bringing it to an operational state.\nBoot Process: The sequence of steps that occur from powering on a computer to loading the operating system.\nBIOS/UEFI: Firmware interfaces that initialize hardware and start the boot process.\nKernel: The core component of an operating system that manages system resources and hardware.\n\nSystemd Terminology:\n\nsystemd: The init system and system/service manager used by modern Linux distributions.\nUnit: Systemd‚Äôs representation of system resources, including services, devices, and mount points.\nService Unit: Configuration files with .service extension that define how to start, stop, and manage daemons.\nSocket Unit: Configuration files with .socket extension that define communication sockets.\nTimer Unit: Configuration files with .timer extension that trigger actions at specified times.\nTarget: A grouping of units that represents a system state (similar to runlevels in older systems).\nDaemon: A background process that runs continuously, providing services.\n\nFile System and Directory Terminology:\n\nFHS (Filesystem Hierarchy Standard): The standard directory structure and contents of Unix-like operating systems.\nRoot Directory (/): The top-level directory in a Linux filesystem hierarchy.\nhome Directory (/home): Contains user home directories where personal files are stored.\netc Directory (/etc): Contains system-wide configuration files.\nbin Directory (/bin): Contains essential command binaries needed for system functionality.\nHidden Files/Directories: Files or directories that begin with a dot (.) and don‚Äôt appear in default directory listings.\nShell Configuration Files: Files like .bashrc and .profile that configure the user‚Äôs command-line environment.\n\nUser and Permissions Terminology:\n\nUser: An account on a Linux system with specific privileges and access rights.\nGroup: A collection of users with shared permissions to files and directories.\nUID (User ID): A numerical identifier assigned to each user on a Linux system.\nGID (Group ID): A numerical identifier assigned to each group on a Linux system.\nPermission: Access rights assigned to users and groups determining what actions they can perform on files and directories.\nchmod: Command used to change file and directory permissions.\nchown: Command used to change file and directory ownership.\numask: Default permissions applied to newly created files and directories.\nACL (Access Control List): Extended permissions that provide more granular control than traditional Unix permissions.\nsetuid/setgid: Special permissions that allow users to execute files with the permissions of the file owner or group.\nSticky Bit: A special permission bit that restricts file deletion in shared directories.\n\n\n\nFirst Boot Process\nWhen you first boot a fresh Ubuntu Server LTS image on your Raspberry Pi, several important initialization processes occur that don‚Äôt happen during subsequent boots. The first boot of your Ubuntu Server LTS on the Raspberry Pi is fundamentally different from subsequent boots because it performs one-time initialization tasks. While later boots will simply load the configured system, this first boot sets up critical system components.\n\nHardware Detection: The system performs comprehensive hardware detection to identify and configure your Raspberry Pi‚Äôs components.\nInitial RAM Disk (initrd): The bootloader loads a temporary filesystem into memory that contains essential drivers and modules needed to mount the real root filesystem.\nFilesystem Check and Expansion: On first boot, the system checks the integrity of the filesystem and often expands it to utilize the full available space on your Flash Drive.\nCloud-Init Processing: Ubuntu Server uses cloud-init to perform first-boot configuration tasks (the processes you see running on the monitor on startup)\n\nSetting the hostname\nGenerating SSH host keys\nCreating the default user account\nRunning package updates\n\nMachine ID Generation: A unique machine ID is generated and stored in /etc/machine-id.\nNetwork Configuration: The system attempts initial network setup based on detected hardware.\n\nThe key difference is that subsequent boots skip these initialization steps since they‚Äôve already been completed, making them significantly faster.\n\n\nService Management with systemd\nSystemd is the modern initialization and service management system for Linux. It‚Äôs responsible for bootstrapping the user space and managing all processes afterward. Key components of systemd include:\n\nUnits: Everything systemd manages is represented as a ‚Äúunit‚Äù with a corresponding configuration file. Units include:\n\nService units (.service): Define how to start, stop, and manage daemons (background processes that are always on)\nSocket units (.socket): Manage network/IPC sockets\nTimer units (.timer): Trigger other units based on timers\nMount units (.mount): Control filesystem mount points\n\nTarget units: Represent system states (similar to runlevels in older systems)\n\nmulti-user.target: Traditional text-mode system\ngraphical.target: Graphical user interface\nnetwork.target: Network services are ready\n\n\nFor example, let‚Äôs take a look at a generic SSH service file.\n[Unit]\nDescription=OpenSSH server daemon\nDocumentation=man:sshd(8) man:sshd_config(5)\nAfter=network.target auditd.service\nWants=network.target\n\n[Service]\nEnvironmentFile=-/etc/default/ssh\nExecStartPre=/usr/sbin/sshd -t\nExecStart=/usr/sbin/sshd -D $SSHD_OPTS\nExecReload=/usr/sbin/sshd -t\nExecReload=/bin/kill -HUP $MAINPID\nKillMode=process\nRestart=on-failure\nRestartPreventExitStatus=255\nType=notify\n\n[Install]\nWantedBy=multi-user.target\nTo break this down:\n\n[Unit]: Metadata and dependencies\n\nDescription: Human-readable service description\nDocumentation: Where to find documentation\nAfter: Units that should be started before this one\nWants: Soft dependencies\n\n[Service]: Runtime behavior\n\nExecStart: Command to start the service\nExecReload: Command to reload the service\nRestart: When to restart the service\nType: How systemd should consider the service started\n\n[Install]: Installation information\n\nWantedBy: Target that should include this service\n\n\nUbuntu Server‚Äôs current standard is systemd, but previously it was SysV. A few key improvements include:\n\nParallel Service Startup: Systemd can start services in parallel, improving boot times.\nDependency Management: Systemd handles service dependencies more effectively.\nService Supervision: Systemd continuously monitors and can automatically restart services.\nSocket Activation: Services can be started on-demand when a connection request arrives.\n\nManaging services is easy using the command line, a crucial component of headless applications, a few examples are:\n\nView service status: systemctl status ssh\nStart a service: sudo systemctl start ssh\nStop a service: sudo systemctl stop ssh\nEnable at boot: sudo systemctl enable ssh\nDisable at boot: sudo systemctl disable ssh\nView logs: journalctl -u ssh\n\n\n\nUnderstanding Your Home Directory\nNow that you‚Äôve logged in and can work on your server, you may wonder where you are and what‚Äôs there. Running pwd will return the file path of your current location. Running ls -a will show you all available files and directories in your current location. Running these, you‚Äôll see a few things specifically for Shell configuration (your terminal/CLI):\n\n.bash_history: Contains a record of commands you‚Äôve executed in the bash shell. This helps with command recall using the up arrow or history command.\n.bash_logout: Executed when you log out of a bash shell. Often used for cleanup tasks like clearing the screen.\n.bashrc: The primary bash configuration file that‚Äôs loaded for interactive non-login shells. It defines aliases, functions, and shell behavior. When you open a terminal window, this file is read.\n.profile: Executed for login shells. It typically sets environment variables and executes commands that should run once per login session, not for each new terminal.\n\n# Sample .bashrc section\n# enable color support of ls and also add handy aliases\nif [ -x /usr/bin/dircolors ]; then\n    test -r ~/.dircolors && eval \"$(dircolors -b ~/.dircolors)\" || eval \"$(dircolors -b)\"\n    alias ls='ls --color=auto'\n    alias grep='grep --color=auto'\n    alias fgrep='fgrep --color=auto'\n    alias egrep='egrep --color=auto'\nfi\n\n# some more ls aliases\nalias ll='ls -alF'\nalias la='ls -A'\nalias l='ls -CF'\nBeyond those, you‚Äôll also find hidden application directories.\n\n.cache: Contains non-essential data that can be regenerated as needed. Applications store temporary files here to improve performance on subsequent runs.\n.dotnet: Contains .NET Core SDK and runtime files if you‚Äôve installed the .NET development platform.\n.ssh: Stores SSH configuration files and keys:\n\nauthorized_keys: Lists public keys that can authenticate to your account\nubuntu_pi_ecdsa & ubuntu_pi_ecdsa.pub: Your private and public ECDSA keys (More on this in the SSH Section)\nknown_hosts: Tracks hosts you‚Äôve connected to previously\nssh_config: Optional configuration file for SSH connections\n\n.sudo_as_admin_successful: A marker file created when you successfully use sudo. Its presence suppresses the ‚Äúsudo capabilities‚Äù message when opening a terminal.\n.vscode-server: Created when you connect to your server using Visual Studio Code‚Äôs remote development feature. Contains the VS Code server components. (More on this in the SSH Section)\n.wget-hsts: Wget‚Äôs HTTP Strict Transport Security database. Tracks websites that require secure (HTTPS) connections.\n\n\n\nThe Root Filesystem\nThe Linux filesystem follows the Filesystem Hierarchy Standard (FHS), which defines the directory structure and contents of Unix-like systems. Here‚Äôs a breakdown of key directories:\n\n/bin: Contains essential command binaries (programs) needed for basic system functionality. These commands are available to all users and are required during boot or in single-user mode.\n\nHistorical note: Originally separated from /usr/bin because early Unix systems had limited disk space on the root partition.\n\n/boot: Contains boot loader files including the Linux kernel, initial RAM disk (initrd), and bootloader configuration (GRUB).\n\nFor Raspberry Pi, this contains the firmware and various boot-related files.\n\n/dev: Contains device files that represent hardware devices. These are not actual files but interfaces to device drivers in the kernel.\n\nExample: /dev/sda represents the first SATA disk.\n\n/etc: Contains system-wide configuration files. The name originated from ‚Äúet cetera‚Äù but is now often interpreted as ‚ÄúEditable Text Configuration.‚Äù Critical files include:\n\n/etc/fstab: Filesystem mount configuration\n/etc/passwd: User account information\n/etc/ssh/sshd_config: SSH server configuration\n\n/home: Contains user home directories where personal files and user-specific configuration files are stored.\n/lib: Contains essential shared libraries needed by programs in /bin and system boot.\n\nOn modern 64-bit systems, you‚Äôll also find /lib64 for 64-bit libraries.\n\n/media: Mount point for removable media like USB drives and DVDs.\n/mnt: Temporarily mounted filesystems. This is often used as a manual mount point.\n/opt: Optional application software packages. Used for third-party applications that don‚Äôt follow the standard file system layout.\n/proc: A virtual filesystem providing process and kernel information. Files here don‚Äôt exist on disk but represent system state.\n\nExample: /proc/cpuinfo shows CPU information.\n\n/root: Home directory for the root user. Separated from /home to ensure it‚Äôs available even if /home is on a separate partition.\n/run: Runtime data for processes started since last boot. This is a tmpfs (memory-based) filesystem.\n/sbin: System binaries for system administration tasks, typically only usable by the root user.\n/srv: Data for services provided by the system, such as web or FTP servers.\n/snap: The /snap directory is, by default, where the files and folders from installed snap packages appear on your system.\n/sys: Another virtual filesystem exposing device and driver information from the kernel. Provides a more structured view than /proc.\n/tmp: Temporary files that may be cleared on reboot. Applications should not rely on data here persisting.\n/usr: Contains the majority of user utilities and applications. Originally stood for ‚ÄúUnix System Resources.‚Äù\n\n/usr/bin: User commands\n/usr/lib: Libraries for the commands in /usr/bin\n/usr/local: Locally installed software\n/usr/share: Architecture-independent data\n\n/var: Variable data files that change during normal operation:\n\n/var/log: System log files\n/var/mail: Mail spool\n/var/cache: Application cache data\n/var/spool: Spool for tasks waiting to be processed (print queues, outgoing mail)\n\n\nThe core philosophy behind this structure separates:\n\nStatic vs.¬†variable content\nShareable vs.¬†non-shareable files\nEssential vs.¬†non-essential components\n\nUnderstanding this hierarchy helps you navigate any Linux system and locate important files intuitively.\n\n\nUser and Group Permissions\n\nBasics\nLinux inherits its permission system from Unix, providing a robust framework for controlling access to files and directories. Understanding this system is essential for maintaining security and proper functionality of your Raspberry Pi server, as well as any Linux based system. At its core, the Linux permission model operates with three basic permission types applied to three different categories of users:\n\nPermission Types:\n\nRead (r): Allows viewing file contents or listing directory contents\nWrite (w): Allows modifying file contents or creating/deleting files within a directory\nExecute (x): Allows running a file as a program or accessing files within a directory\n\nUser Categories:\n\nOwner (u): The user who owns the file or directory\nGroup (g): Users who belong to the file‚Äôs assigned group\nOthers (o): All other users on the system\n\n\nIt‚Äôs not only important to know how to set permissions, but also how to view existing ones. When you run ls -l in a directory, you‚Äôll see a detailed listing including permission information.\n-rw-r--r-- 1 chris chris 1234 May 6 14:32 example.txt\nIn this example, the owner can read and write, while group members and others can only read. The first string of characters -rw-r‚Äìr‚Äì represents the permissions:\n\nFirst character: File type (- for regular file, d for directory, l for symbolic link)\nCharacters 2-4: Owner permissions (rw-)\nCharacters 5-7: Group permissions (r‚Äì)\nCharacters 8-10: Others permissions (r‚Äì)\n\n\n\nchmod\nThe chmod command modifies file permissions in Linux. You can use it in two ways: symbolic mode or numeric (octal) mode.\nSymbolic mode uses letters to represent permission categories (u, g, o, a) and permissions (r, w, x):\n# Give the owner execute permission\nchmod u+x script.sh\n\n# Remove write permission from group and others\nchmod go-w important_file.txt\n\n# Set read and execute for everyone (a=all users)\nchmod a=rx application\n\n# Add write permission for owner and group\nchmod ug+w shared_document.txt\nEach symbol has a specific meaning:\n\nu: Owner permissions\ng: Group permissions\no: Other user permissions\na: All permissions\n+: Add permissions\n-: Remove permissions\n=: Set exact permissions\n\nOctal mode represents permissions as a 3-digit number, where each digit represents the permissions for owner, group, and others:\n\nRead (r) = 4\nWrite (w) = 2\nExecute (x) = 1\n\nPermissions are calculated by adding these values:\n\n7 (4+2+1) = Read, write, and execute\n6 (4+2) = Read and write\n5 (4+1) = Read and execute\n4 (4) = Read only\n0 = No permissions\n\n# rwxr-xr-x (755): Owner can read, write, execute; group and others can read and execute\nchmod 755 script.sh\n\n# rw-r--r-- (644): Owner can read and write; group and others can read only\nchmod 644 document.txt\n\n# rwx------ (700): Owner has all permissions; group and others have none\nchmod 700 private_directory\nBeyond the basic rwx permissions, Linux has three special permission bits:\n\nsetuid (4000): When set on an executable file, it runs with the privileges of the file owner instead of the user executing it.\nsetgid (2000): Similar to setuid but for group permissions. When set on a directory, new files created within inherit the directory‚Äôs group.\nsticky bit (1000): When set on a directory, files can only be deleted by their owner, the directory owner, or root (commonly used for /tmp).\n\n\n\nchown\nThe chown command changes the owner and/or group of files and directories. Do not change ownership in the root directories because many require specific ownership/permissions to function properly.\n# Change the owner of a file\nsudo chown chris file.txt\n\n# Change both owner and group\nsudo chown chris:developers project_files\n\n# Change only the group\nsudo chown :developers shared_documents\n\n# Change recursively for a directory and all its contents\nsudo chown -R chris:chris /home/chris/projects\nThe flags do the following:\n\n-R, --recursive: Change ownership recursively\n-c, --changes: Report only when a change is made\n-f, --silent: Suppress most error messages\n-v, --verbose: Output a diagnostic for every file processed\n\n# Verbose recursive ownership change\nsudo chown -Rv chris:developers /opt/application\n\n\nUnderstanding Permissions\nLinux manages permissions through users and groups:\n\nEach user has a unique User ID (UID)\nEach group has a unique Group ID (GID)\nUsers can belong to multiple groups\nThe first 1000 UIDs/GIDs are typically reserved for system users/groups\n\nImportant files include:\n\n/etc/passwd: Contains basic user account information\n\nFields: username, password placeholder, UID, primary GID, full name, home directory, login shell\n\n\nchris:x:1000:1000:Chris Kornaros:/home/chris:/bin/bash\n\n/etc/shadow: Contains encrypted passwords and password policy information\n\nFields: username, encrypted password, days since epoch of last change, min days between changes, max days password valid, warning days, inactive days, expiration date\n\n\nchris:$6$xyz...hash:19000:0:99999:7:::\n\n/etc/group: Contains group definitions\n\nFields: group name, password placeholder, GID, comma-separated list of members\n\n\ndevelopers:x:1001:chris,bob,alice\nThere are two categories of groups you should understand, Primary and Supplementary:\n\nPrimary Group: Set in /etc/passwd, used as the default group for new files\nSupplementary Groups: Additional groups a user belongs to, defined in /etc/group\n\nYou can view your current user‚Äôs groups with the groups command, or view them for a specific user with groups chris (replace chris with the name of the user). That being said, directory permissions differ slightly from file permissions:\n\nRead (r): List directory contents\nWrite (w): Create, delete, or rename files within the directory\nExecute (x): Access files within the directory (crucial for navigation)\n\n\n\n\n\n\n\nTip\n\n\n\nA common confusion: You may have read permission for a file but not execute permission for its parent directory, preventing access.\n\n\nThe umask (user mask) determines the default permissions for newly created files and directories:\n\nDefault for files: 666 (rw-rw-rw-)\nDefault for directories: 777 (rwxrwxrwx)\nThe umask is subtracted from these defaults, for example, a umask of 022 results in:\n\nFiles: 644 (rw-r‚Äìr‚Äì)\nDirectories: 755 (rwxr-xr-x)\n\n\n# View current umask (in octal)\numask\n\n# Set a new umask\numask 027  # More restrictive: owner full access, group read/execute, others no access\nTraditional Unix permissions have limitations regarding inheritance: new files don‚Äôt inherit permissions from parent directories and changing permissions doesn‚Äôt affect existing files. Modern solutions, however, include: the setgid bit on directories for group inheritance and ACLs (Access Control Lists) with default entries that apply to new files. To setup a collaborative directory with proper permissions:\n# Create a shared directory for developers\nsudo mkdir /opt/projects\nsudo chown chris:developers /opt/projects\nsudo chmod 2775 /opt/projects  # setgid bit ensures new files get 'developers' group\n\n\nAdvanced Permission Concepts\nLike I previously wrote, part of the modern permission solutions include ACLs, or Access Control Lists. ACLs extend the traditional permission model to allow specifying permissions for multiple users and groups. When ACLs are in use, ls -l will show a + after the permission bits. Here‚Äôs a basic example of how to create and manage an ACL:\n# Install ACL support (if not already installed)\nsudo apt install acl\n\n# Set an ACL allowing a specific user read access\nsetfacl -m u:chris:r file.txt\n\n# Set an ACL allowing a specific group write access\nsetfacl -m g:developers:rw file.txt\n\n# Set default ACLs on a directory (inherited by new files)\nsetfacl -d -m g:developers:rw directory/\n\n# View ACLs on a file\ngetfacl file.txt\n-rw-rw-r--+ 1 chris developers 1234 May 6 14:32 file.txt\nA few final notes on permissions that are especially relevant for this project, becaue you‚Äôll be working with external storage devices:\n\nNot all filesystems support the same permission features:\n\next4: Full support for traditional permissions, ACLs, and extended attributes\nNTFS (via NTFS-3G): Simulated Unix permissions, basic ACL support\nFAT32: No native permission support (mounted with fixed permissions)\nexFAT: No native permission support\n\nCommon Permission Patterns:\n\nConfiguration Files: 644 or 640 (owner can edit, restricted read access)\nProgram Binaries: 755 (everyone can execute, only owner can modify)\nWeb Content: 644 for regular files, 755 for directories\nSSH Keys: 600 for private keys (owner only), 644 for public keys\nScripts: 700 or 750 (executable by owner or group)"
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-network",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-network",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Computer Networking",
    "text": "Computer Networking\nThis section provides a brief example of how to connect your server to WiFi. It assumes you are already connected using the wireless network settings you configured in the Requirements Section. That being said, I‚Äôll also go over some basic networking concepts and background information. As a result, some of the decisiions and terminology in this guide will make more sense (it also helps me remember what I‚Äôm doing and why).\n\nKey Terms\nBasic Networking Concepts:\n\nProtocol: A set of rules that determine how data is transmitted between devices on a network. Examples include TCP, UDP, and HTTP.\nMAC Address: Media Access Control address; a unique hardware identifier assigned to network interfaces. It‚Äôs a 48-bit address (e.g., 00:1A:2B:3C:4D:5E) permanently assigned to a network adapter.\nIP Address: A numerical label assigned to each device on a network that uses the Internet Protocol. Functions like a postal address for devices.\nPacket: A unit of data transmitted over a network. Includes both the data payload and header information for routing.\nSubnet: A logical subdivision of an IP network that allows for more efficient routing and security segmentation.\nGateway: A network node that serves as an access point to another network, typically connecting a local network to the wider internet.\nDNS: Domain Name System; translates human-readable domain names (like google.com) into IP addresses computers can understand.\nDHCP: Dynamic Host Configuration Protocol; automatically assigns IP addresses and other network configuration parameters to devices.\n\nNetwork Types and Components:\n\nLAN: Local Area Network; a network confined to a small geographic area, like a home or office.\nWAN: Wide Area Network; connects multiple LANs across large geographic distances.\nRouter: A device that forwards data packets between computer networks, determining the best path for data transmission.\nSwitch: A networking device that connects devices within a single network and uses MAC addresses to forward data to the correct destination.\nBandwidth: The maximum data transfer rate of a network connection, measured in bits per second (bps).\nLatency: The delay between sending and receiving data, typically measured in milliseconds.\n\nLinux Networking Terminology:\n\nInterface: A connection between a device and a network. In Linux, these have names like eth0 (Ethernet) or wlan0 (wireless).\nNetplan: Ubuntu‚Äôs default network configuration tool that uses YAML files to define network settings.\nsystemd-networkd: A system daemon that manages network configurations in modern Linux distributions.\nNetworkManager: An alternative network management daemon that provides detection and configuration for automatic network connectivity.\nSocket: An endpoint for sending or receiving data across a network, defined by an IP address and port number.\n\nSecurity Concepts:\n\nFirewall: Software or hardware that monitors and filters incoming and outgoing network traffic based on predetermined security rules.\nSSH: Secure Shell; a cryptographic network protocol for secure data communication and remote command execution.\nEncryption: The process of encoding information to prevent unauthorized access.\nPort: A virtual point where network connections start and end. Ports are identified by numbers (0-65535).\nNAT: Network Address Translation; allows multiple devices on a local network to share a single public IP address.\nVPN: Virtual Private Network; extends a private network across a public network, enabling secure data transmission.\n\n\n\nComputer Networking\nSimply put, a computer network is a collection of interconnected devices that can communicate with each other using a set of rules called protocols. Networking allows devices to share resources, exchange data, and collaborate on tasks. On a deeper level, it helps to understand the conceptual models that describes how data moves through a network. Before we dive in, let‚Äôs go over some basic terminology.\n\nThe OSI Model\nNow that you understand some common terms and concepts, we can dive into the conceptual models. The Open Systems Interconnection (OSI) Model divides networking into seven layers, each handling specific aspects of network communication.\n\nPhysical Layer: Physical medium, electrical signals, cables, and hardware\nData Link Layer: Physical addressing (MAC addresses), error detection\nNetwork Layer: Logical addressing (IP addresses), routing\nTransport Layer: End-to-end connections, reliability (TCP/UDP)\nSession Layer: Session establishment, management, and termination\nPresentation Layer: Data translation, encryption, compression\nApplication Layer: User interfaces and services (HTTP, SMTP, etc.)\n\n\n\nThe TCP/IP Model\nThe OSI Model is conceptual, but the TCP/IP Model is more practical and has four layers.\n\nNetwork Access Layer:Combines OSI‚Äôs Physical and Data Link layers\nInternet Layer: Similar to OSI‚Äôs Network layer (IP)\nTransport Layer: Same as OSI‚Äôs Transport layer (TCP/UDP)\nApplication Layer: Combines OSI‚Äôs Session, Presentation, and Application layers\n\n\n\nNetwork Protocols\nRemember, a protocol is a set of rules that determine how data is transmitted between devices on a network. You can think of protocols in one of two camps, Connection-Oriented and Connectionless. Within these camps, two protocols stand out as the backbone of the internet‚Äôs data transfers: TCP and UDP.\nTCP (Transmission Control Protocol) is a connection-oriented protocol that establishes a dedicated end-to-end connection before transmitting data. TCP is used when reliability is more important than speed (e.g., web browsing, email, file transfers). It has four defining traits:\n\nReliability: Guarantees delivery of packets in the correct order\nFlow Control: Prevents overwhelming receivers with too much data\nError Detection: Identifies and retransmits lost or corrupted packets\nHandshake Process: Three-way handshake establishes connections\n\nUDP (User Datagram Protocol) is a connectionless protocol that sends data without establishing a dedicated connection. UDP is used for real-time applications (e.g., video streaming, VoIP, online gaming). It also has four defining traits:\n\nSimplicity: No connection setup or maintenance overhead\nSpeed: Faster than TCP due to fewer checks and guarantees\nLower Reliability: No guarantee of delivery or correct ordering\nEfficiency: Better for real-time applications where occasional data loss is acceptable\n\nBeyond those, there are some other important protocols to know, because they provide the foundation for most of the user friendly features we are used to today.\n\nIP (Internet Protocol)\n\nIP handles addressing and routing of packets across networks. There are two versions in common use:\nIPv4: 32-bit addresses (e.g., 192.168.1.1)\nIPv6: 128-bit addresses (e.g., 2001:0db8:85a3:0000:0000:8a2e:0370:7334)\n\nICMP (Internet Control Message Protocol)\n\nICMP helps diagnose network issues by sending error messages and operational information. The ping command uses ICMP to test connectivity.\n\nHTTP/HTTPS (Hypertext Transfer Protocol)\n\nHTTP and its secure variant HTTPS are application-layer protocols used for web browsing.\n\nDNS (Domain Name System)\n\nDNS translates human-readable domain names (like google.com) into IP addresses.\n\n\n\n\n\nNetwork Connections\nThere are two ways for systems to connect to the internet: wired and wireless.\n\nWired Connections\nEthernet is the most common wired networking technology. Its name comes from the term ether referring to a theoretical medium that was believed to carry light waves through space. It was developed by Robert Metcalf and David Boggs at Xerox‚Äôs PARC facility in the 1970s. The goal was to provide a more stable LAN which could facilitate high speed transfers between computers and laser printers. They succeeded, and had improved on a precursor‚Äôs, ALOHAnet, design by creating a system that could detect collisions‚Äì when two devices try to transmit at the same time. Here are some key traits:\n\nReliability: Less susceptible to interference\nSpeed: Typically faster and more stable than wireless\nSecurity: Harder to intercept without physical access\nConnectors: RJ45 connectors on Ethernet cables\nStandards: 10/100/1000 Mbps (Gigabit) are common speeds\n\n\n\nWireless Connections\nWi-Fi allows devices to connect to networks without physical cables. Its name is not short for Wireless Fidelity, but actually a marketing choice by the brand-consulting firm Interbrand. They chose the name because it sounded similar to Hi-Fi. Wi-Fi was developed by numerous researchers and engineers, but the key breakthrough was by Dr.¬†John O‚ÄôSullivan from CSIRO in Australia. His work focused on a wireless LAN, which would eventually become the IEEE (Institute of Electrical and Electronics Engineers) 802.11 standard in 1997. Eventually, Apple would help with widespread adoption by including the AirPort feature on its laptops, enabling W-Fi connectivity out of the box. Here are some key traits:\n\nConvenience: No cables required, more flexible placement\nStandards: 802.11a/b/g/n/ac/ax (Wi-Fi 6) with varying speeds and ranges\nSecurity: WEP, WPA, WPA2, and WPA3 encryption standards (WPA2/WPA3 recommended)\n\n\n\nNetwork Interface Names in Linux\nIn Ubuntu Server, network interfaces follow a predictable naming convention:\n\neth0, eth1: Traditional Ethernet interface names\nwlan0, wlan1: Traditional wireless interface names\nenp2s0, wlp3s0: Modern predictable interface names (based on device location)\n\n\n\nIP Addressing\nAn IP (Internet Protocol) Address, is a unique identifier for a device on the internet, or a LAN. There are two different kinds of addresses: IPv4 and IPv6.\nIPv4 uses 32-bit addresses, providing approximately 4.3 billion unique addresses (now largely exhausted):\n\nFormat: Four octets (numbers 0-255) separated by dots (e.g., 192.168.1.1)\nClasses: Traditionally divided into classes A, B, C, D, and E\nPrivate Ranges:\n\n10.0.0.0 to 10.255.255.255 (10.0.0.0/8)\n172.16.0.0 to 172.31.255.255 (172.16.0.0/12)\n192.168.0.0 to 192.168.255.255 (192.168.0.0/16)\n\nSubnet Masks: Used to divide networks (e.g., 255.255.255.0 or /24)\nIssues: IPv4 address exhaustion due to limited capacity\n\nIPv6 uses 128-bit addresses, providing approximately 3.4√ó10^38 unique addresses:\n\nFormat: Eight groups of four hexadecimal digits separated by colons (e.g., 2001:0db8:85a3:0000:0000:8a2e:0370:7334)\nShorthand: Leading zeros in a group can be omitted, and consecutive groups of zeros can be replaced with :: (only once)\n\nExample: 2001:db8:85a3::8a2e:370:7334\n\nAddress Types:\n\nUnicast: Single interface\nAnycast: Multiple interfaces (closest responds)\nMulticast: Multiple interfaces (all respond)\n\nBenefits: More addresses, improved security, simplified headers, no need for NAT\n\nOne final note, CIDR (Classless Inter-Domain Routing) notation represents IP addresses and their associated routing prefix:\n\nFormat: IP address followed by ‚Äú/‚Äù and prefix length (e.g., 192.168.1.0/24)\nCalculation: A prefix of /24 means the first 24 bits are the network portion, leaving 8 bits for hosts (allowing 2^8 = 256 addresses)\n\n\n\n\nUbuntu Server Networking Tools\nNow that we‚Äôve covered the basic concepts, it‚Äôs time to dive into the actual commands and tools that will let you configure and manage your server‚Äôs network. To start, you can view network interfaces and their statuses using the command ip link show, or ip addr show for your IP Address configuration. You can view only the IPv4 or IPv6 addresses using ip -4 addr or ip -6 addr, respectively.\n\nTesting Connectivity\nAlthough it seems redundant if you already viewed your IP addresses, you can also test connectivity using the ping and traceroute commands. These will be more useful for checking your servers network status from your desktop or laptop.\nTest basic connectivity to a host:\nping -c 4 google.com\n\nTrace the route to a destination:\n# First update and install your packages\nsudo apt update && sudo apt upgrade -y\n\n# Install traceroute\nsudo apt install traceroute -y\n\n# Run traceroute\ntraceroute google.com\n\nCheck the DNS resolution:\nnslookup google.com\n# dig google.com\n\n\n\nViewing Network Statistics\nYou can view more specific network information with the ss command. This command‚Äôs name is an acronym for socket statistics and is used as a replacement for the older netstat plan because it offers faster performance and a more detailed output. Additionally, you can filter by specific protocol.\nss -tuln\nThe tuln flag is made up of four separate flags:\n\n-t, displays only TCP sockets\n-u, displays only UDP sockets\n-l, displays listening sockets\n-n, displays address numerically, instead of resolving them\n\n\n\nConfiguration Files\nFinally, there are a few crucial configuration files that will handle the bulk of your networking. In Ubuntu Server, network interfaces and DNS configurations are configured and stored in the /etc/ directory.\nNetwork Interfaces:\n\n/etc/netplan/: Contains YAML configuration files for Netplan\n/etc/network/interfaces: Configuration method (if NetworkManager is used)\n\nDNS Configuration:\n\n/etc/resolv.conf: DNS resolver configuration\n/etc/hosts: Static hostname to IP mappings\n/etc/hostname: System hostname\n\n\n\n\nsystemd-networkd\nsystemd-networkd is a system daemon that manages network configurations in modern Linux distributions. It‚Äôs part of the systemd suite and provides network configuration capabilities through simple configuration files.\nIt generally works using three key components:\n\nConfiguration Files: You define network settings in .network files located in /etc/systemd/network/\nService Management: systemd-networkd runs as a system service to apply and maintain network configurations\nIntegration: Works closely with other systemd components for DNS resolution and networking\n\n\nBasic Wired Configuration\nsystemd-networkd uses configuration files with .network extension. Each file consists of sections with key-value pairs. A basic configuration for a static IP would look like this:\n# /etc/systemd/network/20-wired.network\n[Match]\nName=eth0\n\n[Network]\nAddress=192.168.1.100/24\nGateway=192.168.1.1\nDNS=8.8.8.8\nDNS=8.8.4.4\nLet‚Äôs walk through the configuration file‚Äôs structure:\n\nFile Naming Convention:\n\nThe file is named 20-wired.network.\nThe number prefix (20-) determines the processing order (lower numbers processed first), allowing you to create prioritized configurations.\nThe suffix .network tells systemd-networkd that this is a network interface configuration file.\n\n[Match] Section:\n\nThis critical section determines which network interfaces the configuration applies to.\nName=eth0: This specifies that the configuration should apply to the eth0 interface only.\nYou can use wildcards (e.g., eth* would match all Ethernet interfaces) or match by other properties such as MAC address using MACAddress=xx:xx:xx:xx:xx:xx.\nBehind the scenes:\n\nsystemd-networkd scans all available network interfaces.\nCompares their properties against those specified in the [Match] section.\nIf all properties match, the configuration is applied to that interface.\n\n\n[Network] Section:\n\nThis section defines the network configuration parameters.\nAddress=192.168.1.100/24: Sets a static IPv4 address with CIDR notation. The /24 represents the subnet mask (equivalent to 255.255.255.0) and defines the network boundary.\nGateway=192.168.1.1: Specifies the default gateway for routing traffic outside the local network. All traffic not destined for the local subnet (192.168.1.0/24) will be sent to this IP address.\nDNS=8.8.8.8 and DNS=8.8.4.4: These are Google‚Äôs public DNS servers. When specified, systemd-networkd will automatically configure /etc/resolv.conf through systemd-resolved. You can specify multiple DNS servers, and they will be tried in order.\n\nBehind the scenes:\n\nsystemd-networkd identifies the eth0 interface\nAssigns the static IP address using kernel netlink sockets\nSets up the routing table to use the specified gateway\nCommunicates with systemd-resolved to configure DNS settings\nMaintains this configuration and reapplies it if the interface goes down and back up\n\n\nThis configuration example works well for server environments where static, predictable networking is preferable. This is a declarative configuration, it describes the desired state, rather than the steps to achieve it, so repeated application produces the same result.\n\n\nDHCP with a Wired Connection\nIf you want to add DHCP, you can use the following:\n# /etc/systemd/network/20-wired.network\n[Match]\nName=eth0\n\n[Network]\nDHCP=yes\nLet‚Äôs walk through the differences between a dynamic and static host configuration file structure:\n\nDHCP=yes: This single line replaces all the static configuration parameters from the previous example.\n\nIt instructs systemd-networkd to obtain IP address, subnet mask, gateway, DNS servers, and other network parameters automatically from a DHCP server.\nYou can also use DHCP=ipv4 to enable only IPv4 DHCP, or DHCP=ipv6 for only IPv6 DHCP, or DHCP=yes for both.\n\nBehind the scenes:\n\nsystemd-networkd identifies the eth0 interface\nInitiates the DHCP client process, which follows the DHCP protocol‚Äôs Discover-Offer-Request-Acknowledge (DORA) sequence:\n\nThe client broadcasts a DISCOVER message\nAvailable DHCP servers respond with OFFER messages\nThe client selects an offer and sends a REQUEST\nThe selected server sends an ACKNOWLEDGE\n\nApplies all the received network parameters (IP, subnet, gateway, DNS)\nSets up a lease timer to manage when the configuration needs renewal\nHandles DHCP lease renewals automatically\n\nAdvantages:\n\nSimplified configuration maintenance - no need to update parameters when network details change\nWorks well in networks where IP assignments are centrally managed\nAutomatically adapts to network changes\n\n\nThis configuration works well for environments where network parameters are dynamic or managed by a network admin through DHCP.\n\n\nWireless Configurations and wpa_supplicant\nWhile wired connections are a basic part of networking, wireless connections require some extra work. More specifically, with systemd-networkd, you‚Äôll need a tool like WPA. Wi-Fi Protected Access (WPA) emerged as a response to weaknesses in the original Wired Equivalent Privacy (WEP) security protocol. As wireless networks became ubiquitous, secure authentication and encryption mechanisms became essential. The Linux ecosystem offers several powerful tools for managing these connections:\n\nwpa_supplicant: The core daemon that handles wireless connections\nwpa_cli: A command-line interface for controlling wpa_supplicant dynamically\nwpa_passphrase: A utility for generating secure password hashes\n\nOn the systemd-networkd side of things, the configuration is simple, broken down in detail below.\n# /etc/systemd/network/25-wireless.network\n[Match]\nName=wlan0\n\n[Network]\nDHCP=yes\n\nWireless Interface:\n\nThe configuration targets wlan0, which is the traditional name for the first wireless network interface in Linux.\n\nMinimal Configuration:\n\nThe file only has the information needed by systemd-networkd to manage the IP addressing aspect of the wireless connection. Note what‚Äôs missing: there‚Äôs no SSID, password, or security protocol information. This is because:\nsystemd-networkd isn‚Äôt designed to handle wireless authentication and association\nThis separation of concerns is intentional in the systemd design philosophy - specialized tools should handle specialized tasks\n\nIntegration with wpa_supplicant:\n\nwpa_supplicant is the standard Linux utility for managing wireless connections\nsystemd-networkd handles the network layer (Layer 3) configuration once wpa_supplicant establishes the data link layer (Layer 2) connection\nThis division follows the OSI model‚Äôs separation of network layers\n\nBehind the scenes:\n\nwpa_supplicant handles wireless scanning, authentication, and association\nOnce a wireless link is established, it notifies the system\nsystemd-networkd detects the active interface that matches wlan0\nIt then initiates the DHCP client process to configure the network parameters\n\nThis separation provides flexibility and security\n\nThe wireless security operations are handled by a dedicated, well-tested component\nNetworking remains under systemd-networkd‚Äôs control for consistency with other interfaces\n\n\nWhile the systemd-networkd configuration is straightforward, things get more complicated with WPA. In standard wpa_supplicant configuration files, wireless passwords are often stored in plaintext. This creates a security vulnerability - anyone with access to the configuration file can view the password.\n# /etc/wpa_supplicant/wpa_supplicant-wlan0.conf\nctrl_interface=/run/wpa_supplicant\nupdate_config=1\n\nnetwork={\n    ssid=\"YourNetworkSSID\"\n    psk=\"YourWiFiPassword\"\n}\nThe wpa_passphrase tool solves this problem by generating a pre-computed hash of the password. Running this is straightforward as the basic syntax is wpa_passphrase [SSID] [passphrase]. Then, WPA outputs a hashed version of your password.\n# Generate a hashed passphrase\nwpa_passphrase \"MyHomeNetwork\" \"MySecurePassword123\"\nTo then use the hashed password in your configuration, you can run the following command, just make sure to remove the line with the plaintext password from the config file after runtime:\n# Generate the hash and save directly to the configuration file\nwpa_passphrase \"MyHomeNetwork\" \"MySecurePassword123\" | sudo tee -a /etc/wpa_supplicant/wpa_supplicant-wlan0.conf\nWhen you use wpa_passphrase: - It combines the SSID and password using the PBKDF2 (Password-Based Key Derivation Function 2) algorithm - It applies 4096 iterations of HMAC-SHA1 for key strengthening - The result is a 256-bit (32-byte) hash represented in hexadecimal format - This hash is what‚Äôs actually used for the authentication process, not the original password\nThis approach makes it virtually impossible to reverse-engineer the original password from the hash.\n# /etc/wpa_supplicant/wpa_supplicant-wlan0.conf\nctrl_interface=/run/wpa_supplicant\nupdate_config=1\n\nnetwork={\n    ssid=\"MyHomeNetwork\"\n    #psk=\"MySecurePassword123\"\n    psk=a8e665b82929d810746c5a1208c472f9d2a25db67a6bc32a99fa4158aea02175\n}\nNow that you have an idea about the basic structure of this file, lets go over some key points:\n\nFile Naming Convention:\n\nThe file wpa_supplicant-wlan0.conf is specifically named to associate with the wlan0 interface.\nThis naming allows different wireless interfaces to have different configurations.\n\nConfiguration Directives:\n\nctrl_interface=/run/wpa_supplicant: This specifies the control interface path, which is a socket that allows programs to communicate with wpa_supplicant. This enables tools like wpa_cli to connect and control wpa_supplicant dynamically.\nupdate_config=1: Allows wpa_supplicant to update the configuration file automatically, useful when network details change or when using wpa_cli to add networks interactively.\n\nNetwork Block:\n\nThe network={} block defines a single wireless network configuration.\nssid=\"YourNetworkSSID\": The Service Set Identifier - the name of the wireless network to connect to.\npsk=\"YourWiFiPassword\": The Pre-Shared Key - the password for the wireless network in plaintext.\n\nSecurity Considerations:\n\nWhen you enter the password in plaintext as shown, wpa_supplicant will automatically convert it to a hash during processing.\nFor better security, you can pre-hash the password using: wpa_passphrase ‚ÄúYourNetworkSSID‚Äù ‚ÄúYourWiFiPassword‚Äù and use the generated hash.\nThe configuration file should have restricted permissions (600) to prevent other users from reading the passwords.\n\nBehind the scenes:\n\nwpa_supplicant reads this configuration at startup\nIt scans for available wireless networks\nWhen it finds the specified SSID, it attempts to authenticate using the provided credentials\nIt handles all the wireless protocol handshakes, including:\n\nAuthentication and association with the access point\nNegotiation of encryption parameters\nEstablishment of the encrypted channel\n\n\n\nOnce connected, it maintains the connection and handles roaming between access points with the same SSID. This configuration represents the minimum needed for a WPA/WPA2 Personal network connection. For more complex scenarios like enterprise authentication (WPA-EAP), additional parameters would be needed in the network block.\nWhile the wpa_supplicant configuration files provide static configuration that saves when you write out, wpa_cli offers interactive, dynamic control over wireless connections. First ensure wpa_supplicant is running with a control interface by using ps aux | grep wpa_supplicant. If it‚Äôs running with the -c flag pointing to a config file that contains the ctrl_interface=/run/wpa_supplicant line, you can connect to it.\n\n\n\n\n\n\nImportant\n\n\n\nAs a heads up, because we already created the wlan0 configuration file manually, the following steps are just for your knowledge. You‚Äôll probably get some messages saying FAIL if you try to run some of the commands, but I think it‚Äôs good to learn them anyways‚Äì even though they aren‚Äôt necessarilly important right now.\n\n\nFirst, start the interactive mode with sudo wpa_cli, or specify the interface with sudo wpa_cli -i wlan0. Let‚Äôs go over some essential commands:\n# Show help\nhelp\n\n# List all available commands\nhelp all\n\n# List available networks\nscan\nscan_results\n\n# Show current status\nstatus\n\n# List configured networks\nlist_networks\n\n# Add a new network\nadd_network\nStep-By-Step: Adding a Network\n&gt; add_network\n0\n&gt; set_network 0 ssid \"MyNetwork\"\nOK\n&gt; set_network 0 psk \"MyPassword\"\nOK\n&gt; set_network 0 priority 5 \nOK\n&gt; enable_network 0\nOK\n&gt; save_config\nOK\n\n# For networks with hashed passwords\n&gt; add_network\n0\n&gt; set_network 1 ssid \"MyNetwork\"\nOK\n&gt; set_network 1 psk 0a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0u1v2w3x4y5z\nOK\n&gt; set_network 1 priority 10\nOK\n&gt; enable_network 1\nOK\n&gt; save_config\nOK\nIt‚Äôs good to know that higher priority values (like 10), are preferred over lower ones (like 5). Now, you can also use wpa by running one off commands or writing scripts with non-interactive mode. Additionally, you should know that when you first boot up your Raspberry Pi, the internet will be managed by Netplan (more on that later in this section). So, if you try to use wpa_cli save_config after creating the config files, that will return FAIL. Instead, once you write the files in the proper directories, run the reconfigure command.\n# Scan for networks\nsudo wpa_cli scan\nsudo wpa_cli scan_results\n\n# Save the current configuration\nsudo wpa_cli save_config\n\n# Reconnect to the network\nsudo wpa_cli reconfigure\nFinally, you can monitor signal quality and connection status by using the signal_poll command. The RSSI (Received Signal Strength Indicator) shows connection quality in dBm. Values closer to 0 indicate stronger signals. Additionally, you can debug connection issues using status.\n&gt; signal_poll\nRSSI=-67\nLINKSPEED=65\nNOISE=9999\nFREQUENCY=5220\n\n&gt; status\nbssid=00:11:22:33:44:55\nfreq=5220\nssid=MyNetwork\nid=0\nmode=station\npairwise_cipher=CCMP\ngroup_cipher=CCMP\nkey_mgmt=WPA2-PSK\nwpa_state=COMPLETED\nip_address=192.168.1.100\nNow that we‚Äôve covered a lot of the great features available with wpa_cli, it‚Äôs time to continue configuring our server. You may remember me mentioning that the Raspberry Pi default networking tool is Netplan. Before we can enable and start the wlan0 service (meaning your primary wifi is on), we need to safely shut down Netplan.\n\n\n\nConverting Netplan to networkd\nIt‚Äôs no surprise that Raspberry Pi uses Netplan as the default network manager because it provides a consistent interface for network configuration; however, there are several reasons you might want to use systemd-networkd directly:\n\nSimplicity: Direct systemd-networkd configuration eliminates a layer of abstraction\nControl: Direct access to all of systemd-networkd‚Äôs features without Netplan‚Äôs limitations\nIntegration: Better alignment with other systemd components\nLearning: Understanding the underlying network configuration system\nPerformance: Potentially faster setup without the translation layer\n\nUbuntu Server uses a layered approach to network configuration:\n\nUser configuration layer: YAML files in /etc/netplan/\nTranslation layer: Netplan reads YAML files and generates configurations for a backend\nBackend layer: Either systemd-networkd or NetworkManager applies the actual configuration\n\nBy removing the middle layer (Netplan), we‚Äôre configuring the backend directly. As you can see from the previous parts of this Networking section in the guide, I like the learning value and longterm potential of systemd, which is why I went with it over Netplan.\n\nStep-by-step Migration\n\nBegin by creating backups of your current network configuration\n\n# Create a backup directory\nsudo mkdir -p /etc/netplan/backups\n\n# Copy all netplan config files\nsudo cp /etc/netplan/*.yaml /etc/netplan/backups/\n\n# Document the current network state\nsudo ip -c addr | sudo tee /etc/netplan/backups/current-ip-addr.txt\nsudo ip -c route | sudo tee /etc/netplan/backups/current-ip-route.txt\n\nReview your existing Netplan so you know what to recreate\n\n# View your current netplan configs\ncat /etc/netplan/*.yaml\n\nNow, create the corresponding systemd-networkd configuration files in /etc/systemd/network/.\n\nFor each interface (wired, wireless, etc.) in your Netplan configuration, create a corresponding .network file, with the appropriate configurations (i.e.¬†static vs.¬†DHCP).\nRemember: For wireless connections, you need both a systemd and a wpasupplicant configuration.\n\n\n# Create the directory if it doesn't exist\nsudo mkdir -p /etc/systemd/network/\n\n# For an Ethernet configuration\nsudo nano /etc/systemd/network/20-wired.network\n\n# For a Wireless configuration\nsudo nano /etc/systemd/network/25-wireless.network\nsudo nano /etc/wpa_supplicant/wpa_supplicant-wlan0.conf\n\nNow that your networkd configuration is in place, disable Netplan.\n\n# Ensure systemd-networkd is enabled\nsudo systemctl enable systemd-networkd\nsudo systemctl enable systemd-resolved\nsudo systemctl start systemd-networkd\n\n# Move the Netplan configurations to a disabled state\nsudo mkdir -p /etc/netplan/disabled\nsudo mv /etc/netplan/*.yaml /etc/netplan/disabled/\n\n# Create a minimal netplan configuration that defers to systemd-networkd\nsudo tee /etc/netplan/01-network-manager-all.yaml &gt; /dev/null &lt;&lt; EOF\nnetwork:\n  version: 2\n  renderer: networkd\nEOF\nLet‚Äôs take a look at the systemctl enable and start commands, because without them we will lose connectivity when turning off netplan. Before we do that, however, what the minimal netplan configuration file does is essentially tell Netplan to just use networkd. We‚Äôll remove it once we are sure everything is up and running.\n\nEnable Command:\n\nsudo systemctl enable systemd-networkd: This configures systemd-networkd to start automatically when the system boots.\nBehind the scenes:\n\nThis command creates the necessary symbolic links in systemd‚Äôs unit directories so that the network daemon will be started by systemd during the boot process.\nIt integrates the service into systemd‚Äôs dependency tree.\nWithout this step, you would need to manually start networkd after each reboot, which is impractical for a server environment.\n\n\nStart Command:\n\nsudo systemctl start systemd-networkd: This launches the systemd-networkd daemon immediately.\nBehind the scenes:\n\nsystemd spawns the networkd process, which then:\n\nReads all .network, .netdev, and .link configuration files in /etc/systemd/network/ and /usr/lib/systemd/network/\nApplies the configurations to matching interfaces\nSets up monitoring for network changes\n\n\n\nRestart Command:\n\nsudo systemctl restart systemd-networkd: This stops and then starts the daemon again, ensuring all configuration changes are applied.\nBehind the scenes:\n\nsystemd sends a termination signal to the running networkd process, waits for it to exit cleanly, and then starts a new instance.\nThe new instance repeats the initialization process, reading all configuration files again.\nThis is the command you‚Äôll use most frequently when making changes to network configurations.\n\n\nWhy Restart Is Necessary:\n\nWhile systemd-networkd does monitor for some changes, editing configuration files doesn‚Äôt automatically trigger a reconfiguration.\nThe restart ensures that:\n\nAll new or modified configuration files are re-read\nAny removed configurations are no longer applied\nAll interface configurations are freshly evaluated against the current state\n\n\nImpact on Network Connectivity:\n\nA restart will temporarily disrupt network connectivity as interfaces are reconfigured\nFor remote servers, use caution when restarting network services to avoid losing your connection\nFor critical remote systems, consider using a command pipeline, like:\n\nsudo systemctl restart systemd-networkd.service || (sleep 30 && sudo systemctl start systemd-networkd.service)\nWhich attempts to restart and then tries to start the service again after 30 seconds if connectivity is lost\n\n\n\n\nApply the systemd-networkd network configuration\n\n# Apply Netplan changes (this will do nothing as we now have a minimal config)\nsudo netplan apply\n\n# Restart systemd-networkd to apply our direct configuration\nsudo systemctl restart systemd-networkd\n\n# The next step is to enable and start the wpa_supplicant service.\n\nsudo systemctl enable wpa_supplicant@wlan0.service\nsudo systemctl start wpa_supplicant@wlan0.service\nThese commands are crucial for integrating wpa_supplicant with systemd, let‚Äôs break them down:\n\nService Template:\n\nThe wpa_supplicant@wlan0.service syntax uses systemd‚Äôs template unit feature.\nThe @ symbol indicates a template service, and wlan0 is the instance name that gets passed to the template.\nThis allows the same service definition to be used for different wireless interfaces.\n\nEnable Command:\n\nsudo systemctl enable wpa_supplicant@wlan0.service: This creates symbolic links from the system‚Äôs service definition directory to systemd‚Äôs active service directory, ensuring the service starts automatically at boot.\nBehind the scenes:\n\nThis modifies systemd‚Äôs startup configuration by adding the service to the correct target units. Typically multi-user.target.\nThe symbolic links created point to the wpa_supplicant service template file.\n\n\nStart Command:\n\nsudo systemctl start wpa_supplicant@wlan0.service: This immediately starts the service without waiting for a reboot.\nBehind the scenes:\n\nsystemd executes the wpa_supplicant binary with appropriate arguments\nDerived from the service template and the instance name (wlan0).\nThe command effectively executed is similar to: /usr/sbin/wpa_supplicant -c /etc/wpa_supplicant/wpa_supplicant-wlan0.conf -i wlan0\n\n\nIntegration with systemd-networkd:\n\nWhen wpa_supplicant successfully connects to a wireless network, it brings the interface up\nsystemd-networkd detects this state change through kernel events\nsystemd-networkd then applies the matching network configuration (our earlier 25-wireless.network file)\nIf DHCP is enabled, the DHCP client process begins\n\nBenefits of this systemd configuration:\n\nDependency management (services can start in the correct order)\nAutomatic restart if the service fails\nStandardized logging through journald\nConsistent management interface alongside other system services\nThe template approach allows for modular configuration that can be easily expanded if you add more wireless interfaces to your Raspberry Pi.\n\n\n\nVerify the new configuration by checking the systemctl status and running simple network check commands\n\n# Check systemd-networkd status\nsystemctl status systemd-networkd\n\n# Check interface status\nip addr show\n\n# Test connectivity\nping -c 4 google.com\nYou should see outputs that look like this: Note, I didn‚Äôt show the output of id addr because I don‚Äôt want to accidentally post my actual IP address online.\n\n\n\nMake the change permanent\n\n# Remove the minimal Netplan configuration\nsudo rm /etc/netplan/01-network-manager-all.yaml\n\n# Mask the Netplan service to prevent it from running\nsudo systemctl mask netplan-wpa@.service\nsudo systemctl mask netplan-ovs-cleanup.service\nsudo systemctl mask netplan-wpa-wlan0.service\nOne final note before moving on, by the time I removed the generic netplan configuration, my system did not have netplan-wpa.service or netplan-wpa-wlan0.service. I forgot to look before I tested the previous steps, so I‚Äôm not sure if I did, but I‚Äôll leave them here just in case someone needs them. That being said, I was able to mask netplan-ovs-cleanup.service successfully.\n\n\nTroubleshooting\nOnce you‚Äôve finished making changes and applying them, verify that everything is up, running, and as you expect.\n# Check systemd-networkd status\nsystemctl status systemd-networkd\n\n# View network status\nnetworkctl status\n\n# List all network links\nnetworkctl list\nThese are crucial commands for troubleshooting and confirming your network configuration, let‚Äôs break them down:\n\nsystemd-networkd Status Check:\n\nsystemctl status systemd-networkd: This displays the current status of the systemd-networkd service. The output includes:\n\nWhether the service is active, inactive, or failed\nWhen it was started and how long it‚Äôs been running\nThe process ID and memory usage\nRecent log entries directly related to the service\n\nBehind the scenes:\n\nThis queries systemd‚Äôs internal service management database and pulls relevant information from the journal logging system.\nUseful pattern: Look for ‚ÄúActive: active (running)‚Äù to confirm the service is working properly and check the logs for any warning or error messages.\n\n\nNetwork Status Overview:\n\nnetworkctl status: This command provides a comprehensive overview of your system‚Äôs network state.\n\nThe output includes:\n\nHostname and domain information\nGateway and DNS server configurations\nCurrent network interfaces and their states\nNetwork addresses (IPv4 and IPv6)\n\n\nBehind the scenes:\n\nThis tool directly communicates with systemd-networkd using its D-Bus API to retrieve the current network state.\nThis command is particularly useful because it aggregates information that would otherwise require multiple different commands to collect.\n\n\nNetwork Links Enumeration:\n\nnetworkctl list: This lists all network interfaces known to systemd-networkd.\n\nThe output shows:\n\nInterface index numbers\nInterface names\nInterface types (ether, wlan, loopback, etc.)\nOperational state (up, down, dormant, etc.)\nSetup state (configured, configuring, unmanaged)\n\n\nBehind the scenes:\n\nLike the status command, this uses systemd-networkd‚Äôs D-Bus API to enumerate all network links and their current states.\nThis provides a quick way to verify which interfaces systemd-networkd is managing and their current status.\n\n\nTroubleshooting with These Commands:\n\nStart with systemctl status systemd-networkd to ensure the service is running\nUse networkctl list to see which interfaces are detected and their states\nIf an interface shows ‚Äúconfiguring‚Äù instead of ‚Äúconfigured,‚Äù check for configuration errors\nUse networkctl status to verify DNS settings and addressing\nFor more detailed logs: journalctl -u systemd-networkd shows all logs from the networkd service\n\n\nThese commands represent the primary diagnostic tools when working with systemd-networkd. They provide a layered approach to troubleshooting - from service-level status to detailed interface information - that helps pinpoint issues in your network configuration. If you need more:\n\nNetwork Connectivity Loss:\n\nConnect directly to the device via console or keyboard/monitor\nCheck logs with journalctl -u systemd-networkd\nRestore the Netplan configuration from your backup if needed\n\nDNS Resolution Issues:\n\nEnsure systemd-resolved is running: systemctl status systemd-resolved\nCheck /etc/resolv.conf is a symlink to /run/systemd/resolve/stub-resolv.conf\nIf not, create it: sudo ln -sf /run/systemd/resolve/stub-resolv.conf /etc/resolv.conf\n\nConfiguration Errors:\n\nVerify syntax with networkctl list to see if interfaces are ‚Äúconfigured‚Äù or ‚Äúconfiguring‚Äù\nCheck for errors with journalctl -u systemd-networkd -n 50\n\n\n\n\n\nAdvanced Networking\n\nSubnets\nA subnet is a logical subdivision of an IP network. Subnetting allows network administrators to partition a large network into smaller, more manageable segments. Subnetting serves several important functions:\n\nAddress Conservation: More efficient allocation of limited IPv4 address space\nSecurity Segmentation: Isolating sensitive systems from general network traffic\nBroadcast Domain Control: Reducing broadcast traffic by limiting its scope\nHierarchical Addressing: Simplifying routing tables and network management\nTraffic Optimization: Improving network performance by segregating traffic types\n\nA subnet mask determines which portion of an IP address refers to the network and which portion refers to hosts within that network. Consider an IPv4 address: 192.168.1.10 with subnet mask 255.255.255.0 (/24)\n\nIn binary:\n\nIP: 11000000.10101000.00000001.00001010\nMask: 11111111.11111111.11111111.00000000\nThe 1s in the mask represent the network portion, while the 0s represent the host portion.\n\nIn CIDR Notation:\n\n/24 means the first 24 bits identify the network (equivalent to 255.255.255.0)\n/16 means the first 16 bits identify the network (equivalent to 255.255.0.0)\n\nSubnet Calculations: For a /24 network\n\nNetwork address: First address in range (e.g., 192.168.1.0)\nBroadcast address: Last address in range (e.g., 192.168.1.255)\nAvailable host addresses: 2^(32-prefix) - 2 = 2^8 - 2 = 254 usable addresses\n\n\nCreating subnets involves both network design and interface configuration. Here‚Äôs how to implement subnetting on a Linux server using systemd-networkd:\n\nScenario 1: Simple Subnet Isolation\n\nThis configuration matches the eth1 interface\nAssigns it the IP 10.0.1.1 within a /24 subnet (255.255.255.0)\nEnables IP forwarding to allow traffic between this subnet and others\nWhen applied, this creates a subnet with 254 usable addresses (10.0.1.1 through 10.0.1.254, excluding the network address 10.0.1.0 and broadcast address 10.0.1.255).\n\n\n# /etc/systemd/network/25-subnet.network\n[Match]\nName=eth1\n\n[Network]\nAddress=10.0.1.1/24\nIPForward=yes\n\nScenario 2: Multiple Subnets on a Single Interface\n\nThis configuration creates three separate subnets accessed through the same physical interface\n\nA /24 subnet (256 addresses) in the 192.168.1.x range\nA /24 subnet (256 addresses) in the 10.10.10.x range\nA /16 subnet (65,536 addresses) in the 172.16.x.x range\n\n\n\nThe system serves as a router/gateway for all three networks simultaneously.\n# /etc/systemd/network/30-multi-subnet.network\n[Match]\nName=eth0\n\n[Network]\nAddress=192.168.1.10/24\nAddress=10.10.10.1/24\nAddress=172.16.1.1/16\n\nDHCP Server Configuration for Subnets\n\nThis configuration, creates a subnet (10.0.1.0/24) on eth1\nEnables a DHCP server\nAllocates IPs from 10.0.1.11 (base + offset of 10) through 10.0.1.210 (for 200 addresses)\nProvides DNS server information to DHCP clients\n\n\n# /etc/systemd/network/25-dhcp-server.network\n[Match]\nName=eth1\n\n[Network]\nAddress=10.0.1.1/24\nIPForward=yes\nDHCPServer=yes\n\n[DHCPServer]\nPoolOffset=10\nPoolSize=200\nEmitDNS=yes\nDNS=8.8.8.8\nAlthough more complicated than simple networking, subnetting can enhance security when configured properly. It improves isolation by putting separate sensitive services onto different subnets, segmentation by limiting broadcast domains to reduce the potential attack surface, and access control by implementing filters between subnets at the router level. You can see an example of a security-enhanced subnet configuration below, as well as a list of commands to troubleshoot your subnet with.\n# /etc/systemd/network/25-secure-subnet.network\n[Match]\nName=eth2\n\n[Network]\nAddress=10.0.3.1/24\nIPForward=yes\nIPMasquerade=yes  # NAT for outgoing connections\nConfigureWithoutCarrier=yes\n\n[DHCPServer]\nPoolOffset=50\nPoolSize=100\nEmitDNS=yes\nDNS=1.1.1.1\n\n# Restrict routes between subnets for this segment\n[Route]\nGateway=_ipv4gateway\nDestination=0.0.0.0/0\n# Check interface configuration\nip addr show\n\n# Verify routing tables\nip route show\nip route show table 200  # For custom route tables\n\n# Test connectivity between subnets\nping 10.0.1.1  # From another subnet\n\n# View ARP table to verify proxy ARP functionality\nip neigh show\n\n# Check systemd-networkd logs for issues\njournalctl -u systemd-networkd -n 50"
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-ssh",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-ssh",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "SSH",
    "text": "SSH\nNow that you have your basic Ubuntu Pi server configured and connected to a network, it‚Äôs time to do some final configurations before beginning the more coding focused development. For the coding side of things, we‚Äôll want to remotely connect using a different computer and connection than the wired keyboard/monitor with our Raspberry Pi. To do this, we‚Äôll utilize VS Code, an open source IDE (integrated development environment) from Microsoft. Before that, we‚Äôll need to setup and configure SSH (Secure Shell), one of the most common ways to connect to a remote server. Simply put, SSH is a network protocol that creates an encrypted tunnel between computers, allowing secure remote management. Think of it as establishing a private, secure telephone line that only authorized parties can use to communicate.\nOnce we have SSH setup, configured, and secured, we‚Äôll use a feature in VS Code called Remote - SSH which lets us use the nice UI of an IDE while working on the actual server. This is really beneficial for a variety of reasons: one of them being the fantastic community-built extensions that drastically improve the development experience, another being the integration with other tools for things like CI/CD.\n\nKey Terms\nSSH Basic Concepts:\n\nSSH (Secure Shell): A cryptographic network protocol for secure communication between computers over an unsecured network.\nSSH Server: The computer or service that accepts incoming SSH connections.\nSSH Client: The application used to initiate connections to SSH servers.\nsshd: The SSH server daemon that listens for and handles SSH connections.\nssh_config: The client-side configuration file that controls outgoing SSH connections.\nsshd_config: The server-side configuration file that controls incoming SSH connections.\nHost Key: A cryptographic key that identifies an SSH server.\n\nAuthentication Methods:\n\nPassword Authentication: Authentication using a traditional username and password.\nPublic Key Authentication: Authentication using asymmetric cryptographic key pairs.\nPrivate Key: The secret half of a key pair that should never be shared.\nPublic Key: The shareable half of a key pair that can be distributed to servers.\nPassphrase: An optional password that encrypts and protects a private SSH key.\nauthorized_keys: A file containing public keys that are allowed to authenticate to an SSH server.\nknown_hosts: A file on the client side that stores server host keys to verify server identity.\n\nSSH Key Types and Security:\n\nRSA (Rivest-Shamir-Adleman): A widely used public-key cryptosystem for secure data transmission.\nECDSA (Elliptic Curve Digital Signature Algorithm): A cryptographic algorithm offering good security with shorter key lengths.\nEd25519: A modern, secure, and efficient public-key signature system.\nKey Length: The size of a cryptographic key, typically measured in bits.\nFingerprint: A short sequence used to identify a longer public key.\nSSH Agent: A program that holds private keys in memory to avoid repeatedly typing passphrases.\n\nSSH Security and Tools:\n\nPort Forwarding: The ability to tunnel network connections through an SSH connection.\nSSH Tunnel: An encrypted network connection established through SSH.\nSCP (Secure Copy Protocol): A means of securely transferring files between hosts based on SSH.\nSFTP (SSH File Transfer Protocol): A secure file transfer protocol that operates over SSH.\nUFW (Uncomplicated Firewall): A simplified firewall management interface for iptables.\nFail2Ban: An intrusion prevention software that protects servers from brute-force attacks.\n\n\n\nSSH Basics\n\nSSH Client vs Server Configuration\nThe SSH system uses two main configuration files with distinct purposes:\n\nssh_config:\n\nLives on your client machine (like your laptop)\nControls how your system behaves when connecting to other SSH servers\nAffects outgoing SSH connections\nLocated at /etc/ssh/ssh_config (system-wide) and ~/.ssh/config (user-specific)\n\nIf your server ever moves or connects to a new IP address, simply update it in the user config file\n\n\nsshd_config:\n\nLives on your server (the Raspberry Pi)\nControls how your SSH server accepts incoming connections\nDetermines who can connect and how\nLocated at /etc/ssh/sshd_config\nRequires root privileges to modify\nChanges require restarting the SSH service\n\n\n\n\n\nKey-Based Authentication Setup\n\nUnderstanding SSH Keys and Security\nThis guide uses ECDSA-384 keys, which offer several advantages:\n\nUses the NIST P-384 curve, providing security equivalent to 192-bit symmetric encryption\nBetter resistance to potential quantum computing attacks compared to smaller key sizes\nStandardized under FIPS 186-4\nExcellent balance between security and performance\n\n\n\nGenerating Your SSH Keys\nYou might remember from the beginning of this guide that you can generate an SSH key-pair when flashing the image using RPi Imager. Even if you did that, PasswordAuthentication will still be enabled in the server‚Äôs /etc/ssh/sshd_config. That being said, if you didn‚Äôt do that and want to learn how you can handle this all, the old fashioned way, then on your laptop, generate a new SSH key pair:\n# Generate a new SSH key pair using ECDSA-384\nssh-keygen -t ecdsa -b 384 -C \"ubuntu-pi-server\"\nThis command:\n\n-t ecdsa: Specifies the ECDSA algorithm\n-b 384: Sets the key size to 384 bits\n-C \"ubuntu-pi-server\": Adds a descriptive comment\n\nThe command generates two files:\n\n~/.ssh/ubuntu_pi_ecdsa: Your private key (keep this secret!)\n~/.ssh/ubuntu_pi_ecdsa.pub: Your public key (safe to share)\n\n\n\nInstalling Your Public Key on the Raspberry Pi\nTransferring your public key to your Raspberry Pi is easy, just know the following will only work if you currently have password authentication enabled.\nssh-copy-id -i ~/.ssh/ubuntu_pi_ecdsa.pub chris@ubuntu-pi-server\nThis command:\n\nConnects to your Pi using password authentication\n\nIf you‚Äôre restoring your config, you‚Äôll need to temporarily set PasswordAuthentication in /etc/ssh/sshd_config to yes\n\nCreates the .ssh directory if needed\nAdds your public key to authorized_keys\nSets appropriate permissions automatically\n\n\n\n\nServer-Side SSH Configuration\nA client-server relationship is a fundamental computing model that underpins most network communications and distributed systems. This architecture divides computing responsibilities between service requestors (clients, your laptop in this case) and service providers (servers, the Raspberry Pi in this case).\nA server is a computer program or device that provides functionality, resources, or services to multiple clients. The Raspberry Pi in this case.\n\nService Provider: Responds to client requests rather than initiating communication\nResource Management: Manages shared resources (files, databases, computational power)\nContinuous Operation: Typically runs continuously, waiting for client requests\nScalability: Often designed to handle multiple concurrent client connections\nExamples: Web servers, database servers, file servers, mail servers, authentication servers\n\nClient-server communication follows a request-response pattern:\n\nConnection: The client establishes a connection to the server\nRequest: The client sends a formatted request for a specific service\nProcessing: The server processes the request according to its business logic\nResponse: The server returns appropriate data or status information\nDisconnection or Persistence: The connection may be terminated or maintained for future requests\n\nThis communication typically occurs over TCP/IP networks using standardized protocols that define the format and sequence of messages exchanged.\n\nUnderstanding Server Host Keys\nYour Pi‚Äôs /etc/ssh and /home/chris/.ssh directories contains several important files:\n\nAuthorized keys (in /home/chris/.ssh/authorized_keys)\nHost key pairs (public and private) for different algorithms (in /etc/ssh)\nConfiguration files and directories\nThe moduli file for key exchange\n\n\n\n\nClient-Side Configuration\nA client is a computer program or device that requests services, resources, or information from a server.\n\nRequest Initiator: Clients always initiate communication with servers\nUser Interface: Often provides the interface through which users interact with remote services\nLimited Resources: Typically has fewer resources than servers and offloads intensive processing\nDependency: Relies on servers to fulfill requests and cannot function independently for networked operations\nExamples: Web browsers, email clients, SSH clients, mobile applications\n\nFrom a technical perspective, clients:\n\nFormulate and send requests using specific protocols (HTTP, FTP, SMTP, etc.)\nWait for and process server responses\nPresent results to users or use them for further operations\n\n\nSetting Up Your SSH Config\nCreate or edit ~/.ssh/config on your laptop:\nHost ubuntu-pi-server\n    HostName ubuntu-pi-server\n    User chris\n    IdentityFile ~/.ssh/ubuntu_pi_ecdsa\n    Port 45000\n\n\n\n\n\n\nSSH Config: Include\n\n\n\nIf your ssh isn‚Äôt picking up on the ~/.ssh/ssh_config then you might need to specify it in the system config. Find the line in /etc/ssh/ssh_config that says Include and add the absolute file path. If you need to include more than your user specific config, such as the default /etc/ssh/ssh_config.d/* just add that absolute path separated by a space from any other path included.\n\n\n\n\nManaging Known Hosts\n\nBack up your current known_hosts file:\n\ncp ~/.ssh/known_hosts ~/.ssh/known_hosts.backup\n\nView current entries:\n\nssh-keygen -l -f ~/.ssh/known_hosts\n\nRemove old entries:\n\n# Remove specific host\nssh-keygen -R ubuntu-pi-server\n\nHash your known_hosts file for security:\n\nssh-keygen -H -f ~/.ssh/known_hosts\n\n\nSecuring the Key File\nWhen using SSH key-based authentication, adding a password to your key enhances security by requiring a passphrase to use the key. This guide explains how to add and remove a password from an existing SSH key.\nAdding a Password to an SSH Key\nIf you already have an SSH key and want to add a password to it, use the following command:\nssh-keygen -p -f ~/.ssh/id_rsa\nExplanation:\n\n-p: Prompts for changing the passphrase.\n-f ~/.ssh/id_rsa: Specifies the key file to modify (adjust if your key has a different name). You will be asked for the current passphrase (leave blank if none) and then set a new passphrase.\n\nRemoving a Password from an SSH Key\nIf you want to remove the passphrase from an SSH key, run:\nssh-keygen -p -f ~/.ssh/id_rsa -N \"\"\nExplanation:\n\n-N \"\": Sets an empty passphrase (removes the password).\nThe tool will ask for the current passphrase before removing it.\n\nVerifying the Changes\nAfter modifying the key, test the SSH connection from your CLI, or using an SSH tunnel.\nssh -i ~/.ssh/id_rsa user@your-server\nIf you added a passphrase, you‚Äôll be prompted to enter it when connecting.\nBy using a passphrase, your SSH key is protected against unauthorized use in case it gets compromised. If you frequently use your SSH key, consider using an SSH agent (ssh-agent) to cache your passphrase securely.\n\n\n\nImplementing SSH Security Measures\nSecuring SSH is critical because it serves as the primary gateway for remote server management, making it a prime target for attackers. A compromised SSH connection can lead to unauthorized access, data breaches, privilege escalation, and complete system takeover. By implementing robust SSH security measures like key-based authentication, non-standard ports, and intrusion prevention systems, you significantly reduce your attack surface while maintaining convenient remote access. Proper SSH hardening also helps meet compliance requirements for many industries while providing detailed audit logs for security monitoring. For a 24/7 self-hosted server exposed to the internet, optimized SSH security isn‚Äôt optional‚Äîit‚Äôs essential for protecting your system and the data it contains.\n\nBack up the original configuration:\n\nsudo cp /etc/ssh/sshd_config /etc/ssh/sshd_config.backup-$(date +%Y%m%d)\n\nOptimize host key settings in sshd_config:\n\n# Specify host key order (prioritize ECDSA)\nHostKey /etc/ssh/ssh_host_ecdsa_key\nHostKey /etc/ssh/ssh_host_ed25519_key\nHostKey /etc/ssh/ssh_host_rsa_key\n\nStrengthen the moduli file:\n\n# Back up the existing file\nsudo cp /etc/ssh/moduli /etc/ssh/moduli.backup\n\n# Remove moduli less than 3072 bits\nsudo awk '$5 &gt;= 3072' /etc/ssh/moduli &gt; /tmp/moduli\nsudo mv /tmp/moduli /etc/ssh/moduli\n\nApply changes:\n\n# Test the configuration\nsudo sshd -t\n\n# Restart the SSH service (on Ubuntu Server)\nsudo systemctl restart ssh\n\n# Verify the service status\nsudo systemctl status ssh\nJust note, you‚Äôll probably need to reboot (sudo reboot) your server before all of the changes fully take place. Once you‚Äôve done that, you may need to run sudo systemctl start ssh.\n\nFirewall Configuration with ufw\nA firewall acts as a barrier between your server and potentially hostile networks by controlling incoming and outgoing traffic based on predetermined rules. UFW provides a user-friendly interface to the underlying iptables firewall system in Linux.\n\nDefault Deny Policy: Start with blocking all connections and only allow specific permitted traffic\nStateful Inspection: Track the state of active connections rather than just examining individual packets\nPort Control: Allow or block access based on specific network ports\nSource Filtering: Control traffic based on originating IP addresses or networks\n\n# Install UFW (if it isn't already)\nsudo apt install ufw\n\n# Allow SSH connections\nsudo ufw allow ssh\n\n# Enable the firewall\nsudo ufw enable\nBehind the scenes, UFW translates these simple commands into complex iptables rules, making firewall management accessible without sacrificing security. The underlying iptables system uses a chain-based architecture to process packets through INPUT, OUTPUT, and FORWARD chains. Now, you‚Äôll want to add rules for example, allowing traffic on a specific port if you took the step to choose a nonstandard, one that isn‚Äôt the default Port 22, for this guide, I‚Äôm choosing 45000.\n# Add a new rule in the port/protocol format\nsudo ufw add 45000/tcp\n\n# Allow traffic between subnets 10.0.1.0/24 and 10.0.2.0/24\nsudo ufw allow from 10.0.1.0/24 to 10.0.2.0/24\n\n# See a list of all rules\nsudo ufw status numbered\n\n# Remove the default rules\nsudo ufw delete 1\n\n\nFail2Ban\nFail2Ban is a security tool designed to protect servers from brute force attacks. It works by monitoring log files for specified patterns, identifying suspicious activity (like multiple failed login attempts), and banning the offending IP addresses using firewall rules for a set period. It‚Äôs especially useful for securing SSH, FTP, and web services.\nThe best part is the project is entirely open source, you can view the source code and contribute here.\n# Install Fail2Ban\nsudo apt update\nsudo apt install fail2ban\n\n# Start and enable Fail2Ban\nsudo systemctl start fail2ban\nsudo systemctl enable fail2ban\n\n# Check the status of all jails\nsudo fail2ban-client status\n\n# Check the status of a specific jail\nsudo fail2ban-client status sshd\n\n# View banned IPs\nsudo iptables -L -n | grep f2b\nI want to add that fail2ban automatically pulls values for its jails depending on how you‚Äôve configured things on your system, at least I assume so. I‚Äôm assuming that because I never configured specific ssh rules for fail2ban, but it knows to allow the port I set in my sshd_config. That being said, you can see how simple it was to setup these tools, and how they work together to create a comprehensive security system:\n\nUFW establishes the baseline by controlling which ports are accessible\nFail2Ban adds behavioral analysis by monitoring authentication attempts\nTogether they provide both static and dynamic protection\n\nThis layered approach follows the defense-in-depth principle essential to modern cybersecurity. By combining a properly configured firewall with an intrusion prevention system, you significantly reduce the attack surface of your Ubuntu Pi Server.\n\n\nRegular Security Checks\n\nMonitor SSH login attempts:\n\nsudo journalctl -u ssh\n\nCheck authentication logs:\n\nsudo tail -f /var/log/auth.log\n\n\n\nSCP (Secure Copy Protocol) and rsync (Remote Sync)\nThis section outlines the process of securely copying files between your Ubuntu Pi Server and your Client machine. I‚Äôll cover two powerful methods: SCP (Secure Copy Protocol) and rsync. Both tools operate over SSH, ensuring your file transfers remain encrypted and secure.\nSCP is a simple file transfer utility built on SSH that allows you to copy files between computers. It‚Äôs straightforward for basic transfers but lacks advanced features for large or frequent transfers.\nrsync is a more sophisticated file synchronization and transfer utility that offers several advantages over SCP:\n\nIncremental transfers: Only sends parts of files that changed\nResume capability: Can continue interrupted transfers\nBandwidth control: Can limit how much network it uses\nPreservation options: Maintains file timestamps, permissions, etc.\nDirectory synchronization: Can mirror directory structures\nExclusion patterns: Can skip specified files/directories\n\n\nEnsuring Your SSH Configuration Works\nBefore attempting file transfers, verify your SSH connection is properly configured:\nssh -F ~/.ssh/config chris@ubuntu-pi-server\nThis command explicitly specifies the user configuration file location with the -F flag.\nNote: To ensure SSH always uses your user-specific config:\n\nSet proper permissions on your config file:\n\nchmod 600 ~/.ssh/config\n\nUpdate the system-wide SSH config to include your user config:\n\nsudo nano /etc/ssh/ssh_config\nAdd this line:\nInclude ~/.ssh/config\nAfter applying these changes, you should be able to connect using the simplified command:\nssh ubuntu-pi-server\n\n\nCopying Individual Files from Server to Client\nThe basic syntax for copying files from your server to your local machine is shown below. Know that in all code examples in this section, you should run it in a terminal on your client/local machine:\nscp ubuntu-pi-server:~/configs/wpa_supplicant-wlan0.conf ~/Documents/raspberry_pi_server/configs\nscp ubuntu-pi-server:~/configs/25-wireless.network ~/Documents/raspberry_pi_server/configs\nEach command performs the following actions:\n\nscp: Invokes the secure copy program\nubuntu-pi-server:~/configs/wpa_supplicant-wlan0.conf: Specifies the source file on the remote server\n~/Documents/raspberry_pi_server/25-wireless.network: Specifies the destination directory on your local machine\n\n\n\nCopying Multiple Files at Once\nTo copy all Bash scripts from a directory in one command:\nscp chris@ubuntu-pi-server:~/configs/*.sh ~/Documents/raspberry_pi_server/configs\nThe wildcard pattern *.sh tells SCP to match all files with the .sh extension. Here, I‚Äôve included the username chris@ explicitly, which can help resolve connection issues if your SSH config isn‚Äôt being properly recognized.\n\n\nRecursively Copying Directories\nTo copy entire directories with their contents:\nscp -r chris@ubuntu-pi-server:~/mnt/backups/ ~/Documents/raspberry_pi_server/backups/configs\nThe -r flag (recursive) tells SCP to copy directories and their contents.\n\n\nCopying Files from Client to Server\nTo send files in the opposite direction (local to remote):\nscp -r ~/Documents/pi-scripts chris@ubuntu-pi-server:~/scripts\n\n\nTransferring Files with rsync\nFor larger files or when you need to synchronize directories, rsync offers significant advantages over SCP.\nTo copy a single file from server to client:\nrsync -avz ubuntu-pi-server:~/configs/25-wireless.network ~/Documents/raspberry_pi_server/configs\nLet‚Äôs break down these common flags:\n\n-a: Archive mode, preserves permissions, timestamps, etc. (shorthand for -rlptgoD)\n-v: Verbose, shows detailed progress\n-z: Compresses data during transfer, saving bandwidth\n\n\n\nSyncing Directories with rsync\nTo sync an entire directory from server to client:\nrsync -avz --progress ubuntu-pi-server:~/configs/ ~/Documents/raspberry_pi_server/configs\nThe --progress flag shows a progress bar for each file transfer, which is particularly useful for large files.\nImportant Note: The trailing slash on the source path (~/configs/) is significant\n\nWith trailing slash: Copies the contents of the directory\nWithout trailing slash: Copies the directory itself and its contents\n\n\n\nSyncing in Reverse (Client to Server)\nTo sync files from your client to the server:\nrsync -avz --progress ~/Documents/raspberry_pi_server/configs chris@ubuntu-pi-server:~/configs/\n\n\nUsing rsync with Dry Run\nBefore performing large transfers, you can see what would happen without actually making changes:\nrsync -avzn --progress ~/Documents/raspberry_pi_server/configs chris@ubuntu-pi-server:~/configs/\nThe -n flag (or --dry-run) simulates the transfer without changing any files, letting you verify what would happen.\n\n\nIncremental Backups with rsync\nrsync excels at keeping directories in sync over time. After the initial transfer, subsequent runs only transfer what‚Äôs changed:\nrsync -avz --delete ~/Documents/raspberry_pi_server/configs chris@ubuntu-pi-server:~/configs/\nThe --delete flag removes files from the destination that no longer exist in the source, creating a perfect mirror. Use with caution!\n\n\nAdvanced rsync Examples\n\n\nCustom SSH Parameters\nTo specify specific ssh paramters, such as key file or port:\nrsync -avz --progress -e 'ssh -p 45000 -i ~/.ssh/ubuntu_pi_ecdsa'  chris@192.168.1.151:/mnt/backups/configs/master backups/configs/\nThe e flag tells rsync to execute ssh with those specific flags, when it initiates the connection.\n\n\nExcluding Files or Directories\nTo skip certain files or directories during transfer:\nrsync -avz --exclude=\"*.tmp\" --exclude=\"node_modules\" ~/Documents/raspberry_pi_server/configs chris@ubuntu-pi-server:~/configs/\nThis command excludes all .tmp files and the node_modules directory.\n\n\nSetting Bandwidth Limits\nIf you need to limit how much network bandwidth rsync uses:\nrsync -avz --bwlimit=1000 ~/Documents/raspberry_pi_server/configs chris@ubuntu-pi-server:~/configs/\nThe --bwlimit=1000 restricts transfer speed to 1000 KB/s (approximately 1 MB/s).\n\n\nPreserving Hard Links\nWhen backing up systems that use hard links (like Time Machine or some backup solutions):\nrsync -avH ~/Documents/raspberry_pi_server/configs chris@ubuntu-pi-server:~/configs/\nThe -H flag preserves hard links, which can save significant space in backups.\n\n\nChoosing Between SCP and rsync\nUse SCP when: - You need a quick, one-time file transfer - You want a simple command with minimal options - The files are small and not changing frequently\nUse rsync when: - You need to synchronize directories - You‚Äôre transferring large files that might get interrupted - You want to maintain exact mirrors of directory structures - You‚Äôre setting up automated backups - You need to preserve file attributes like permissions and timestamps - You need to exclude certain files or patterns\n\nSSH Configuration: Ensure your SSH config is properly set up before attempting file transfers\nSCP: Simple, straightforward file copying between systems\nrsync: More powerful synchronization tool with many options for efficiency\n\nSSH is now correctly configured and working using ssh ubuntu-pi-server.\nBash scripts can be securely copied from the Ubuntu Pi Server to the client machine using scp.\n\nJust take note of the specific syntax used, namely server-name:path/to/files\n\nThe user can now maintain local backups of important scripts efficiently.\n\nEnables you to develop where you‚Äôd like and then easily move files to test scripts\n\n\nTrailing Slashes: Pay attention to trailing slashes in paths, as they change behavior\nDry Run: Use --dry-run with rsync to preview what will happen\nAutomation: Consider creating scripts for routine backup tasks\n\nBoth SCP and rsync are invaluable tools for managing files on your Raspberry Pi server. While SCP is perfect for quick, simple transfers, rsync provides the power and flexibility needed for maintaining backups and keeping systems synchronized."
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-partitions",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-partitions",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Partitions",
    "text": "Partitions\n\nKey Terms\nPartition Basics:\n\nPartition: A logical division of a physical storage device.\nDisk: A physical storage device (HDD, SSD, etc.).\nPartition Table: A data structure on a disk that describes how the disk is divided.\nMBR (Master Boot Record): A traditional partition scheme limited to 2TB drives and 4 primary partitions.\nGPT (GUID Partition Table): A modern partition scheme supporting larger drives and more partitions.\nPrimary Partition: A partition that can be bootable and hold an operating system.\nExtended Partition: A special type of partition that acts as a container for logical partitions (MBR only).\nLogical Partition: A partition created within an extended partition (MBR only).\nBoot Partition: A partition containing files needed to start the operating system.\nRoot Partition: The primary partition containing the operating system and most files.\n\nFilesystem Types:\n\nFilesystem: The method used to organize and store data within a partition.\next4: The fourth extended filesystem, a journaling filesystem commonly used in Linux.\nFAT32: File Allocation Table 32-bit, a simple filesystem compatible with most operating systems.\nexFAT: Extended File Allocation Table, designed for flash drives with support for larger files than FAT32.\nNTFS: New Technology File System, primarily used by Windows.\nBtrfs: B-tree File System, a modern Linux filesystem with advanced features like snapshots.\nJournaling: A technique that maintains a record of filesystem changes before committing them.\nMounting: The process of making a filesystem accessible through the file hierarchy.\nMount Point: A directory where a filesystem is attached to the system‚Äôs file hierarchy.\n\nPartitioning Tools:\n\nfdisk: A traditional command-line utility for disk partitioning.\nparted: A more powerful partitioning tool with support for larger drives and GPT.\ngdisk: A GPT-focused partitioning utility.\nsfdisk: A scriptable version of fdisk for automation.\ngparted: A graphical partition editor for Linux.\nmkfs: Command used to create a filesystem on a partition.\nfsck: Filesystem consistency check and repair tool.\nblkid: Command that displays attributes of block devices like UUID.\nlsblk: Command that lists information about block devices.\nfstab: System configuration file that defines how filesystems are mounted.\n\n\n\nPartitioning Basics\nPartitions are logical divisions of a physical storage device. Think of a storage device like a large piece of land, and partitions as fenced areas within that land dedicated to different purposes. Each partition appears to the operating system as a separate disk, even though physically they‚Äôre on the same device. Remember from the beginning of this guide, I‚Äôm currently using a Flash Drive for my primary memory and a microSD card for backups; however, the SSD is what I want to serve as the boot device. Once we complete the partitioning, we can flash the base image from RPi onto the SSD and then reboot, with the SSD as the boot device.\n\nSeparation of concerns: Isolate the operating system from user data, which improves security and simplifies backups\nPerformance optimization: Different filesystems can be used for different workloads\nMulti-boot capability: Install multiple operating systems on the same physical device\nData protection: Limiting the scope of filesystem corruption to a single partition\nResource management: Setting size limits for specific system functions\n\nFor our Raspberry Pi server, proper partitioning creates a solid foundation for everything else you‚Äôll build. We‚Äôll primarily use ext4 for Linux partitions and FAT32 for the microSD card that needs broader compatibility.\n\n\n\n\n\n\n\n\nFilesystem\nBest For\nFeatures\n\n\n\n\next4\nLinux\n\nJournaling\nLarge file support\nBackwards compatible\n\n\n\nFAT32\nCross-platform compatibility\n\nWorks with virtually all operating systems\nLimited to 4GB Files\n\n\n\nexFAT\nModern cross-platform\n\nSupports large files\nNo built-in journaling\n\n\n\nNTFS\nWindows compatibility\n\nJournaling\nPermissions\nCompression\n\n\n\nBtrfs\nAdvanced Linux systems\n\nSnapshots\nChecksums\nCompression\n\n\n\n\nFinally, let‚Äôs cover some important terms:\n\nPartition Table: A data structure on a disk that describes how the disk is divided\n\nMBR (Master Boot Record): Traditional partition scheme limited to 2TB drives and 4 primary partitions\nGPT (GUID Partition Table): Modern scheme supporting larger drives and more partitions\n\nPartition Types:\n\nPrimary: Can be bootable and hold an operating system\nExtended: Acts as a container for logical partitions (MBR only)\nLogical: Created within an extended partition (MBR only)\n\nFilesystem: The method used to organize and store data within a partition\n\nCommon Linux filesystems: ext4, Btrfs\nCross-platform filesystems: FAT32, exFAT\n\n\n\n\nPartitioning Tools\nSeveral command-line tools are available for disk partitioning on Linux. Each has strengths for different scenarios:\n\n\n\n\n\n\n\n\n\nTool\nStrengths\nLimitations\nBest For\n\n\nfdisk\n\nSimple interface\nWidely available\n\n\nLimited GPT support in older versions\n\n\nBasic partitioning tasks\n\n\n\nparted\n\nFull GPT support\nHandles large drives\n\n\nMore complex syntax\n\n\nAdvanced partitioning needs\n\n\n\ngdisk\n\nGPT focused\nSimilar to fdisk\n\n\nLess common on minimal installations\n\n\nGPT-specific operations\n\n\n\nsfdisk\n\nScriptable for automation\n\n\nLess user-friendly\n\n\nAutomated deployments\n\n\n\n\nFor this project, and after doing some research, I chose parted for both the microSD card and SSD partitioning because:\n\nIt fully supports both MBR and GPT partition tables\nIt can handle drives larger than 2TB (relevant for the SSD)\nIt provides a more consistent interface across different partition table types\nIt supports both interactive and command-line usage\nIt‚Äôs included in most Ubuntu installations\n\n\n\nPartitioning a MicroSD Card for Backups\nLet‚Äôs partition our microSD card to serve as backup media. You can get great quality cards from Amazon Basics that are perfect for this use case. We‚Äôll use a simple, effective partition scheme. Before we dive into the actual commands, it‚Äôs important to remember that you can‚Äôt modify the memory of the active primary drive. Meaning, that you‚Äôll need to use an SSD or thumb drive as the boot media while you modify the SD card. Similarly, you‚Äôll need to use a different piece of boot media (you could use the micro SD) when partitioning the SSD.\nNow, let‚Äôs walk through this step-by-step:\n\nIdentify the device name of the microSD. Your microSD card will typically appear as something like /dev/mmcblk0 (what mine showed as) or /dev/sdX (where X is a letter like a, b, c). This command lists block devices with key information:\n\nsudo lsblk -o NAME,SIZE,FSTYPE,TYPE,MOUNTPOINT\nThe lsblk command lists all block devices, which includes your storage devices. The -o flag specifies which columns to display in the output. This final check ensures you‚Äôre working with the correct device and helps you confirm the partition structure you just created.\n\nNAME: Device identifier\nSIZE: Storage capacity\nFSTYPE: Current filesystem type\nTYPE: Whether it‚Äôs a disk or partition\nMOUNTPOINT: Where it‚Äôs currently mounted (if applicable)\n\n\nFor a backup microSD card, we‚Äôll use a simple partition layout with a single partition using ext4 filesystem, which provides good performance and Linux compatibility.\n\n# Start parted on the microSD card (replace /dev/mmcblk0 with your device)\nsudo parted /dev/mmcblk0\n\n# View the partition table for a specific device, or all\nprint mmcblk0\nprint all\n\n# Inside parted, create a new GPT partition table\n&gt; (parted) mklabel gpt\nWarning: The existing disk label on /dev/mmcblk0 will be destroyed and all data on this disk will be lost. Do you want to continue?\nYes/No? Yes\n\n# Create a single partition using the entire card\n&gt; (parted) mkpart primary ext4 0% 100%\n\n# Set a name for easy identification\n&gt; (parted) name 1 backups\n\n# Verify the partition layout\n&gt; (parted) print\n\n# Exit parted\n&gt; (parted) quit\n\nmklabel gpt: Creates a new GPT partition table (preferred over MBR for modern systems)\nmkpart primary ext4 0% 100%: Creates a primary partition using the ext4 filesystem that spans the entire device\nname 1 backup: Names the first partition ‚Äúbackup‚Äù for easy identification\nprint: Shows the current partition layout\nquit: Exits the parted utility\n\n\nAfter creating the partition, we need to format it with the ext4 filesystem. Double check the current layout of memory on your system with sudo lsblk -o NAME,SIZE,FSTYPE,TYPE,MOUNTPOINT before formatting the filesystem, to get the specific SD Card partitions device name:\n\n# Format the partition (adjust if your device/partition is different)\nsudo mkfs.ext4 -L backups /dev/mmcblk0p1\n\n-L backup: Sets the filesystem label to ‚Äúbackup‚Äù\n/dev/mmcblk0p1: The partition we just created (p1 indicates the first partition)\nYou‚Äôll see an output similar to this:\n\n\n\nNow, we need to prepare the SD card for backups. You can make those changes with the following commands:\n\n# Create a mount point\nsudo mkdir -p /mnt/backups\n\n# Add an entry to /etc/fstab for automatic mounting\necho \"UUID=$(sudo blkid -s UUID -o value /dev/mmcblk0p1) /mnt/backups ext4 defaults,noatime 0 2\" | sudo tee -a /etc/fstab\n\n# Restart the systemd daemon to get the changes made to fstab\nsudo systemctl daemon-reload\n\n# Mount the filesystem from fstab\nsudo mount /dev/mmcblk0\n\n# Create backup directories\nsudo mkdir -p /mnt/backups/{configs,logs}\n\n# Set ownership (replace 'chris' with your username)\nsudo chown -R chris:chris /mnt/backups/\n\n# Set secure permissions\nsudo chmod -R 700 /mnt/backups/\n\nmkdir -p: Creates directories and parent directories if they don‚Äôt exist\nblkid -s UUID -o value: Gets the UUID (unique identifier) of the partition\ndefaults,noatime: Mount options for good performance (noatime disables recording access times)\n0 2: The fifth field (0) disables dumping, the sixth field (2) enables filesystem checks\nmount -a: Mounts all filesystems specified in fstab\nchmod -R 700: Sets permissions so only the owner can read/write/execute\n\n\n\nPartitioning your SSD\nFor a Raspberry Pi server, a two-partition scheme offers the perfect balance of simplicity and functionality. This approach mirrors what RPi Imager creates automatically, but gives us control over the sizes:\n\nA small FAT32 boot partition for firmware and boot files\nA large ext4 root partition for the entire operating system and data\n\nThis simplified structure eliminates the complexity of separate swap and data partitions while maintaining full functionality. The Raspberry Pi can use swap files instead of dedicated partitions, which provides more flexibility for managing memory as your needs change.\n\nFor the Samsung T7 SSD, we‚Äôll follow a similar workflow. The Samsung T7 SSD will likely appear as /dev/sdX (where X is a letter like a, b, c), mine is /dev/sdb.\n\nsudo lsblk -o NAME,SIZE,FSTYPE,TYPE,MOUNTPOINT\n\nIn this code block, I‚Äôll show you a way to use parted without entering the interactive mode.\n\n# Create a new GPT partition table\nsudo parted /dev/sdb mklabel gpt\n\n# Create the EFI System Partition (ESP)\nsudo parted /dev/sdb mkpart boot fat32 1MiB 513MiB\nsudo parted /dev/sdb set 1 esp on\nsudo parted /dev/sdb set 1 boot on\n\n# Create the root partition\nsudo parted /dev/sdb mkpart ubuntu-root ext4 513MiB 100%\n\n# Verify the partition layout\nsudo parted /dev/sdb print\n\n/dev/sdc1: 512MB FAT32 partition for boot files.\n/dev/sdc2: Remaining space (about 931GB) ext4 partition for the entire system.\nThe set 1 boot on command marks the partition as bootable.\nThe set 1 esp on marks it as an EFI System Partition, ensuring compatibility with both legacy and UEFI boot methods.\n\n\nNow we need to format each partition.\n\n# Format the ESP partition\nsudo mkfs.fat -F32 -n BOOT /dev/sdb1\n\n# Format the root partition\nsudo mkfs.ext4 -L ubuntu-root /dev/sdb2\n\nmkfs.fat -F32: Creates a FAT32 filesystem\n-n ESP: Sets the volume label to ‚ÄúESP‚Äù\nmkfs.ext4: Creates an ext4 filesystem\n-L ubuntu-root: Sets the filesystem label\n/dev/sdb1, /dev/sdb2, etc.: The specific partitions we created\n\n\nNow, we will verify that the partitions went as we hoped\n\n# Check the partition layout\nsudo lsblk -o NAME,SIZE,FSTYPE,LABEL,TYPE,MOUNTPOINT\n\n# Verify the filesystem types and labels\nsudo blkid | grep sdc\nThis approach to partitioning offers several advantages:\n\nMatches RPi Imager default: Aligns with what users expect from standard Raspberry Pi installations\nEasier to manage: Fewer partitions mean simpler maintenance and troubleshooting\nThe Raspberry Pi firmware requires a FAT32 boot partition to find and load the kernel.\nThe 512MB size ensures plenty of space for kernel updates and multiple kernel versions if needed.\n\nNow you‚Äôre ready to flash Ubuntu Server to these properly prepared partitions! The RPi Imager will use this partition structure and write the system files to the correct locations.\n\nNow we‚Äôll need to mount the partitions by setting up mount points and telling the system to use them.\n\n# Check the partition layout\nsudo lsblk -o NAME,SIZE,FSTYPE,LABEL,TYPE,MOUNTPOINT\n\n# Verify the filesystem types and labels\nsudo blkid | grep sdc\n\n\nAdvanced Partitioning\nAs you begin to utilize your server more, you‚Äôre bound to use up more memory. So, it‚Äôs important to monitor your partition space usage.\n# View disk usage\ndf -h\n\n# View inode usage (for number of files)\ndf -i\n\n# View detailed filesystem information\nsudo tune2fs -l /dev/sda2 | grep -E 'Block count|Block size|Inode count|Inode size'\n\ndf -h: Shows disk usage in human-readable format\ndf -i: Shows inode usage (inode = index node, representing a file)\ntune2fs -l: Lists filesystem information for ext2/3/4 filesystems\ngrep -E: Filters output for specified patterns\n\nFurthermore, you may realize that you want to reformat your SSD at some point because your storage needs changed. You can reformat the partitions using the following code.\n# For online resizing of ext4 (unmounting not required)\nsudo parted /dev/sda\n(parted) resizepart 4 100%  # Resize partition 4 to use all available space\n(parted) quit\n\n# After resizing the partition, expand the filesystem\nsudo resize2fs /dev/sda4\n\nresizepart 4 100%: Resizes partition 4 to use 100% of the remaining available space\nresize2fs: Resizes an ext2/3/4 filesystem to match the partition size"
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-backups",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-backups",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Backups and Basic Automation",
    "text": "Backups and Basic Automation\nNow that we‚Äôve configured the basics, from permissions to networking and ssh to partitions, we‚Äôll want to save those changes in case something happens and to ensure a seamless transition to the SSD for boot media. You‚Äôve already seen some basic backups. The process is the same, essentially creating a folder and then putting a copy of the current file into it and maybe adding a .bak extension to make it clear this is a previous version. That being said, to go through and do this for each and every folder we‚Äôve made changes in is impractical now, let alone in the future when more complex configurations are done. So, in this section, we‚Äôll go over creating a basic script to backup all of our configs and automating the backups.\nFor this section, we‚Äôll use rsync because it provides several important advantages over simple copy commands, that you may remember from the section on ssh:\n\nIncremental backups that only transfer changed files\nPreservation of file permissions, ownership, and timestamps\nBuilt-in compression for efficient transfers\nDetailed progress information and logging\nThe ability to resume interrupted transfers\n\nBefore we start, make sure you have:\n\nA mounted backup drive at /mnt/backups/\n\n\nKey Terms\nBackup Concepts:\n\nBackup: A copy of data that can be recovered if the original is lost or damaged.\nFull Backup: A complete copy of all selected data.\nIncremental Backup: A backup of only the data changed since the last backup.\nDifferential Backup: A backup of all data changed since the last full backup.\nSnapshot: A point-in-time copy of data, often using filesystem features for efficiency.\nRestoration: The process of recovering data from a backup.\nRetention Policy: Rules determining how long backups should be kept.\nBackup Rotation: A systematic approach to reusing backup media over time.\nOffsite Backup: Backups stored in a different physical location for disaster recovery.\n\nBackup Tools and Methods:\n\nrsync: A utility for efficiently copying and synchronizing files locally or remotely.\ntar: Tape Archive, a utility for collecting multiple files into a single archive file.\ndd: A low-level utility that can copy data at the block level.\ncron: A time-based job scheduler in Unix-like systems.\nanacron: A job scheduler that doesn‚Äôt require the system to be running continuously.\nsystemd timers: An alternative to cron for scheduling recurring tasks.\nArchive: A single file containing multiple files, often compressed.\nCompression: Reducing the size of data to save storage space.\nDeduplication: Eliminating duplicate copies of repeating data to save space.\nChecksums: Values calculated from file contents to verify data integrity.\n\nAutomation Concepts:\n\nScript: A file containing a series of commands to be executed.\nShell Script: A script written in a shell language like Bash.\nCrontab: A configuration file specifying scheduled tasks.\nScheduler: A system component that executes tasks at specified times.\nEnvironment Variable: A named value that can affect the behavior of running processes.\nExit Code: A value returned by a command indicating its success or failure.\nRedirection: Changing where command input comes from or output goes to.\nPipeline: Connecting multiple commands by passing the output of one as input to another.\nBackground Process: A process that runs without user interaction, often denoted by an ampersand (&).\nJob Control: Managing the execution of multiple processes from a shell.\n\n\n\nBackup Basics\nFirst, in case you didn‚Äôt do this earlier, we‚Äôll prepare the backup directory structure and set appropriate permissions:\n# Create backup directories if they don't exist\nsudo mkdir -p /mnt/backups/configs\nsudo mkdir -p /mnt/backups/system\n\n# Change ownership to your user (replace 'chris' with your username)\nsudo chown -R chris:chris /mnt/backups\n\n# Set appropriate permissions\nsudo chmod -R 700 /mnt/backups  # Only owner can read/write/execute\nWhile it‚Äôs definitely beneficial to have a local copy of your backups to easily roll back changes, it isn‚Äôt the most secure solution to have all of your information in one place. Furthermore, the SSD is partitioned, but it doesn‚Äôt currently have an OS or any files stored. So, now it‚Äôs time to take advantage of the microSD card we formatted earlier.\nFor the purpose of this guide, I‚Äôll be showing you how to use rsync for a remote transfer to your client machine and how to automatically store backups on the SD card. The script we‚Äôll use saves all of the key user and system information (things like passwords), as well as the configuration changes we made. Additionally, as long as your SD card is mounted to /mnt/backups the backup will automatically be saved to the external memory.\n\n\nConfig Backups\nThe following script demonstrates how to perform the backup while preserving all file attributes:\n#!/bin/bash\n# Using the {} around DATEYMD in the file path ensure it's specified as the variable's value, and the subsequent parts are not included\n\nDATEYMD=$(date +%Y%m%d)\nBACKUP_DIR=\"/mnt/backups/configs/$DATEYMD\"\nLOG_DIR=\"/mnt/backups/logs\"\nLOG_FILE=\"$LOG_DIR/${DATEYMD}_config_backup.log\"\n\n# Check if script is run as root\nif [ \"$(id -u)\" -ne 0 ]; then\n    echo \"Error: This script must be run as root. Try using sudo.\"\n    exit 1\nfi\n\n# Create backup directory\nmkdir -p \"$BACKUP_DIR\"\n\n{\n    # 1. User and Group Information\n    echo \"Backing up User and Group configuration...\"\n    sudo rsync -aAXv /etc/passwd \"$BACKUP_DIR/passwd.bak\"\n    sudo rsync -aAXv /etc/group \"$BACKUP_DIR/group.bak\"\n    sudo rsync -aAXv /etc/shadow \"$BACKUP_DIR/shadow.bak\"\n    sudo rsync -aAXv /etc/gshadow \"$BACKUP_DIR/gshadow.bak\"\n\n    # 2. Crontab Configurations\n    echo \"Backing up Crontab configuration...\"\n    sudo rsync -aAXv /etc/crontab \"$BACKUP_DIR/\"\n    sudo rsync -aAXv /var/spool/cron/crontabs/. \"$BACKUP_DIR/crontabs/\"\n\n     # 3. SSH Configuration\n    echo \"Backing up SSH configuration...\"\n    sudo rsync -aAXv /etc/ssh/. \"$BACKUP_DIR/ssh/\"\n    \n    # Create user_ssh directory\n    mkdir -p \"$BACKUP_DIR/user_ssh\"\n    \n    # Copy SSH user configuration with explicit handling of authorized_keys\n    rsync -aAXv /home/chris/.ssh/config \"$BACKUP_DIR/user_ssh/\" 2&gt;/dev/null || true\n    rsync -aAXv /home/chris/.ssh/id_* \"$BACKUP_DIR/user_ssh/\" 2&gt;/dev/null || true\n    rsync -aAXv /home/chris/.ssh/known_hosts \"$BACKUP_DIR/user_ssh/\" 2&gt;/dev/null || true\n    \n    # Explicitly backup authorized_keys if it exists\n    if [ -f /home/chris/.ssh/authorized_keys ]; then\n        echo \"Backing up authorized_keys file...\"\n        rsync -aAXv /home/chris/.ssh/authorized_keys \"$BACKUP_DIR/user_ssh/\"\n    else\n        echo \"No authorized_keys file found in /home/chris/.ssh/\"\n    fi\n\n    # 4. UFW (Uncomplicated Firewall) Configuration\n    echo \"Backing up ufw configuration...\"\n    sudo rsync -aAXv /etc/ufw/. \"$BACKUP_DIR/ufw/\"\n    sudo ufw status verbose &gt; \"$BACKUP_DIR/ufw_rules.txt\"\n\n    # 5. Fail2Ban Configuration\n    echo \"Backing up fail2ban configuration...\"\n    sudo rsync -aAXv /etc/fail2ban/. \"$BACKUP_DIR/fail2ban/\"\n\n    # 6. Network Configuration\n    echo \"Backing up Network configuration...\"\n    sudo rsync -aAXv /etc/network/. \"$BACKUP_DIR/network/\"\n    sudo rsync -aAXv /etc/systemd/network/. \"$BACKUP_DIR/systemd/network/\"\n    sudo rsync -aAXv /etc/netplan/. \"$BACKUP_DIR/netplan/\"\n    sudo rsync -aAXv /etc/hosts \"$BACKUP_DIR/hosts.bak\"\n    sudo rsync -aAXv /etc/hostname \"$BACKUP_DIR/hostname.bak\"\n    sudo rsync -aAXv /etc/resolv.conf \"$BACKUP_DIR/resolv.conf.bak\"\n    sudo rsync -aAXv /etc/wpa_supplicant/. \"$BACKUP_DIR/wpa_supplicant/\"\n\n    # 7. Systemd Services and Timers\n    echo \"Backing up Systemd Timers configuration...\"\n    sudo rsync -aAXv /etc/systemd/system/. \"$BACKUP_DIR/systemd/\"\n\n    # 8. Logrotate Configuration\n    echo \"Backing up Logrotate configuration...\"\n    sudo rsync -aAXv /etc/logrotate.conf \"$BACKUP_DIR/logrotate.conf.bak\"\n    sudo rsync -aAXv /etc/logrotate.d/. \"$BACKUP_DIR/logrotate.d/\"\n\n    # 9. Timezone and Locale\n    echo \"Backing up Timezone and Locale configuration...\"\n    sudo rsync -aAXv /etc/timezone \"$BACKUP_DIR/timezone.bak\"\n    sudo rsync -aAXv /etc/localtime \"$BACKUP_DIR/localtime.bak\"\n    sudo rsync -aAXv /etc/default/locale \"$BACKUP_DIR/locale.bak\"\n\n    # 10. Keyboard Configuration\n    echo \"Backing up Keyboard configuration...\"\n    sudo rsync -aAXv /etc/default/keyboard \"$BACKUP_DIR/keyboard.bak\"\n\n    # 11. Filesystem Table (fstab)\n    echo \"Backing up filesystem table (fstab)...\"\n    sudo rsync -aAXv /etc/fstab \"$BACKUP_DIR/fstab.bak\"\n    \n    # 12. Backup Package List\n    echo \"Backing up package list...\"\n    dpkg --get-selections &gt; \"$BACKUP_DIR/package_list.txt\"\n\n    # Set appropriate permissions\n    echo \"Configuring backup directory permissions...\"\n    sudo chown -R chris:chris \"$BACKUP_DIR\"\n    sudo chmod -R 600 \"$BACKUP_DIR\"\n\n    echo \"Configuration backup completed at: $BACKUP_DIR\"\n\n} &gt; \"$LOG_FILE\" 2&gt;&1\n\necho \"Logs available at: $LOG_FILE\"\n# Make the script executable\nchmod +x /scripts/config_backup.sh\n\n# Run the script\n./scripts/config_backup.sh\nThe rsync commands use several important options:\n\n-a: Archive mode, preserves almost everything\n-A: Preserve ACLs (Access Control Lists)\n-X: Preserve extended attributes\n-v: Verbose output\n--one-file-system: Don‚Äôt cross filesystem boundaries\n--hard-links: Preserve hard links\n--exclude: Skip specified directories\n\nThe package backup commands use specific flags as well:\n\ndpkg --get-selections: outputs a list of all packages and their status (installed, deinstall, purge)\nThis creates a complete snapshot of your system‚Äôs package state\n\n\nRemote Transfers of Backups\nWe covered rsync vs.¬†scp earlier, so remember that rsync is specifically designed for copying and transferring files, so it offers more sophisticated file synchronization capabilities than basic tools like SCP. If you need a refresher, run the following command from your client machine (laptop), just change the paths to match what your system uses.\nrsync -avz --partial --progress --update chris@ubuntu-pi-server:/mnt/backups/configs/master/ ~/Documents/raspberry_pi_server/backups/configs/master\nThe flags do the following:\n\n-a: Archive mode, which preserves permissions, timestamps, symbolic links, etc.\n-v: Verbose output, showing what files are being transferred\n-z: Compress data during transfer for faster transmission\n--partial: Keep partially transferred files, allowing you to resume interrupted transfers\n--progress: Show progress during transfer\n--update: Skip files that are newer on the receiver (only transfer if source is newer)\n\n\n\n\nRestoring from Backup\nNow that we‚Äôve backed up all of the configurations we‚Äôve made so far, it‚Äôs time to create a script that restores that backup. At the time of writing this, I‚Äôve probably had to reflash a fresh image and reconfigure things between 10 and 20 times. I‚Äôm so good at it, that I can now do it all in under 20 minutes. That being said, it‚Äôs much easier to do when you can just run a script that takes all of the configurations from your Master backup and overwrites the defaults.\nFirst, here are some important things to remember:\n\nThe --delete option during restore will remove files at the destination that don‚Äôt exist in the backup. Use with caution.\nConsider using rsync‚Äôs --dry-run option to test backups and restores without making changes.\nThe backup includes sensitive system files. Store it securely and restrict access.\nConsider encrypting the backup directory for additional security.\nTest the restore process in a safe environment before using in production.\n\nAfter writing this, you can test the script by running it on your server with the boot media you‚Äôve been using (not the SSD)‚Äì just make sure you save the master/ backup and any scripts/configs externally first. You‚Äôll know this succeeds, if nothing changes after the reboot. When you‚Äôve verified that‚Äôs done, we‚Äôll shutdown the server and make the SSD the boot media. For now, let‚Äôs write the config_restore script.\n#!/bin/bash\n\n# Simple Configuration Restoration Script for Ubuntu Pi Server\nBACKUP_DIR=${1:-\"/mnt/backups/configs/master\"}\n\n# Check if script is run as root\nif [ \"$(id -u)\" -ne 0 ]; then\n    echo \"Error: This script must be run as root. Try using sudo.\"\n    exit 1\nfi\n\n# Check if the backup directory exists\nif [ ! -d \"$BACKUP_DIR\" ]; then\n    echo \"Error: Backup directory not found: $BACKUP_DIR\"\n    echo \"Usage: $0 [backup_directory_path]\"\n    exit 1\nfi\n\n# Begin restoration process\necho \"Starting configuration restoration from $BACKUP_DIR...\"\necho \"This will overwrite current system configurations with those from the backup.\"\nread -p \"Continue with restoration? (y/n): \" CONFIRM\nif [[ \"$CONFIRM\" != \"y\" && \"$CONFIRM\" != \"Y\" ]]; then\n    echo \"Restoration aborted by user.\"\n    exit 0\nfi\n\n# 1. Restore User and Group Information\necho \"Restoring user and group information...\"\n[ -f \"$BACKUP_DIR/passwd.bak\" ] && rsync -a \"$BACKUP_DIR/passwd.bak\" /etc/passwd\n[ -f \"$BACKUP_DIR/group.bak\" ] && rsync -a \"$BACKUP_DIR/group.bak\" /etc/group\n[ -f \"$BACKUP_DIR/shadow.bak\" ] && rsync -a \"$BACKUP_DIR/shadow.bak\" /etc/shadow\n[ -f \"$BACKUP_DIR/gshadow.bak\" ] && rsync -a \"$BACKUP_DIR/gshadow.bak\" /etc/gshadow\n\n# Explicitly Set Permissions for Critical System Files\necho \"Fixing critical system file permissions...\"\nchmod 644 /etc/passwd   # Read-write for root, read-only for everyone else\nchmod 644 /etc/group    # Read-write for root, read-only for everyone else  \nchmod 640 /etc/shadow   # Read-write for root, read-only for shadow group\nchmod 640 /etc/gshadow  # Read-write for root, read-only for shadow group\n\n# 2. Restore SSH Configuration\necho \"Restoring SSH configuration...\"\n[ -d \"$BACKUP_DIR/ssh\" ] && rsync -a \"$BACKUP_DIR/ssh/\" /etc/ssh/\nchmod 600 /etc/ssh/ssh_host_*_key 2&gt;/dev/null || true\nchmod 644 /etc/ssh/ssh_host_*_key.pub 2&gt;/dev/null || true\n\n# 3. Restore UFW Configuration\necho \"Restoring UFW configuration...\"\nif [ -d \"$BACKUP_DIR/ufw\" ]; then\n    apt-get install -y ufw &gt;/dev/null\n    rsync -a \"$BACKUP_DIR/ufw/\" /etc/ufw/\nfi\n\n# 4. Restore Fail2Ban Configuration\necho \"Restoring Fail2Ban configuration...\"\nif [ -d \"$BACKUP_DIR/fail2ban\" ]; then\n    apt-get install -y fail2ban &gt;/dev/null\n    rsync -a \"$BACKUP_DIR/fail2ban/\" /etc/fail2ban/\nfi\n\n# 5. Restore Network Configuration\necho \"Restoring network configuration...\"\n[ -d \"$BACKUP_DIR/network\" ] && rsync -a \"$BACKUP_DIR/network/\" /etc/network/\n[ -d \"$BACKUP_DIR/systemd/network\" ] && rsync -a \"$BACKUP_DIR/systemd/network/\" /etc/systemd/network/\n[ -d \"$BACKUP_DIR/netplan\" ] && rsync -a \"$BACKUP_DIR/netplan/\" /etc/netplan/\n[ -f \"$BACKUP_DIR/hosts.bak\" ] && rsync -a \"$BACKUP_DIR/hosts.bak\" /etc/hosts\n[ -f \"$BACKUP_DIR/hostname.bak\" ] && rsync -a \"$BACKUP_DIR/hostname.bak\" /etc/hostname\n[ -f \"$BACKUP_DIR/resolv.conf.bak\" ] && rsync -a \"$BACKUP_DIR/resolv.conf.bak\" /etc/resolv.conf\n[ -d \"$BACKUP_DIR/wpa_supplicant\" ] && rsync -a \"$BACKUP_DIR/wpa_supplicant/\" /etc/wpa_supplicant/\n\n# 6. Restore Filesystem Table (fstab)\necho \"Restoring filesystem table (fstab)...\"\n[ -f \"$BACKUP_DIR/fstab.bak\" ] && rsync -a \"$BACKUP_DIR/fstab.bak\" /etc/fstab\n\n# 7. Restore Package List\necho \"Reinstalling packages from backup...\"\nif [ -f \"$BACKUP_DIR/package_list.txt\" ]; then\n    apt-get update && apt-get install -y dselect\n    dpkg --set-selections &lt; \"$BACKUP_DIR/package_list.txt\"\n    apt-get dselect-upgrade -y\n\n# Restart services\nsystemctl restart systemd-networkd wpa_supplicant@wlan0.service ssh ufw fail2ban \n\necho \"Configuration restoration completed. A system reboot is recommended.\"\nread -p \"Would you like to reboot now? (y/n): \" REBOOT\n[[ \"$REBOOT\" == \"y\" || \"$REBOOT\" == \"Y\" ]] && reboot\n\nexit 0\nYou probably have some questions about the script let me explain some of the decisions I made while doing some trial and error testing.\n\nOriginally, I had the backup directory as a value in the script call itself, now it just defaults to the master/ backup\n\nThis backup is one I know that works and is in the format I‚Äôm hoping\nEasier to have a standard version to reference than relying on monthly backups\n\nI had a lot of issues with incorrect permissions after restoring backups previously, so it needs to be run with sudo\nFirst, the user and group information is important, a lot of processes behind the scenes rely on these configurations\n\nPart of this, I added an explicit chmod call because after the reboot, I was getting an error with whoami\n\nThe command whoami returns which user you are/currently running commands as\nThe user and group info wasn‚Äôt exactly the same, it was leaving my user chris as UID 1000, but changing the group to 1003\nThe chmod call fixes that\nYou can use getent group | grep 'chris' to view all group IDs and assignments\n\n\nSecond, restoring the ssh configurations ensures security and remote connectivity\nThird, ufw increases your system security\nFourth, fail2ban does the same by improving security\nFifth, restoring the network configurations, originally, I had issues because networkd wasn‚Äôt included\n\nThis block ensures all of the systemd configurations are included\n\nSixth, restoring the Filesystem Table (fstab)\n\nThis is easy, since fstab is already a file, we just overwrite what‚Äôs there\n\nSeventh, package restoration\n\ndpkg --set-selections: takes the saved list and marks packages for installation or removal\napt-get dselect-upgrade: then acts on these selections to install missing packages\nThe dselect tool is installed first because it‚Äôs needed for the upgrade process\n\nThen, the specific services we modified are all explicitly started\n\nWhile developing this, some of the services wouldn‚Äôt necessarily start, so I would run into network or ssh issues post-reboot\n\nFinally, the script asks you to reboot your system so all of the changes take affect\n\n# Make the script executable\nchmod +x /scripts/config_restore.sh\n\n# Run the script\nsudo ./scripts/config_restore.sh\n\n\nAutomating Backups with Crontab\nNow that we have both backup and restore scripts in place, the next step is to automate the backup process. Manual backups are valuable but prone to human error, we might forget to run them‚Äì or run them inconsistently. Automation ensures your server configurations are backed up regularly without requiring your intervention, providing an essential safety net against data loss and configuration issues.\nCron is a time-based job scheduler in Unix-like operating systems, including Ubuntu. It enables users to schedule commands or scripts to run automatically at specified intervals. The crontab (cron table) is a configuration file that contains the schedule of cron jobs with their respective commands. Each user on the system can have their own crontab file, and there‚Äôs also a system-wide crontab that requires root privileges to modify.\n\nCreating Automated Backup Jobs\nLet‚Äôs schedule our configuration backup script to run automatically every week. First, we‚Äôll open the crontab editor for the entire system. If you‚Äôre running a more robust system with various users and groups, you‚Äôll probably want to use crontab -e to configure your user specific schedules. That being said, for the time being and because the system is configured, I‚Äôll setup my cron jobs to be system-wide as well:\n# Open the crontab file for the system\nsudo nano /etc/crontab\n\n# Run configuration backup every Sunday at 10:00 PM\n0 22 * * 0 root /home/chris/scripts/config_backup.sh\nHere‚Äôs a handy way to visualize the crontab syntax, with some explanations for each component down below. Note that we‚Äôre using the absolute path /home/chris/scripts/config_backup.sh rather than ./scripts/config_backup.sh. The absolute path starts from the root directory, making it work correctly regardless of the current working directory when cron executes the job as root. When cron runs your commands, it doesn‚Äôt necessarily use the same current or home directory you might expect, so absolute paths are more reliable.\n\n0: - At minute 0\n22: At 10 PM\n*: Every day of the month\n*: Every month\n0: Only on Sunday (day 0)\nroot: Run the command as the root user\nsudo home/chris/scripts/config_backup.sh: The command to execute\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ minute (0-59)\n‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ hour (0-23)\n‚îÇ ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ day of the month (1-31)\n‚îÇ ‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ month (1-12)\n‚îÇ ‚îÇ  ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ day of the week (0-6) (Sunday=0)\n‚îÇ ‚îÇ  ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ user to run the command as\n‚îÇ ‚îÇ  ‚îÇ ‚îÇ ‚îÇ ‚îÇ                                   \n0 22 * * 0 root /scripts/config_backup.sh\nYou may also want to create a monthly backup job that you can use for a /master backup more consistent with your most recent development.\n# Add line to create a master backup on the 1st of every month at 4:00 AM\n0 4 1 * * root /home/chris/scripts/config_backup.sh && rsync -a /mnt/backups/configs/$(date +\\%Y\\%m\\%d)/ /mnt/backups/configs/master/\nThis job runs at 4:00 AM on the first day of each month, creating a regular backup and then copying it to the /mnt/backups/configs/master/ directory. The date command is escaped with backslashes (%) because crontab interprets percent signs specially. Finally, To prevent our backup drive from filling up, let‚Äôs add a job to automatically remove backups older than 90 days, except for the master backup:\n# Add this line to the system crontab\n0 23 * * 0 root find /mnt/backups/configs/ -maxdepth 1 -type d -name \"20*\" -mtime +90 -not -path \"*/master*\" -exec rm -rf {} \\;\n\n0 5 * * 0: Run at 5:00 AM every Sunday\nfind /mnt/backups/configs/: Start searching in the configs directory\n-maxdepth 1: Only look in the immediate directory, not subdirectories\n-type d: Only look for directories\n-name \"20*\": Only match directories starting with ‚Äú20‚Äù (our date-formatted directories)\n-mtime +90: Only match items modified more than 90 days ago\n-not -path \"*/master*\": Exclude anything with ‚Äúmaster‚Äù in the path\n-exec rm -rf {} \\;: Delete each matching directory\n\n\n\nVerifying Jobs\nNow, let‚Äôs verify that everything saved and works. First, close out of nano using Ctrl+O (write-out, or save) and Ctrl+X (Exit). Then let‚Äôs print the contents with cat /etc/crontab. You can also run the following:\n# Check cron service status\nsudo systemctl status cron\n\n# View cron logs\nsudo grep CRON /var/log/syslog\n\n\nTroubleshooting\nI had no issues while making the above changes; however, after restarting cron I got the following message from the systemctl status command.\n\nAfter doing some research, I ran sudo systemctl cat cron.service to view the systemd configuration for cron. In the cron.service file there is the line ExecStart=/usr/sbin/cron -f -P $EXTRA_OPTS. The service is trying to use the $EXTRA_OPTS variable, but it‚Äôs not defined anywhere. The service is configured to look for environment variables in a file specified by this line EnvironmentFile=-/etc/default/cron. The dash before the filepath means this file is optional - if it doesn‚Äôt exist, systemd continues anyway. This explains why cron still works despite the warning, which is why the status command returned a warning, but not any errors. Here‚Äôs what the cron ini file looks like.\n# /usr/lib/systemd/system/cron.service\n[Unit]\nDescription=Regular background program processing daemon\nDocumentation=man:cron(8)\nAfter=remote-fs.target nss-user-lookup.target\n\n[Service]\nEnvironmentFile=-/etc/default/cron\nExecStart=/usr/sbin/cron -f -P $EXTRA_OPTS\nIgnoreSIGPIPE=false\nKillMode=process\nRestart=on-failure\nSyslogFacility=cron\n\n[Install]\nWantedBy=multi-user.target\nLet‚Äôs try resolving this by utilizing systemd and creating a system override. We‚Äôll add one line to the etc/systemd/system/cron.service.d/override.conf. As a note, you‚Äôve probably run into the .d/ paths on your server. Whenever you see .d added to the end of a file path, like /etc/systemd/system/cron.service.d, it just means that‚Äôs a directory which has config overrides.\n# Create a systemd override\nsudo systemctl edit cron.service\n### Editing /etc/systemd/system/cron.service.d/override.conf\n### Anything between here and the comment below will become the contents of the drop-in file\n\nEnvironment=\"EXTRA_OPTS=\"\n\n### Edits below this comment will be discarded\n\n\n### /usr/lib/systemd/system/cron.service\n# [Unit]\n# Description=Regular background program processing daemon\n# Documentation=man:cron(8)\n# After=remote-fs.target nss-user-lookup.target\n# \n# [Service]\n# EnvironmentFile=-/etc/default/cron\n# ExecStart=/usr/sbin/cron -f -P $EXTRA_OPTS\n# IgnoreSIGPIPE=false\n# KillMode=process\n# Restart=on-failure\n# SyslogFacility=cron\n# \n# [Install]\n# WantedBy=multi-user.target\nAfter you implement the solution, restart the systemctl daemon and the cron service.\nsudo systemctl daemon-reload\nsudo systemctl restart cron\nsudo systemctl status cron\nNow, you should see everything working correctly.\n\nThis should resolve the warning message. The service is already working correctly with -f (stay in foreground, which systemd needs) and -P (send messages to syslog) flags, so we‚Äôre just providing an empty value for the EXTRA_OPTS variable to stop systemd from complaining about it being undefined.\nWith these automated backup jobs in place, your Raspberry Pi server will maintain a regular backup schedule without manual intervention. The weekly backups provide recent restore points, while the monthly master backup ensures you always have a stable configuration to fall back on if needed. The cleanup job prevents your backup drive from running out of space over time."
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-change_boot_media",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-change_boot_media",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Changing Your Boot Media Device",
    "text": "Changing Your Boot Media Device\n\nKey Terms\nBoot Process Terminology:\n\nBoot Sequence: The ordered steps followed during system startup.\nBoot Media: The storage device from which a system loads its operating system.\nBoot Loader: Software responsible for loading the operating system kernel.\nFirmware: Software embedded in hardware that provides low-level control.\nUEFI (Unified Extensible Firmware Interface): A modern firmware interface that replaces BIOS.\nEFI System Partition (ESP): A FAT32-formatted partition containing boot loaders and files needed by UEFI.\nBoot Flag: A marker that identifies a partition as bootable.\nGRUB (GRand Unified Bootloader): A popular boot loader for Linux systems.\nKernel Parameters: Options passed to the Linux kernel during boot.\ninitramfs: An initial RAM filesystem loaded during boot to prepare the actual root filesystem.\n\nMedia Transition Concepts:\n\nCloning: Creating an exact copy of a storage device or partition.\nImaging: Creating a file representation of the contents of a storage device.\nBootable Media: Storage media configured to start an operating system.\nFlash: To write data to a memory device, particularly firmware or an operating system image.\nDevice ID: A unique identifier assigned to hardware components.\nUUID (Universally Unique Identifier): A standardized identifier format used to identify filesystems.\nPartition UUID: A unique identifier assigned to a specific partition.\nFilesystem UUID: A unique identifier assigned to a filesystem on a partition.\nsyslinux: A lightweight boot loader for Linux systems.\ndd: A command-line utility used for low-level copying of data between devices.\n\n\n\nBoot Configuration Transition Process\nWhen you‚Äôre moving from one boot device to another on a Raspberry Pi, you‚Äôre essentially telling the system where to find its operating system files. The Raspberry Pi‚Äôs bootloader looks for specific files on a FAT32 partition to begin the boot process, then loads the main operating system from the root partition. This transition is a fundamental server administration skill that every system administrator should understand.\nThe transition involves several critical steps.\n\nFirst, we prepare the SSD with a fresh operating system installation.\nThen we properly shut down the system to ensure no data corruption occurs.\nNext, we physically reconfigure the hardware to make the SSD the primary boot device.\nFinally, we verify that everything works correctly and remove the temporary boot media.\n\nThink of this process like moving into a new house. You‚Äôve already built the structure (partitioning), but now you need to move all your belongings (the operating system) into it, update your mailing address (boot configuration), and ensure everything works in the new location.\nThe good news, we‚Äôve already done most of the work required and what we haven‚Äôt is going to be something we did for the original boot device. We‚Äôll first run a few commands from the server, do one thing on a different computer, and then we‚Äôll be back to the server.\n1. First, you‚Äôll need to flash a fresh Ubuntu Server LTS image onto your newly partitioned SSD. This process will write the operating system files to the appropriate partitions you created. This should be done on your Raspberry Pi server.\n# Before removing the SSD, check its device identifier one more time\nsudo lsblk -o NAME,SIZE,FSTYPE,LABEL,TYPE,MOUNTPOINT\nNow you‚Äôll need to use RPi Imager on your MacBook:\n\nConnect the SSD to your MacBook\nOpen Raspberry Pi Imager\nSelect Ubuntu Server LTS (same version you used before)\nSelect your SSD as the storage device\nUse the exact same advanced settings you used when first setting up your Raspberry Pi:\n\nSame username (chris)\nSame password\nSame SSH key settings\nSame WiFi credentials\n\n\nThe reason we use identical settings is to reduce the amount of changes needed to replicate the environment we previously configured. Your SSH keys, user permissions, and network configurations will all match what we know works for an initial boot, eliminating the need to reconfigure everything from scratch.\n2. Before making any hardware changes, we need to ensure all data is written to disk and all processes are safely terminated. This prevents corruption and data loss during the transition.\n# Save any unsaved work and exit all applications\n# Ensure no important processes are running\n\n# Sync all file system buffers to disk\nsudo sync\n\n# Check for any open files on your current boot device (thumb drive)\nsudo lsof | grep -E '^[^ ]+ +[^ ]+ +[^ ]+ +[^ ]+ +[^ ]+ +[^ ]+ +FIFO'\n\n# Display active processes to ensure nothing critical is running\nps aux | grep -v '\\['\n\nThe sync command forces all pending disk writes to complete immediately. This is crucial because Linux uses write caching for performance, meaning data might still be in memory waiting to be written.\nThe lsof command lists open files, helping you identify any processes that might be accessing the current boot device.\nThe ps aux command shows all active processes, giving you a final check that nothing important is running.\n\n3. A proper shutdown sequence ensures all services stop gracefully and file systems are cleanly unmounted:\n# Perform a clean system shutdown\nsudo shutdown -h now\nThe shutdown command initiates a clean shutdown sequence. The -h flag tells the system to halt (power off) after shutdown. This is important because the system may not power off, especially on older systems‚Äîit could just bring it to single-user mode or runlevel 1, depending on configuration. While now indicates the shutdown should happen immediately. This command:\n\nSends a termination signal to all running processes\nAllows services to save their state and clean up\nUnmounts all filesystems in the correct order\nFinally powers down the system\n\n4. Now comes the physical hardware transition. With your Raspberry Pi powered off:\n\nRemove the thumb drive (current boot device)\nConnect the SSD via USB to the Raspberry Pi\nSet aside the microSD card (backup device)\nEnsure all connections are secure\n\nThis step is straightforward but crucial. The Raspberry Pi will attempt to boot from the first bootable device it finds. By removing the thumb drive and connecting the SSD, we‚Äôre ensuring the Pi finds and uses the SSD as its boot device. We do not need to remove the SD card used for backups, because it never had an OS flashed onto it. The card‚Äôs file system just provides extra memory, instead of being an extra operating system.\n5. Power on your Raspberry Pi and observe the boot process.\nMake sure to connect your monitor and keyboard before the boto begins. The boot process should proceed similarly to your initial setup. The Raspberry Pi firmware reads the configuration from the SSD‚Äôs boot partition, loads the kernel, and then mounts the root filesystem. If everything works correctly, you should see the familiar Ubuntu Server boot messages and eventually reach a login prompt.\nGood news, you won‚Äôt need to do this again (for this server). After this, you‚Äôll have your core system, memory, and configurations complete.\n6. After booting, you‚Äôll need to log in directly to the Raspberry Pi using a keyboard and monitor. This is because the SSH service may not be running automatically on the fresh installation.\n\nConnect a keyboard and monitor to your Raspberry Pi\nLog in with your username and password\nStart the SSH service manually:\n\n# Start the SSH service to enable remote connections\nsudo systemctl start ssh\n\n# Verify the service is running\nsudo systemctl status ssh\n\n# Test a remote connection from your client machine\n# Test SSH connection (initially with password authentication)\nssh chris@192.168.1.151\n\nYou‚Äôll need to specify the local IP because none of the ssh configs are updated yet\n\nYour nonstandard port, 45000 in this guide\nThe local IP may be a different one in the 192.168.0.0/16 range (which is reserved for local IPs)\n\nIf this is working and you can remotely connect, then we can move on to the next step\n\n7. With SSH access established, transfer your configuration backup and restore scripts from your computer to the Raspberry Pi:\nFirst, make sure to create the directory, because it won‚Äôt exist on a fresh boot. The, from your client computer, a MacBook in my case, you‚Äôll run the rsync commands to move the master backup and scripts. Notice that you can use either the ~ shortcut to denote your home directory, or write the path explicitly. We‚Äôll also need to move the master backup to the proper backup directory, so the restore script works properly.\n# On the Raspberry Pi, create the backup directory structure\nsudo mkdir -p /mnt/backups/configs/\n\n# From your MacBook, copy your backup files to the Pi\nrsync -avz ~/path/to/backups/configs/master chris@ubuntu-pi-server:/home/chris/\n\n# Copy your restore script to the Pi\nrsync -avz ~/path/to/scripts/config_restore.sh chris@ubuntu-pi-server:~/scripts/\n\n# Move the master backup directory to the correct location\nsudo mv /home/chris/master /mnt/backups/configs/\n8. Run the Configuration Restore Script\n# Make sure the script is executable\nchmod +x ~/scripts/config_restore.sh\n\n# Run the restore script\nsudo ./scripts/config_restore.sh\n9. Verify the System\n# Verify the root filesystem device\ndf -h /\n\n# Verify the microSD card is mounted properly\ndf -h /mnt/backups\n\n# Check that your backup files are accessible\nls -la /mnt/backups/configs/\n\n# Verify your network settings\nip addr show\n\n# Check System configurations\nsudo systemctl status ssh\nsudo systemctl status ufw\nsudo systemctl status fail2ban\nsudo systemctl status systemd-networkd\nsudo systemctl status wpa_supplicant@wlan0.service\nThe df -h / command shows disk usage statistics for the root filesystem, including which device it‚Äôs mounted from. You should see /dev/sdb2 (or similar) listed as the root device, and /dev/sdb1 as the boot device, not the thumb drive identifier you used before. You‚Äôll see a similar output, just focused on your /backups directory when running the second command. The ls -la command shows you all of the contents of a directory, as well as the permissions. The other commands you should be familiar with by now.\n\n\nFinal Thoughts\nOur Ubuntu Raspberry Pi Server is now booting from the SSD, and all your previous configurations have been restored. You may notice that on reboot, running sudo lsblk will show your SSD under a different This configuration provides a solid foundation for the more advanced server features we‚Äôll implement in the next sections. By moving to SSD boot, we‚Äôve significantly improved our server‚Äôs performance profile. SSDs offer several advantages over traditional storage media:\n\nFaster boot times: Your Raspberry Pi will start much more quickly\nImproved I/O performance: Database operations, file access, and application loading will be noticeably faster\nBetter reliability: SSDs have no moving parts, making them more resilient to physical impacts\nLower power consumption: SSDs typically use less power than traditional hard drives\n\nThe transition to SSD boot also aligns with modern server practices, where solid-state storage is becoming the standard for production environments. This configuration will serve us well as we expand into containerization with Docker and orchestration with Kubernetes.\nThe last thing to do, is give your Raspberry Pi some decency. Now, I‚Äôll attach the fan and case to the Raspberry Pi, so it‚Äôll be ready to run 24/7. This is a straightforward process, but I needed the internet‚Äôs help to figure out which cables connected to which pin.\n\nSafely shutdown your server, and unplug everything.\n\n\n\nInsert the fan into the case.\n\n\n\nInsert the board, with the SD Card inserted, into the case.\n\n\n\nConnect the Red Wire to Pin 4 (5V), Black Wire to Pin 6 (GND), and the Blue Wire to Pin 8 (GPIO14). Leave the Blue Wire unplugged if you want the fan to be always on.\n\n\n\nMake sure everything is all set, if you are passionate about wire management, you can probably make it prettier than I did.\n\n\n\nAdmire your handsome server\n\n\nNow, we‚Äôre ready to move on to some of the fun stuff. If youre fan didn‚Äôt turn on right away, it‚Äôs probably because you followed my images and plugged in the blue (control) wire. When this is connected, it turns the fan off. From what I researched online, it sounds like there is a way to keep it plugged in and then configure the fans usage parameters. That being said, I felt like diving into managing a circuit board from your OS would have been too much of a tangent/rabbit hole for this guide, so that‚Äôs probably something I‚Äôll explore in the future, separately."
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-remote_vs_code",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-remote_vs_code",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Remote Development with VS Code",
    "text": "Remote Development with VS Code\n\nKey Terms\nRemote Development Concepts:\n\nIDE (Integrated Development Environment): A software application that provides comprehensive facilities for software development.\nRemote Development: Developing code on a remote machine while using local tools and interfaces.\nHeadless Development: Writing code on a system without a graphical interface, often remotely.\nExtension: Add-ons that enhance the functionality of development tools.\nWorkspace: A collection of files and folders that make up a development project.\nSync: The process of keeping files consistent between local and remote systems.\nPort Forwarding: Redirecting communication from one network port to another.\nDevelopment Container: A container configured specifically for development purposes.\n\nVS Code Specific Terminology:\n\nVS Code: Visual Studio Code, a code editor developed by Microsoft.\nRemote - SSH Extension: A VS Code extension that allows connecting to and developing on remote machines over SSH.\nRemote Explorer: A VS Code interface for managing remote connections.\nSSH Target: A remote machine configured for SSH access in VS Code.\nSSH Config: Configuration file defining SSH connection properties.\nDev Container: A containerized development environment defined for a VS Code project.\nWorkspace Settings: Project-specific configurations in VS Code.\nSettings Sync: A feature that synchronizes VS Code settings across different instances.\nTask: Configured commands that can be executed within VS Code.\nLaunch Configuration: Settings that define how to debug applications in VS Code.\n\n\n\nVS Code and IDE-based Development\nRemote development with Visual Studio Code transforms your Raspberry Pi server from a command-line environment into a fully-featured development platform. While terminal-based SSH access provides basic server management capabilities, VS Code Remote SSH creates a seamless bridge between your local machine‚Äôs powerful editor interface and your remote server‚Äôs computing resources. This approach combines the convenience of a graphical editor with syntax highlighting, intelligent code completion, and integrated debugging while executing code directly on your Raspberry Pi server‚Äîperfect for resource-intensive tasks or when working with server-specific configurations.\nThe VS Code Remote SSH extension uses your existing SSH configurations to establish a secure connection to your server. Once connected, the extension installs a lightweight VS Code server component on your Raspberry Pi, enabling real-time file editing, terminal access, and extension functionality‚Äîall while maintaining the security of your SSH connection. This method eliminates the need to constantly transfer files between local and remote systems, making development significantly more efficient.\n\nInstalling VS Code and the Remote - SSH Extension\nFirst, you‚Äôll need to install VS Code on your local machine (MacBook Air for me):\n\nDownload the appropriate version for your operating system from VS Code‚Äôs official website\nInstall VS Code following the standard installation process for your operating system\nLaunch VS Code\n\nNext, install the Remote SSH extension by selecting the VS Code extensions tab and then search for Remote - SSH.\n\nAfter that, install the extension.\n\nNow, it‚Äôs time to actually connect remotely. We‚Äôll do this using VS Code‚Äôs command palette. The MacBook shortcut is Cmd+Shift+P. You‚Äôll see that you can also open and edit your ssh_config in VS Code, using the command palette to select your configuration. Then, you‚Äôll essentially interact with your server the same way you would a client-locally developed application in VS Code. First, select the folder you want to open, for the time being, I‚Äôm working out of my home directory /home/chris. Then, you can open up one of your backup scripts, notice how much nicer the UI is when dealing with bash scripts. Finally, you can also still run commands from the command line (obviously, otherwise it‚Äôd be hard to use a headless server), notice here I was copying some of the configuration files we‚Äôve modified to my home. I do that so I can back those up to my laptop.\n\nSelect the Connect to Host... option.\n\nSelect the host you want to connect to, in my case ubuntu-pi-server.\n\nYou‚Äôll see this along the bottom of your window, while VS Code connects to your remote server.\n\nThe first time you connect, VS Code will:\n\nInstall the VS Code server component on your Raspberry Pi\nCreate a .vscode-server directory in your home folder\nEstablish a secure connection\nLoad the remote workspace\n\nIf you set up SSH key authentication as described earlier in the guide, the connection should establish without requiring a password. If you‚Äôre using a passphrase-protected SSH key, you‚Äôll be prompted to enter it. Then, Select the folder option, same as you would locally with VS Code, and then select what you would like to work out of.\n\nAfter connecting successfully, the remote connection is indicated in the bottom-left corner of the VS Code window where you‚Äôll see ‚ÄúSSH: ubuntu-pi-server‚Äù. VS Code will operate as if you were working directly on the Raspberry Pi. Open a file by clicking on it in the file explorer, or type code filename.sh into your terminal CLI to open a file. You can open a new terminal by going to the command palette and selecting Terminal: Create New Terminal or, on MacOS, use Ctrl+backtick. Note: I haven‚Äôt been able to use this to edit files that require root privileges just yet. It seems like my issue is with how/where the code command is stored, and how sudo tries to interpret it.\nThe explorer view will now display the file system of your Raspberry Pi, not your local machine. Any file operations (create, edit, delete) will happen directly on the remote server.\n\nTo install an extension that should run on the remote server (like language servers, debugging tools, etc.):\n\nClick on the Extensions icon in the activity bar\nFind the extension you want to install\nClick the ‚ÄúInstall in SSH: ubuntu-pi-server‚Äù button instead of the regular ‚ÄúInstall‚Äù button\n\nExtensions are categorized into:\n\nUI Extensions: Run on your local machine (themes, UI enhancements)\nWorkspace Extensions: Run on the remote server (language servers, debuggers)\n\nThis separation ensures optimal performance while providing a complete development experience.\n\nFinally, remember to close your connection, although closing the window does the same.\nUsing VS Code with Remote SSH fundamentally transforms your development experience with the Raspberry Pi in several ways:\n\nEdit with confidence: No more struggling with terminal-based editors like nano or vim when complex editing is needed\nSeamless navigation: Easily browse the server‚Äôs filesystem with familiar GUI tools\nIntegrated tools: Git integration, debugging, and terminal access all in one environment\nExtension ecosystem: Leverage thousands of VS Code extensions while working on your remote project\nProductive workflow: Maintain your preferred development environment regardless of the target platform"
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-monitor_maintain",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-monitor_maintain",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Monitoring and Maintenance",
    "text": "Monitoring and Maintenance\n\nKey Terms\nSystem Monitoring Concepts:\n\nMonitoring: The process of observing and tracking system performance and status.\nMetrics: Measurable values that indicate performance or resource usage.\nResource Utilization: The degree to which system resources are being used.\nLoad Average: A measure of CPU utilization over time.\nThreshold: A predefined value that, when exceeded, may trigger notifications or actions.\nBaseline: Normal or expected performance values for comparison.\nReal-time Monitoring: Observing system status as it happens.\nHistorical Data: Saved metrics showing performance trends over time.\nAlert: A notification triggered when monitored values exceed thresholds.\nDashboard: A visual interface displaying multiple metrics at once.\n\nMonitoring Tools and Commands:\n\ntop: A command-line utility showing real-time system resource usage.\nhtop: An enhanced version of top with a more user-friendly interface.\niotop: A utility for monitoring disk I/O usage by processes.\niostat: A command that reports CPU and disk I/O statistics.\nvmstat: A tool that displays virtual memory statistics.\nfree: A command that displays amount of free and used memory.\ndf: A utility that reports filesystem disk space usage.\ndu: A command that estimates file and directory space usage.\nnetstat: A command-line tool that displays network connections, routing tables, and interface statistics.\nss: A modern replacement for netstat for investigating sockets.\n\nMaintenance Terminology:\n\nPatch: A piece of software designed to update or fix issues in a program.\nUpdate: New versions of software that add features or fix bugs.\nUpgrade: A significant update that may involve major changes.\nPackage Manager: A system for installing, updating, and removing software packages.\napt: Advanced Package Tool, the package management system used by Debian and Ubuntu.\nRepository: A storage location from which software packages can be retrieved.\nDependency: A software package required by another package to function.\nCleanup: The process of removing unnecessary files or data.\nScheduled Maintenance: Regular, planned maintenance activities.\nPreventive Maintenance: Activities performed to prevent system failures.\n\nLog Management:\n\nLog: A record of events that occur within the system.\nsyslog: A standard for message logging.\njournald: Systemd‚Äôs logging service that collects and stores logging data.\nLog Rotation: The process of archiving and removing old log files.\nLog Level: The severity or importance assigned to a log entry.\nstdout: Standard output stream where normal process output is written.\nstderr: Standard error stream where error messages are written.\nAudit Log: A record of events relevant to security.\nlogrotate: A utility that manages automatic rotation of log files.\nCentralized Logging: Collecting logs from multiple systems in a central location.\n\n\n\nMonitoring and Maintenance Basics\nFor a 24/7 server, storage reliability is critical. A self-hosted server relies heavily on its storage device, and I/O errors can disrupt services and potentially lead to data loss. Monitoring your SSD‚Äôs health and properly configuring your system to handle power interruptions are essential practices for maintaining a reliable self-hosted server.\nContinuous uptime demands proactive management rather than reactive troubleshooting. Without proper monitoring, subtle hardware degradation can progress undetected until catastrophic failure occurs, resulting in extended downtime and potential permanent data loss. Regular maintenance prevents small issues from cascading into system-wide failures and helps maintain consistent performance over time. For Raspberry Pi servers specifically, where hardware operates in potentially suboptimal conditions (varying temperatures, consumer-grade power supplies, and external storage), monitoring becomes even more crucial. Proper maintenance routines extend hardware lifespan, optimize resource usage, and ensure security vulnerabilities are promptly addressed. The slight overhead of implementing monitoring and scheduled maintenance is substantially offset by avoiding the significant costs of emergency recovery, both in terms of lost data and service disruption. Furthermore, systematically collected performance data enables informed decisions about capacity planning and system upgrades, ensuring your server evolves to meet changing demands without overprovisioning or unexpected resource exhaustion.\nAfter chaning my boot media, I ended up with a an I/O error on my SSD, which resulted in data loss. To give a quick summary, this means the processs by which data is written to and read from memory had an issue. This section will walk through how to diagnose and resolve that issue, how and why to configure security updates on a patching schedule, finally how we can manage logs to ensure effective system information and memory usage.\n\n\nDiagnosing and Resolving SSD Issues\n1. Install the primary tool for storage health monitoring\n# Always update your current system packages before installing a new one\nsudo apt update\nsudo apt install -y smartmontools\nThe smartmontools package provides utilities for monitoring storage devices using SMART (Self-Monitoring, Analysis, and Reporting Technology) - a monitoring system included in most modern storage devices. These tools allow you to check the internal health parameters of your storage device and identify potential issues before they lead to data loss.\n2. Identify your SSD\nsudo lsblk -o NAME,SIZE,MODEL,SERIAL,MOUNTPOINT\n\nRemember, this command lists all block devices with their names, sizes, model information, serial numbers, and mount points. From the output, we can see:\n\nThe Samsung T7 SSD (PSSD T7) is /dev/sda with a capacity of 931.5GB\nIt has two partitions: sda1 (boot partition) and sda2 (root filesystem)\nAn SD card is mounted at /mnt/backups\n\nThe -o flag specifies which columns to display, giving us a comprehensive view of all storage devices connected to the Raspberry Pi.\n3. Check Basic SSD Health\nsudo smartctl -H /dev/sda\n\nThe -H flag performs a basic health check, asking the drive to evaluate its own condition. The result ‚ÄúPASSED‚Äù indicates the drive believes it‚Äôs functioning properly at a hardware level. This is the quickest way to assess if the drive has detected any internal failures.\n4. View Detailed SMART Information\nsudo smartctl -a /dev/sda\n  \nKey findings from the output:\n\nTemperature: 27¬∞C (excellent, well below the 52¬∞C warning threshold)\nPower On Hours: 2,673 (about 111 days)\nUnsafe Shutdowns: 61 (significant issue)\nMedia and Data Integrity Errors: 0 (good)\nAvailable Spare: 100% (excellent)\nPercentage Used: 0% (drive health is excellent)\n\nThe 62 Unsafe Shutdowns occur when the system loses power or crashes without properly unmounting the filesystems. When this happens:\n\nWrite operations may be interrupted: If the system is writing data to the SSD when power is lost, the write operation may be incomplete, leading to partially written files.\nJournal transactions remain unfinished: Modern filesystems like ext4 use journaling to track changes before they‚Äôre committed. Power loss can leave these journals in an inconsistent state.\nFilesystem metadata corruption: Critical filesystem structures may be left in an inconsistent state, potentially making files or directories inaccessible.\nDirty cache data loss: Data waiting in the cache to be written to disk is lost during a sudden power cut.\n\nThese unsafe shutdowns are particularly problematic for SSDs because they use complex internal mechanisms like wear leveling and garbage collection. Interrupting these processes can lead to:\n\nPartial page programming\nIncomplete block erasures\nInconsistent mapping tables\nLost block allocation information\n\nWhile modern SSDs have power loss protection mechanisms, consumer-grade external SSDs like the Samsung T7 may have limited protection compared to enterprise-grade drives. The high number (62) indicates a pattern of improper shutdowns, likely due to:\n\nPower interruptions to the Raspberry Pi\n\nThis was definitely one of the issues I had\nI was using sudo shutdown now and then unplugging, rather than adding the -h flag to halt power\nSometimes, I just unplugged the power (this is bad too)\n\nSystem crashes requiring hard resets\nUnplugging the SSD without properly unmounting it\n\nThis also happened at least once and was probably a major contributor\n\nPower saving features improperly configured\n\nThe -a flag displays all SMART information available from the drive, providing a comprehensive view of the drive‚Äôs health and history.\n5. Attempt a SMART Self-Test\nThis step doesn‚Äôt apply, if you‚Äôre using the Samsung T7, because it doesn‚Äôt support self-tests, which you can see from the smartctl output. That being said, if you‚Äôre using an SSD that allows this, the step may be helpful. The Samsung T7 portable SSD doesn‚Äôt support SMART self-tests. This is common for external/portable SSDs, as they often implement a more limited set of SMART commands. The -t short flag attempts to initiate a short self-test, which would typically take a few minutes to complete.\nsudo smartctl -t short /dev/sda\nThis is the output I received.\nsmartctl 7.4 2023-08-01 r5530 [aarch64-linux-6.8.0-1024-raspi] (local build)\nCopyright (C) 2002-23, Bruce Allen, Christian Franke, www.smartmontools.org\nSelf-tests not supported\n6. Check Filesystem Integrity\n# Check the boot partition\nsudo fsck -n /dev/sda1\n\nOutput shows two issues:\n\nDifferences between boot sector and backup\nDirty bit set - indicating improper unmounting\n\n# Check the root partition\nsudo fsck -n /dev/sda2\n\nOutput indicates the filesystem is clean but warns it‚Äôs currently mounted.\n\nThe -n flag performs a read-only check without making changes, allowing you to safely examine mounted filesystems.\nThe findings confirm that improper shutdowns have affected the filesystem integrity, particularly on the boot partition.\n\n7. Check System Logs for Errors\nsudo dmesg | grep -i 'error\\|ata\\|sda\\|failed\\|i/o'\n\nsudo journalctl -p err..emerg -o short-precise | grep -i 'sda\\|disk\\|i/o\\|error'\n\nsudo journalctl -u smartd\n\nThe output shows normal disk detection and mounting events, without indicating any current I/O errors. No errors related to the disk were found in the system journal. No entries found, indicating the SMART monitoring service isn‚Äôt running. These commands search through different system logs for any reported disk errors. The absence of current errors suggests that despite the history of unsafe shutdowns, the filesystem has remained resilient enough to recover without logging critical errors.\n8. Check the Power Supply Status\nvcgencmd get_throttled\n\nOutput: 0x0\nThis indicates no power-related issues have been detected since boot. The vcgencmd get_throttled command is specific to Raspberry Pi systems and reports if the system has experienced undervoltage, overheating, or other throttling conditions. A non-zero value would indicate power problems affecting the Raspberry Pi. The result 0x0 is good news, suggesting that your current power supply is adequate for normal operation.\n\n\nImplementing Preventive Measures\n1. Enable SMART Monitoring Service\n# Configure the SMART monitoring service\nsudo nano /etc/smartd.conf\n\nThe SMART monitoring daemon (smartd) continuously checks your drive‚Äôs health and can alert you to developing issues. Add this line to monitor your SSD (replace sda with your device if different): /dev/sda -a -o on -S on -s (S/../.././02|L/../../6/03) -m root. This should go above the line with DEVICESCAN, because every configuration after it is ignored.\nThis configuration:\n\n-a: Monitors all SMART attributes\n-o on: Enables automatic offline testing\n-S on: Saves error logs\n-s (S/../.././02|L/../../6/03): Schedules short tests at 2 AM daily and long tests on Saturdays at 3 AM\n-m root: Sends email alerts to the root user\n\nThen enable and start the service.\nsudo systemctl enable smartd\nsudo systemctl start smartd\nTo receive email alerts, you‚Äôll need to configure a mail transfer agent like postfix. Personally, I just didn‚Äôt include that functionality now, but it‚Äôs something I‚Äôll eventually configure; however, I left it in for those curious. The SMART monitoring service provides proactive protection by continuously monitoring your drive‚Äôs health metrics. It can detect deteriorating conditions before they lead to data loss and send you notifications when potential issues are identified.\n2. Configure Proper Filesystem Mount Options\n# Edit /etc/fstab\nsudo nano /etc/fstab\n\n# Add specific options for your root partition\nsudo blkid | grep sda2\nThen add this line to your /etc/fstab file: UUID=your-uuid-here / ext4 defaults,noatime,commit=60 0 1. While your SSD partitions automatically mount at boot, optimizing the mount options can significantly improve resilience against power failures.\nLet‚Äôs look at these options in detail:\n\ndefaults: This incorporates standard mount options: rw (read-write), suid (allow setuid), dev (interpret device files), exec (permit execution of binaries), auto (mountable with -a), nouser (only root can mount), and async (asynchronous I/O).\nnoatime: Disables updating access time attributes on files when they‚Äôre read. This reduces unnecessary write operations, which is especially beneficial for SSDs that have limited write cycles. Every time you read a file without this option, the system would write an update to the file‚Äôs metadata recording when it was last accessed.\ncommit=60: Changes how often filesystem changes are committed to disk (in seconds). The default is 5 seconds, meaning data may stay in RAM for up to 5 seconds before being written to disk. Increasing this to 60 seconds reduces write operations but increases the potential for data loss during a crash. However, it‚Äôs a reasonable compromise for most systems.\n0: This refers to the dump flag. A value of 0 indicates the filesystem should not be backed up by the dump utility (which is rarely used these days).\n1: This is the fsck order. A value of 1 means this filesystem should be checked first during boot if a check is needed. The root filesystem always gets 1, while other filesystems get 2 or higher, or 0 to skip checks.\n\n# Apply the changes without rebooting\nsudo mount -o remount /\nThese optimized mount options help mitigate the impact of unexpected shutdowns by reducing unnecessary writes and ensuring more efficient I/O operations. The trade-off between performance and data safety is balanced by the commit interval - 60 seconds provides reasonable protection while reducing write pressure.\nBy implementing the following measures:\n\nVerifying adequate power supply: The vcgencmd get_throttled output of 0x0 indicates your current power supply is stable.\nEnabling SMART monitoring: Setting up the smartd service provides ongoing monitoring and early warning of potential drive issues.\nOptimizing filesystem mount options: The modified fstab entries with noatime and commit=60 strike a balance between reducing unnecessary writes and maintaining data integrity.\n\nThese implementations together create a more resilient system that:\n\nContinuously monitors drive health\nReduces unnecessary write operations that contribute to SSD wear\nBalances performance with data integrity requirements\nProvides early warning of developing issues\n\nFor a 24/7 self-hosting server, these measures provide essential protection against the most common causes of data loss and system instability. While no solution can completely eliminate the risk from sudden power loss, these configurations significantly reduce the likelihood of filesystem corruption and data loss.\n\n\nLog Management\nEffective log management is crucial for maintaining system health, troubleshooting issues, and detecting security incidents on your Raspberry Pi server. Logs provide insights into system behavior, application performance, and potential security threats. This section will cover fundamental log concepts and practical strategies for managing logs efficiently.\n\nLog Basics\nLinux logs record events occurring within the system and applications, providing essential information for monitoring and troubleshooting. Understanding where logs are stored and how to interpret them allows you to efficiently diagnose problems and maintain system health.\nLet‚Äôs explore the key log locations on your Ubuntu-based Raspberry Pi server:\n# View the main system log\nsudo tail -n 50 /var/log/syslog\n\n# View authentication attempts\nsudo tail -n 50 /var/log/auth.log\n\n# View kernel messages\nsudo dmesg | tail -n 50\n\n# View systemd journal logs\nsudo journalctl -n 50\nKey log files and their purposes:\n\n/var/log/syslog: General system messages and activities\n/var/log/auth.log: Authentication and authorization events\n/var/log/kern.log: Kernel messages and warnings\n/var/log/dpkg.log: Package installation and removal logs\n/var/log/apt/: APT package management activities\n/var/log/fail2ban.log: Failed login attempt blocks\n/var/log/nginx/: Web server logs (if nginx is installed)\n\nUbuntu Server uses two primary logging systems:\n\nTraditional syslog: Text files stored in /var/log/\nSystemd journal: Binary logs accessed through journalctl\n\n# View only error and higher severity messages\nsudo journalctl -p err..emerg\nLog severity levels (from lowest to highest):\n\ndebug: Detailed debugging information\ninfo: Normal operational messages\nnotice: Normal but significant events\nwarning: Potential issues that aren‚Äôt errors\nerr: Error conditions\ncrit: Critical conditions requiring attention\nalert: Actions that must be taken immediately\nemerg: System is unusable\n\n\n\nManagement Tools and Strategy\nProper log management ensures you can efficiently access the information you need while preventing logs from consuming excessive disk space. A balanced strategy involves log rotation, centralized collection, and automated monitoring.\nFirst, let‚Äôs configure logrotate to manage log file growth. We can also create a custom logrotate config for application logs. While this doesn‚Äôt apply to anything we‚Äôve done yet, it will be useful once you start developing and deplying applications on your server. I‚Äôll include this below for anyone that wants a starting point.\n# Examine the main logrotate configuration\ncat /etc/logrotate.conf\n\n# View the service-specific configurations\nls -l /etc/logrotate.d/\n\n# Customize application log management\nsudo nano /etc/logrotate.d/myapplogs\n\n\n/home/chris/apps/*/logs/*.log {\n    weekly\n    rotate 4\n    compress\n    delaycompress\n    missingok\n    notifempty\n    create 0640 chris chris\n}\nThis configuration:\n\nweekly: Rotates logs once per week\nrotate 4: Keeps 4 rotated log files before deletion\ncompress: Compresses rotated logs with gzip\ndelaycompress: Delays compression until the next rotation cycle\nmissingok: Doesn‚Äôt generate errors if log files are missing\nnotifempty: Doesn‚Äôt rotate empty log files\ncreate 0640 chris chris: Creates new log files with specified permissions and ownership\n\nThen, let‚Äôs setup some persistent storage for our journal logs.\n# Configure persistent storage for journal logs\nsudo nano /etc/systemd/journald.conf\n[Journal]\nStorage=persistent\nCompress=yes\nSystemMaxUse=500M\nSystemMaxFileSize=50M\nMaxRetentionSec=1month\n# Apply changes\nsudo systemctl restart systemd-journald\nThis configuration:\n\nStorage=persistent: Saves logs across reboots\nCompress=yes: Compresses older journal files\nSystemMaxUse=500M: Limits total journal size to 500MB\nSystemMaxFileSize=50M: Limits individual journal files to 50MB\nMaxRetentionSec=1month: Automatically removes logs older than a month\n\nFor more detailed systemd journal queries:\n# View logs from a specific service\nsudo journalctl -u ssh\n\n# View logs from a specific time period\nsudo journalctl --since \"2024-05-10\" --until \"2024-05-12\"\n\n# View logs from the current boot\nsudo journalctl -b\nAdvanced log management strategies include:\n\nCentralized logging: Consider setting up a central log server for multiple devices using rsyslog.\nLog analysis tools: For more sophisticated analysis, tools like Logwatch provide automated log summaries.\nSecurity monitoring: Configure fail2ban to monitor logs for security threats and respond automatically.\nCustom alerting: Set up alerts for specific critical events using simple grep scripts and cron jobs.\n\nThese log management practices ensure your Raspberry Pi server maintains manageable, accessible logs while providing the information needed for effective system monitoring and troubleshooting."
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-docker",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-docker",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Docker",
    "text": "Docker\n\nKey Terms\nContainer Concepts:\n\nContainer: A lightweight, standalone, executable package that includes everything needed to run an application.\nImage: A read-only template containing instructions for creating a container.\nContainerization: The process of packaging an application with its dependencies into a container.\nIsolation: The separation of applications from each other and the underlying system.\nVirtual Machine (VM): A virtualized instance of an operating system running on hypervisor software.\nContainer Runtime: Software that executes containers and manages their lifecycle.\nNamespace: A Linux kernel feature that isolates system resources for containers.\nControl Group (cgroup): A Linux kernel feature that limits, accounts for, and isolates resource usage of process groups.\nLayer: A set of read-only files that represent filesystem differences in a Docker image.\nUnion Filesystem: A filesystem service that layers multiple directories into a single unified view.\n\nDocker Specific Terminology:\n\nDocker: A platform for developing, shipping, and running applications in containers.\nDocker Engine: The runtime that builds and runs Docker containers.\nDocker Hub: A cloud-based registry service for Docker images.\nDocker Desktop: An application for managing Docker on Windows and Mac.\nDockerfile: A text file containing instructions to build a Docker image.\nDocker Compose: A tool for defining and running multi-container Docker applications.\nDocker Swarm: A native clustering and orchestration solution for Docker.\nDocker Network: A communication system that enables containers to communicate with each other and the outside world.\nDocker Volume: A mechanism for persisting data generated by and used by Docker containers.\nDocker Registry: A storage and distribution system for Docker images.\n\nContainer Management:\n\nTag: A label attached to an image version for identification.\nRepository: A collection of related Docker images with the same name but different tags.\nPull: The action of downloading an image from a registry.\nPush: The action of uploading an image to a registry.\nBuild: The process of creating a Docker image from a Dockerfile.\nRun: The command to start a container from an image.\nExec: A command to run additional processes in a running container.\nCommit: Creating a new image from changes made to a container.\nStop/Start: Commands to halt and resume container execution.\nRemove: The action of deleting a container or image.\n\nCI/CD with Containers:\n\nCI/CD (Continuous Integration/Continuous Deployment): Practices that automate the integration and deployment of code changes.\nPipeline: A series of automated steps that code changes go through from development to production.\nBuild Automation: The process of automating the creation of software builds.\nIntegration Testing: Testing the interaction between integrated units/modules.\nDeployment Strategy: A planned approach for releasing changes to production.\nArtifact: A byproduct of software development, such as a compiled application or container image.\nRegistry Authentication: The process of securely accessing a container registry.\nWebhook: An HTTP callback triggered by specific events in a development workflow.\nGitHub Actions: GitHub‚Äôs built-in CI/CD tool.\nJenkins: An open-source automation server often used for CI/CD."
  },
  {
    "objectID": "pages/guides/posts/raspberry_pi_server.html#sec-kubernetes",
    "href": "pages/guides/posts/raspberry_pi_server.html#sec-kubernetes",
    "title": "Guide to Setting Up Raspberry Pi for Data Engineering and Virtualization Projects",
    "section": "Kubernetes",
    "text": "Kubernetes\n\nKey Terms\nKubernetes Architecture:\n\nKubernetes (K8s): An open-source platform for automating deployment, scaling, and management of containerized applications.\nCluster: A set of worker machines (nodes) that run containerized applications managed by Kubernetes.\nControl Plane: The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle - of containers.\nNode: A worker machine in Kubernetes, which may be a virtual or physical machine.\nMaster Node: A node that controls the Kubernetes cluster.\nWorker Node: A node that runs applications and workloads.\nkubelet: An agent that runs on each node to ensure containers are running in a Pod.\nkube-proxy: A network proxy that runs on each node to maintain network rules.\netcd: A consistent and highly-available key-value store used as Kubernetes‚Äô backing store for all cluster data.\nContainer Runtime Interface (CRI): The primary protocol for communication between kubelet and container runtime.\n\nKubernetes Resources:\n\nPod: The smallest deployable unit in Kubernetes that can contain one or more containers.\nDeployment: A resource that provides declarative updates for Pods and ReplicaSets.\nReplicaSet: A resource that ensures a specified number of pod replicas are running at any given time.\nService: An abstraction which defines a logical set of Pods and a policy by which to access them.\nNamespace: A mechanism to divide cluster resources between multiple users or projects.\nConfigMap: A resource that stores non-confidential data in key-value pairs.\nSecret: A resource that stores sensitive information such as passwords and tokens.\nVolume: A directory accessible to all containers in a pod, which may be backed by various storage types.\nIngress: A resource that manages external access to services in a cluster, typically HTTP.\nStatefulSet: A resource used to manage stateful applications.\n\nKubernetes Management:\n\nkubectl: The command-line tool for interacting with a Kubernetes cluster.\nkubeadm: A tool for creating and managing Kubernetes clusters.\nHelm: A package manager for Kubernetes that helps install and manage applications.\nManifest: YAML or JSON files that describe Kubernetes resources.\nScaling: Increasing or decreasing the number of replicas of an application.\nSelf-healing: The ability of Kubernetes to automatically replace failed containers.\nRolling Update: A deployment strategy that updates pods one at a time without service interruption.\nBlue-Green Deployment: A deployment strategy that maintains two production environments.\nCanary Deployment: A deployment strategy that releases a new version to a small subset of users.\nAffinity/Anti-Affinity: Rules that influence pod scheduling based on the topology of other pods.\n\nDistributed Computing:\n\nDistributed System: A system whose components are located on different networked computers.\nPySpark: The Python API for Apache Spark, a unified analytics engine for large-scale data processing.\nData Parallelism: A computation pattern where the same operation is performed on different pieces of data simultaneously.\nTask Parallelism: A computation pattern where different operations are performed simultaneously.\nWorker: A node or process that executes assigned tasks.\nMaster: A node or process that coordinates the distribution of tasks to workers.\nJob: A complete computational task, often broken down into smaller tasks.\nTask: A unit of work assigned to a worker.\nDAG (Directed Acyclic Graph): A structure used to represent the sequence of operations in a job.\nResource Allocation: The process of assigning computational resources to jobs or tasks."
  }
]
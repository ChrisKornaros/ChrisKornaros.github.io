[
  {
    "objectID": "pages/guides/guides.html",
    "href": "pages/guides/guides.html",
    "title": "Guides",
    "section": "",
    "text": "Welcome to the Guides section of the website! This is the landing page for step-by-step guides and instructions for various tools and workflows that I‚Äôve used in the past or am currently exploring.",
    "crumbs": [
      "Guides"
    ]
  },
  {
    "objectID": "pages/guides/guides.html#future-guides-planned",
    "href": "pages/guides/guides.html#future-guides-planned",
    "title": "Guides",
    "section": "Future Guides (Planned)",
    "text": "Future Guides (Planned)\nHere are some topics I plan to cover in the future: - DuckDB - dbt - PostgreSQL\nStay tuned for updates!",
    "crumbs": [
      "Guides"
    ]
  },
  {
    "objectID": "pages/guides/guides.html#current-guides",
    "href": "pages/guides/guides.html#current-guides",
    "title": "Guides",
    "section": "Current Guides",
    "text": "Current Guides",
    "crumbs": [
      "Guides"
    ]
  },
  {
    "objectID": "pages/guides/posts/uv.html",
    "href": "pages/guides/posts/uv.html",
    "title": "uv, the Python Project and Package Manager",
    "section": "",
    "text": "A basic guide on using uv the package and projects manager for Python developers.",
    "crumbs": [
      "uv"
    ]
  },
  {
    "objectID": "pages/guides/posts/uv.html#introduction",
    "href": "pages/guides/posts/uv.html#introduction",
    "title": "uv, the Python Project and Package Manager",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\nVS Code Shortcuts\n\n\n\nIf you‚Äôre using VS Code, here are some useful shortcuts. - Note, use CMD-K CMD-S to open the keyboard shortcuts. - SHFT-CMD-i inserts a code block\n\n\nuv is an Open Source project by Astral, the makers of ruff, that is self described (and worthy of the title) as an extremely fast Python package and project manager, written in Rust.\n\nüöÄ A single tool to replace pip, pip-tools, pipx, poetry, pyenv, twine, virtualenv, and more.\n‚ö°Ô∏è 10-100x faster than pip.\nüêç Installs and manages Python versions.\nüõ†Ô∏è Runs and installs Python applications.\n‚ùáÔ∏è Runs scripts, with support for inline dependency metadata.\nüóÇÔ∏è Provides comprehensive project management, with a universal lockfile.\nüî© Includes a pip-compatible interface for a performance boost with a familiar CLI.\nüè¢ Supports Cargo-style workspaces for scalable projects.\nüíæ Disk-space efficient, with a global cache for dependency deduplication.\n‚è¨ Installable without Rust or Python via curl or pip.\nüñ•Ô∏è Supports macOS, Linux, and Windows.\n\nI‚Äôm only just beginning to learn and use the tool in my own projects (including converting my existing project environments to uv) and from what I‚Äôve seen it‚Äôs going to make life much easier. That being said, while you overwrite the muscle memory developed for years with pip and venv, there will be some growing pains; however, for those who are less familiar with what I‚Äôm talking about, I‚Äôll still explain some basic concepts and snags that I both run and ran into.",
    "crumbs": [
      "uv"
    ]
  },
  {
    "objectID": "pages/guides/posts/uv.html#basic-workflow-and-guide",
    "href": "pages/guides/posts/uv.html#basic-workflow-and-guide",
    "title": "uv, the Python Project and Package Manager",
    "section": "Basic workflow and guide",
    "text": "Basic workflow and guide\n\nConcepts to Know Before Getting Started\n\nBasic knowledge of directories, bash (zsh in the case of MacOS), and using the CLI bash\nBasic knowledge of Python, common project structures, and simple workflows Python\nBasic knowledge of git (for local version control) and GitHub (for collaboration) git and GitHub basics\n\n\n\nInitializing a Project\n\nLocal Repository\nThe nice thing about uv is that it‚Äôs designed to make Python development easier, so there aren‚Äôt any head-scratching gotchas.\nFor the sake of this example and entire template, let‚Äôs assume I‚Äôm currently sitting in my main directory. For some that might be home, others app, for MacOS the default is /usr/yourusername, or maybe you prefer to put all projects in a Documents or Projects folder. Anyways, to start up a project you can do one of two things:\n\nHave uv do everything, and then change directories\n\nuv init uv_basic\ncd uv_basic\n\nCreate the directory, change directories, and then have uv do everything\n\nmkdir uv_basic\ncd uv_basic\nuv init\n\n\nThis will create 4 files and initalize a local git repository:\n\n.python-version\n.pyproject.toml\nhello.py\nREADME.md\n.git\n.gitignore\n\n\n\n\n\n\n\nuv and the .gitignore file\n\n\n\nThe nice thing about uv is that it autopopulates your .gitignore file with a few files and patterns, not to mention, it provides some basic tagging for what it puts in there. Just open the file (it‚Äôs plain text) to see. Since I‚Äôm saving my progress with this repo using git, I want to keep the overall file size down. So, I also included the .html and .ipynb file that Quarto generates because they can get large fast. Additionally, when you initialize your GitHub repo with the CLI‚Äôs repo creation process, I don‚Äôt include a README or .gitignore, because those are included in uv init.\n\n\n\n\nRemote Repository\nFor anyone familiar with software development you‚Äôve probably heard of GitHub or GitLab. I‚Äôm more familiar, professionally and personally, with GitHub (which is what I‚Äôll be using in this example); however, there are a large amount of people that prefer GitLab because it is better for some enterprise and personal use cases‚Äì GitHub vs.¬†GitLab. For this, you‚Äôll want to install the GitHub CLI. Then, you can follow along.\n\nVerify the installations and make sure to get your credentials setup, in git\n\nwhich gh and which git\n\nAdd your name and email\n\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your-email@example.com\"\n\nAuthenticate access to GitHub\n\ngh auth login\nUsing the CLI option, follow the instructions\nSelect HTTPS for the easier connection option\n\nVerify you have proper access to your GitHub\n\ngh auth status ```bash github.com ‚úì Logged in to github.com account itsmeChis (keyring)\n\n\n\nActive account: true\nGit operations protocol: https\nToken: gho_************************************\nToken scopes: ‚Äòdelete_repo‚Äô, ‚Äògist‚Äô, ‚Äòread:org‚Äô, ‚Äòrepo‚Äô, ‚Äòworkflow‚Äô\n\n\n   2. `gh repo list`\n5. Assuming you haven't, create your project repo from the CLI (you can also do so using the GitHub.com GUI, but I prefer this way to reinforce my learning)\n   1. `gh repo create`\n   2. `Create a new repository from scratch`\n   3. `uv basic`\n   4. *optional description*\n   5. `Public`\n   6. `GNU Affero General Public License v3.0` [Which license do you need?](https://choosealicense.com)\n![GitHub CLI Repo Creation](images/gh_cli_repo_setup.png)\n1. Set the newly created repo as the local git repo's upstream\n   1. This will result in an error (`git pull`)\n   2. Set the global config to merge [git pull](https://git-scm.com/docs/git-pull)\n   3. git pull with a commit message\n   4. git status to verify\n   5. git push\n\n### Adding and managing dependencies {#sec-2-dep}\nThus far, the workflow with uv isn't too dissimilar from using pip and venv, but managing dependencies and testing scripts is where uv shines. As you'll see below, with pip and venv, you have to manually create the virtual environment, activate it, install dependencies, manage requirements files, and then run your script. With uv, however, almost all of that is done automatically and things like uv pip list or uv venv are only there for backwards compatibility. A lot of the tedious pieces of the DevOps workflow are now obsolete or handled in the background.\n\n#### Using pip and venv\nWhen using a combination of pip and venv, your typical workflow is straightforward, but becomes complicated if you need to uninstall certain packages or make quick, iterative tests of code. \n\n```bash\nmkdir uv_basic\ncd uv_basic\npython -m venv .venv\nsource .venv/bin/activate\npip install duckdb\npip install numpy\npip freeze &gt; requirements.txt\npython script.py\n\n# Realize you don't need numpy, so you want to uninstall it and keep your environment cleaner\ndeactivate\nrm -r .venv\npython -m venv .venv\nsource .venv/bin/activate\n# Two options here, delete numpy from requirements.txt, not scalable with many packages, or reinstall just duckdb, also not scaleable\npip install duckdb\npip freeze &gt; requirements.txt\npython script.py\nAs you can see, the initial workflow isn‚Äôt horrible, but if you need to make a change to the environment or just want to test something small, the number of steps quickly multiplies.\n\n\nUsing uv\nCompare that with the streamlined uv workflow.\nuv init uv_basic\ncd uv_basic\nuv add duckdb\nuv add numpy\nuv run script.py\nuv remove numpy\nuv run script.py\nThe workflow improvements and efficiency should be obvious. The nice thing is that uv functions as your standalone virtual environment, without the need for activation or deactivation. Using uv add will add a dependency to both your pyproject.toml file and your uv.lock file. Additionally, if you are more familiar with verifying using pip, running uv pip list will show that the package is there (although the pip functionality is obsolete and only for backwards compatibility at this point). If you want to remove a package, simply use uv remove and that will also remove it from the .toml and .lock files. The last feature you‚Äôll need to understand (to use uv at a basic level) is uv sync. Simply put, it syncs your environment with the project‚Äôs dependencies/lock file. This ensures that the exact versions specified in your lockfile are used in your environment‚Äì dependencies may be added, removed, or updated if there are updates to the declared dependencies.\nTo cap this off, here are some common use cases for uv sync: - Run uv sync (without ‚Äìfrozen) to keep dependencies up-to-date and to resolve changes. - Use uv sync ‚Äìfrozen to validate dependencies without altering them\n\n\n\nConverting your Legacy Projects to uv\nNow that you‚Äôve seen the benefits of uv, as well as the workflow differences, you probably want to give it a try or even convert entire projects to uv. The good news is that this is simple and only requires a few modifications to get things up and running. The general workflow is the same as I outlined above, you‚Äôll just be cleaning up your local environment and reinstalling things along the way. The project I converted to use uv for this example utilizes DuckDB and dbt for the database and data modeling/ETL. I‚Äôll include some dbt specific information, for example if you move your database file from a subdirectory to the main one, remember to update your dbt profiles in your global dbt location.\n\nChange directories to your specific project directory\nRun uv init, it will create any file or folder that isn‚Äôt currently in the main folder\n\nIf you already have a .git folder and commit history, uv will not delete or overwrite the original folder.\n\nAdd all of the dependencies you need, then remove your requirements file (it‚Äôs no longer needed)\n\nAs of writing this, I wasn‚Äôt sure how to use uv add with the legacy requirements file, uv pip install -r kind of worked, but didn‚Äôt actually add the dependencies to the .toml or .lock files\nThere must be an easier way to bulk add dependencies, but I manually did it\nIn my case, I had to remember to add both dbt and dbt-duckdb, so the adapter would work\n\nInstall all of the CLI tools that you need, and don‚Äôt want or use globally\n\nIn my case, I need jupyter, quarto, and dbt, but I also have the latter two installed globally\n\nVerify that uv can run things correctly\n\nI first used uv run hello.py to verify that the basic functionality is there\nThen, I ran a more complex script, that imports and uses duckdb, to ensure the packages are installing and running as intended\nThen, I used uv tool list to verify which CLI tools are installed\nFinally, I verified that the CLI tools work, by using uv run dbt run --select transform to test dbt model functionality in uv\n\n\n\n\nFinal Thoughts\nSo that‚Äôs it! Overall, uv is incredibly easy to setup and configure because it builds on the classic workflows, while simplifying or abstracting some of the process. You also saw how easy it is to start using uv with older projects that use the legacy workflow. At the time of writing this, I‚Äôve only been using uv for a few days, so I‚Äôm sure there are things I got wrong or missed, please comment to let me know!\nI‚Äôm happy to chat and love learning about data, as well as what folks in this space are working on. Connect with me on Bluesky @chriskornaros.bsky.social to follow along with what I‚Äôm working on, learning, or just to say hi! Below are some other notes and thoughts I had while working on this write up.\n\nGeneral Notes\n\nIt seems that while tools are specific to a uv project instance (i.e.¬†uv_basic returns the .venv dir when asking which jupyter, but test before intalling anything say it can‚Äôt be found), when you use uv tool install it installs it to the system wide uv\nuv pip list defaults to the global (non-uv or non-pip) python environment (in my case it‚Äôs pip and wheel), but once you install something (using add, pip install, etc.) it switches the context to the current parent uv dir (i.e.¬†test, instead of uv_basic)\n\nTools are still listed even after this\n\nuv tool install only works when installing python package specific tools, but DuckDB for Python (for example) doesn‚Äôt come packaged with the DuckDB CLI tools, so uv tool install duckdb won‚Äôt install the DuckDB CLI features\nIt seems that saving variable with duckdb.sql(‚Ä¶).show() and then printing the type of that, just prints the query output, insteaed of the type\nBased on tests, the workflow changes are as follows",
    "crumbs": [
      "uv"
    ]
  },
  {
    "objectID": "pages/projects/projects.html",
    "href": "pages/projects/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Landing page for the project portfolio portion of this website. Contains all of my public repositories and projects (for now, may include future consulting or paid side work, but I don‚Äôt do that at the moment), including both the code in repositories and write ups (where applicable).\nCurrently, there are two categories of projects I‚Äôm working on:\n\nData Engineering and Architecture\nData Science and Machine Learning",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/bank_etl.html",
    "href": "pages/projects/data_engineering/posts/bank_etl.html",
    "title": "Bank Marketing ETL Project",
    "section": "",
    "text": "Piggy bank\nPersonal loans are a lucrative revenue stream for banks. The typical interest rate of a two-year loan in the United Kingdom is around 10%. This might not sound like a lot, but in September 2022 alone UK consumers borrowed around ¬£1.5 billion, which would mean approximately ¬£300 million in interest generated by banks over two years!\nYou have been asked to work with a bank to clean the data they collected as part of a recent marketing campaign, which aimed to get customers to take out a personal loan. They plan to conduct more marketing campaigns going forward so would like you to ensure it conforms to the specific structure and data types that they specify so that they can then use the cleaned data you provide to set up a PostgreSQL database, which will store this campaign‚Äôs data and allow data from future campaigns to be easily imported.\nThey have supplied you with a csv file called \"bank_marketing.csv\", which you will need to clean, reformat, and split the data, saving three final csv files. Specifically, the three files should have the names and contents as outlined below:",
    "crumbs": [
      "Data Engineering and Architecture",
      "Bank Marketing ETL"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/bank_etl.html#client.csv",
    "href": "pages/projects/data_engineering/posts/bank_etl.html#client.csv",
    "title": "Bank Marketing ETL Project",
    "section": "client.csv",
    "text": "client.csv\n\n\n\n\n\n\n\n\n\ncolumn\ndata type\ndescription\ncleaning requirements\n\n\n\n\nclient_id\ninteger\nClient ID\nN/A\n\n\nage\ninteger\nClient‚Äôs age in years\nN/A\n\n\njob\nobject\nClient‚Äôs type of job\nChange \".\" to \"_\"\n\n\nmarital\nobject\nClient‚Äôs marital status\nN/A\n\n\neducation\nobject\nClient‚Äôs level of education\nChange \".\" to \"_\" and \"unknown\" to np.NaN\n\n\ncredit_default\nbool\nWhether the client‚Äôs credit is in default\nConvert to boolean data type: 1 if \"yes\", otherwise 0\n\n\nmortgage\nbool\nWhether the client has an existing mortgage (housing loan)\nConvert to boolean data type: 1 if \"yes\", otherwise 0",
    "crumbs": [
      "Data Engineering and Architecture",
      "Bank Marketing ETL"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/bank_etl.html#campaign.csv",
    "href": "pages/projects/data_engineering/posts/bank_etl.html#campaign.csv",
    "title": "Bank Marketing ETL Project",
    "section": "campaign.csv",
    "text": "campaign.csv\n\n\n\n\n\n\n\n\n\ncolumn\ndata type\ndescription\ncleaning requirements\n\n\n\n\nclient_id\ninteger\nClient ID\nN/A\n\n\nnumber_contacts\ninteger\nNumber of contact attempts to the client in the current campaign\nN/A\n\n\ncontact_duration\ninteger\nLast contact duration in seconds\nN/A\n\n\nprevious_campaign_contacts\ninteger\nNumber of contact attempts to the client in the previous campaign\nN/A\n\n\nprevious_outcome\nbool\nOutcome of the previous campaign\nConvert to boolean data type: 1 if \"success\", otherwise 0.\n\n\ncampaign_outcome\nbool\nOutcome of the current campaign\nConvert to boolean data type: 1 if \"yes\", otherwise 0.\n\n\nlast_contact_date\ndatetime\nLast date the client was contacted\nCreate from a combination of day, month, and a newly created year column (which should have a value of 2022);  Format = \"YYYY-MM-DD\"",
    "crumbs": [
      "Data Engineering and Architecture",
      "Bank Marketing ETL"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/bank_etl.html#economics.csv",
    "href": "pages/projects/data_engineering/posts/bank_etl.html#economics.csv",
    "title": "Bank Marketing ETL Project",
    "section": "economics.csv",
    "text": "economics.csv\n\n\n\n\n\n\n\n\n\ncolumn\ndata type\ndescription\ncleaning requirements\n\n\n\n\nclient_id\ninteger\nClient ID\nN/A\n\n\ncons_price_idx\nfloat\nConsumer price index (monthly indicator)\nN/A\n\n\neuribor_three_months\nfloat\nEuro Interbank Offered Rate (euribor) three-month rate (daily indicator)\nN/A\n\n\n\nimport pandas as pd\nimport numpy as np\n\n# Start coding here...\ndf = pd.read_csv(\"bank_marketing.csv\")\n\nfor col in [\"credit_default\", \"mortgage\", \"previous_outcome\", \"campaign_outcome\"]:\n    print(col)\n    print(\"--------------\")\n    print(df[col].value_counts())\ncredit_default\n--------------\nno         32588\nunknown     8597\nyes            3\nName: credit_default, dtype: int64\nmortgage\n--------------\nyes        21576\nno         18622\nunknown      990\nName: mortgage, dtype: int64\nprevious_outcome\n--------------\nnonexistent    35563\nfailure         4252\nsuccess         1373\nName: previous_outcome, dtype: int64\ncampaign_outcome\n--------------\nno     36548\nyes     4640\nName: campaign_outcome, dtype: int64\nclient = df[['client_id', 'age', 'job', 'marital', 'education', 'credit_default', 'mortgage']]\ncampaign = df[['client_id', 'number_contacts', 'contact_duration', 'previous_campaign_contacts', 'previous_outcome', 'campaign_outcome', 'day', 'month']]\neconomics = df[['client_id', 'cons_price_idx', 'euribor_three_months']]\n\nprint(client.head())\nprint(campaign.head())\nprint(economics.head())\n   client_id  age        job  marital    education credit_default mortgage\n0          0   56  housemaid  married     basic.4y             no       no\n1          1   57   services  married  high.school        unknown       no\n2          2   37   services  married  high.school             no      yes\n3          3   40     admin.  married     basic.6y             no       no\n4          4   56   services  married  high.school             no       no\n   client_id  number_contacts  contact_duration  ...  campaign_outcome day month\n0          0                1               261  ...                no  13   may\n1          1                1               149  ...                no  19   may\n2          2                1               226  ...                no  23   may\n3          3                1               151  ...                no  27   may\n4          4                1               307  ...                no   3   may\n\n[5 rows x 8 columns]\n   client_id  cons_price_idx  euribor_three_months\n0          0          93.994                 4.857\n1          1          93.994                 4.857\n2          2          93.994                 4.857\n3          3          93.994                 4.857\n4          4          93.994                 4.857\nimport numpy as np\n\nclient_c = client.copy()\nclient_c['job'] = client_c['job'].replace('.', '_')\nclient_c['education'] = client_c['education'].str.replace('.', '_')\nclient_c['education'].replace('unknown', np.NaN, inplace=True)\nclient_c['credit_default'] = client_c['credit_default'].apply(lambda x: 1 if x == 'yes' else 0)\nclient_c['credit_default'] = client_c['credit_default'].astype('bool')\nclient_c['mortgage'] = client_c['mortgage'].apply(lambda x: 1 if x == 'yes' else 0)\nclient_c['mortgage'] = client_c['mortgage'].astype('bool')\n\nprint(client_c.head())\n   client_id  age        job  marital    education  credit_default  mortgage\n0          0   56  housemaid  married     basic_4y           False     False\n1          1   57   services  married  high_school           False     False\n2          2   37   services  married  high_school           False      True\n3          3   40     admin.  married     basic_6y           False     False\n4          4   56   services  married  high_school           False     False\ncampaign_c = campaign.copy()\n\ncampaign_c['previous_outcome'] = campaign_c['previous_outcome'].apply(lambda x: 1 if x == 'success' else 0).astype('bool')\ncampaign_c['campaign_outcome'] = campaign_c['campaign_outcome'].apply(lambda x: 1 if x == 'yes' else 0).astype('bool')\ncampaign_c['year'] = 2022\ncampaign_c['last_contact_date'] = pd.to_datetime(campaign_c['day'].astype(str) + campaign_c['month'] + campaign_c['year'].astype(str), format='%d%b%Y')\n\ncampaign_c.head()\n\n\n\n\n\n\n\n\nclient_id\n\n\nnumber_contacts\n\n\ncontact_duration\n\n\nprevious_campaign_contacts\n\n\nprevious_outcome\n\n\ncampaign_outcome\n\n\nday\n\n\nmonth\n\n\nyear\n\n\nlast_contact_date\n\n\n\n\n\n\n0\n\n\n0\n\n\n1\n\n\n261\n\n\n0\n\n\nFalse\n\n\nFalse\n\n\n13\n\n\nmay\n\n\n2022\n\n\n2022-05-13\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n149\n\n\n0\n\n\nFalse\n\n\nFalse\n\n\n19\n\n\nmay\n\n\n2022\n\n\n2022-05-19\n\n\n\n\n2\n\n\n2\n\n\n1\n\n\n226\n\n\n0\n\n\nFalse\n\n\nFalse\n\n\n23\n\n\nmay\n\n\n2022\n\n\n2022-05-23\n\n\n\n\n3\n\n\n3\n\n\n1\n\n\n151\n\n\n0\n\n\nFalse\n\n\nFalse\n\n\n27\n\n\nmay\n\n\n2022\n\n\n2022-05-27\n\n\n\n\n4\n\n\n4\n\n\n1\n\n\n307\n\n\n0\n\n\nFalse\n\n\nFalse\n\n\n3\n\n\nmay\n\n\n2022\n\n\n2022-05-03\n\n\n\n\n\ncampaign_c['previous_outcome'].value_counts()\nFalse    39815\nTrue      1373\nName: previous_outcome, dtype: int64\nclient = client_c\ncampaign = campaign_c.drop(['month', 'day', 'year'], axis=1)\n\nprint(client.head())\nprint(campaign.head())\nprint(economics.head())\n   client_id  age        job  marital    education  credit_default  mortgage\n0          0   56  housemaid  married     basic_4y           False     False\n1          1   57   services  married  high_school           False     False\n2          2   37   services  married  high_school           False      True\n3          3   40     admin.  married     basic_6y           False     False\n4          4   56   services  married  high_school           False     False\n   client_id  number_contacts  ...  campaign_outcome  last_contact_date\n0          0                1  ...             False         2022-05-13\n1          1                1  ...             False         2022-05-19\n2          2                1  ...             False         2022-05-23\n3          3                1  ...             False         2022-05-27\n4          4                1  ...             False         2022-05-03\n\n[5 rows x 7 columns]\n   client_id  cons_price_idx  euribor_three_months\n0          0          93.994                 4.857\n1          1          93.994                 4.857\n2          2          93.994                 4.857\n3          3          93.994                 4.857\n4          4          93.994                 4.857\nclient.to_csv('client.csv', index=False)\ncampaign.to_csv('campaign.csv', index=False)\neconomics.to_csv('economics.csv', index=False)",
    "crumbs": [
      "Data Engineering and Architecture",
      "Bank Marketing ETL"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/data_engineering.html",
    "href": "pages/projects/data_engineering/data_engineering.html",
    "title": "Data Engineering and Architecture Projects",
    "section": "",
    "text": "Landing page for all my project posts related to Data Engineering and Architecture.",
    "crumbs": [
      "Data Engineering and Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/data_engineering.html#projects",
    "href": "pages/projects/data_engineering/data_engineering.html#projects",
    "title": "Data Engineering and Architecture Projects",
    "section": "Projects",
    "text": "Projects",
    "crumbs": [
      "Data Engineering and Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/login_validation.html",
    "href": "pages/projects/data_science/posts/login_validation.html",
    "title": "User Login Validation",
    "section": "",
    "text": "login_img\n\n\nYou recently joined a small startup as a junior developer. The product managers have come to you for help improving new user sign-ups for the company‚Äôs flagship mobile app.\nThere are lots of invalid and incomplete sign-up attempts crashing the app. Before creating new accounts, you suggest standardizing validation checks by writing reusable Python functions to validate names, emails, passwords, etc. The managers love this idea and task you with coding core validation functions for improving sign-ups. It‚Äôs your job to write these custom functions to check all user inputs to ensure they meet minimum criteria before account creation to reduce crashes.\n# Re-run this cell\n# Preloaded data for validating email domain.\ntop_level_domains = [\n    \".org\",\n    \".net\",\n    \".edu\",\n    \".ac\",\n    \".gov\",\n    \".com\",\n    \".io\"\n]\n# Start coding here. Use as many cells as you need.\ndef validate_name(name):\n    if type(name) != str:\n            return False\n    elif len(name) &lt;= 2: \n        return False\n    else:\n        return True\n\ndef validate_email(email):\n    if '@' not in email:\n        return False\n    for domain in top_level_domains:\n        if domain in email:\n            return True\n    return False",
    "crumbs": [
      "Data Science and Machine Learning",
      "Login Validation"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/netflix.html",
    "href": "pages/projects/data_science/posts/netflix.html",
    "title": "Netflix Movies",
    "section": "",
    "text": "Movie popcorn on red background\nNetflix! What started in 1997 as a DVD rental service has since exploded into one of the largest entertainment and media companies.\nGiven the large number of movies and series available on the platform, it is a perfect opportunity to flex your exploratory data analysis skills and dive into the entertainment industry. Our friend has also been brushing up on their Python skills and has taken a first crack at a CSV file containing Netflix data. They believe that the average duration of movies has been declining. Using your friends initial research, you‚Äôll delve into the Netflix data to see if you can determine whether movie lengths are actually getting shorter and explain some of the contributing factors, if any.\nYou have been supplied with the dataset netflix_data.csv , along with the following table detailing the column names and descriptions. This data does contain null values and some outliers, but handling these is out of scope for the project. Feel free to experiment after submitting!",
    "crumbs": [
      "Data Science and Machine Learning",
      "Netflix Movies"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/netflix.html#the-data",
    "href": "pages/projects/data_science/posts/netflix.html#the-data",
    "title": "Netflix Movies",
    "section": "The data",
    "text": "The data\n\nnetflix_data.csv\n\n\n\nColumn\nDescription\n\n\n\n\nshow_id\nThe ID of the show\n\n\ntype\nType of show\n\n\ntitle\nTitle of the show\n\n\ndirector\nDirector of the show\n\n\ncast\nCast of the show\n\n\ncountry\nCountry of origin\n\n\ndate_added\nDate added to Netflix\n\n\nrelease_year\nYear of Netflix release\n\n\nduration\nDuration of the show in minutes\n\n\ndescription\nDescription of the show\n\n\ngenre\nShow genre\n\n\n\n# Importing pandas and matplotlib\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Start coding!\nnetflix_df = pd.read_csv('netflix_data.csv')\nnetflix_subset = netflix_df[netflix_df[\"type\"] == \"Movie\"]\nnetflix_movies = netflix_subset[[\"title\", \"country\", \"genre\", \"release_year\", \"duration\"]]\nnetflix_movies.head()\n\n\n\n\n\n\n\n\ntitle\n\n\ncountry\n\n\ngenre\n\n\nrelease_year\n\n\nduration\n\n\n\n\n\n\n1\n\n\n7:19\n\n\nMexico\n\n\nDramas\n\n\n2016\n\n\n93\n\n\n\n\n2\n\n\n23:59\n\n\nSingapore\n\n\nHorror Movies\n\n\n2011\n\n\n78\n\n\n\n\n3\n\n\n9\n\n\nUnited States\n\n\nAction\n\n\n2009\n\n\n80\n\n\n\n\n4\n\n\n21\n\n\nUnited States\n\n\nDramas\n\n\n2008\n\n\n123\n\n\n\n\n6\n\n\n122\n\n\nEgypt\n\n\nHorror Movies\n\n\n2019\n\n\n95\n\n\n\n\n\nshort_movies = netflix_movies[netflix_movies[\"duration\"]&lt;60]\nshort_movies.head(20)\n\n\n\n\n\n\n\n\ntitle\n\n\ncountry\n\n\ngenre\n\n\nrelease_year\n\n\nduration\n\n\n\n\n\n\n35\n\n\n#Rucker50\n\n\nUnited States\n\n\nDocumentaries\n\n\n2016\n\n\n56\n\n\n\n\n55\n\n\n100 Things to do Before High School\n\n\nUnited States\n\n\nUncategorized\n\n\n2014\n\n\n44\n\n\n\n\n67\n\n\n13TH: A Conversation with Oprah Winfrey & Ava ‚Ä¶\n\n\nNaN\n\n\nUncategorized\n\n\n2017\n\n\n37\n\n\n\n\n101\n\n\n3 Seconds Divorce\n\n\nCanada\n\n\nDocumentaries\n\n\n2018\n\n\n53\n\n\n\n\n146\n\n\nA 3 Minute Hug\n\n\nMexico\n\n\nDocumentaries\n\n\n2019\n\n\n28\n\n\n\n\n162\n\n\nA Christmas Special: Miraculous: Tales of Lady‚Ä¶\n\n\nFrance\n\n\nUncategorized\n\n\n2016\n\n\n22\n\n\n\n\n171\n\n\nA Family Reunion Christmas\n\n\nUnited States\n\n\nUncategorized\n\n\n2019\n\n\n29\n\n\n\n\n177\n\n\nA Go! Go! Cory Carson Christmas\n\n\nUnited States\n\n\nChildren\n\n\n2020\n\n\n22\n\n\n\n\n178\n\n\nA Go! Go! Cory Carson Halloween\n\n\nNaN\n\n\nChildren\n\n\n2020\n\n\n22\n\n\n\n\n179\n\n\nA Go! Go! Cory Carson Summer Camp\n\n\nNaN\n\n\nChildren\n\n\n2020\n\n\n21\n\n\n\n\n181\n\n\nA Grand Night In: The Story of Aardman\n\n\nUnited Kingdom\n\n\nDocumentaries\n\n\n2015\n\n\n59\n\n\n\n\n200\n\n\nA Love Song for Latasha\n\n\nUnited States\n\n\nDocumentaries\n\n\n2020\n\n\n20\n\n\n\n\n220\n\n\nA Russell Peters Christmas\n\n\nCanada\n\n\nStand-Up\n\n\n2011\n\n\n44\n\n\n\n\n233\n\n\nA StoryBots Christmas\n\n\nUnited States\n\n\nChildren\n\n\n2017\n\n\n26\n\n\n\n\n237\n\n\nA Tale of Two Kitchens\n\n\nUnited States\n\n\nDocumentaries\n\n\n2019\n\n\n30\n\n\n\n\n242\n\n\nA Trash Truck Christmas\n\n\nNaN\n\n\nChildren\n\n\n2020\n\n\n28\n\n\n\n\n247\n\n\nA Very Murray Christmas\n\n\nUnited States\n\n\nComedies\n\n\n2015\n\n\n57\n\n\n\n\n285\n\n\nAbominable Christmas\n\n\nUnited States\n\n\nChildren\n\n\n2012\n\n\n44\n\n\n\n\n295\n\n\nAcross Grace Alley\n\n\nUnited States\n\n\nDramas\n\n\n2013\n\n\n24\n\n\n\n\n305\n\n\nAdam Devine: Best Time of Our Lives\n\n\nUnited States\n\n\nStand-Up\n\n\n2019\n\n\n59\n\n\n\n\n\ncolors = []\nfor lab, row in netflix_movies.iterrows():\n    if row['genre'] == \"Children\":\n        colors.append(\"Blue\")\n    elif row['genre'] == \"Documentaries\":\n        colors.append(\"Red\")\n    elif row['genre'] == \"Stand-Up\":\n        colors.append(\"Green\")\n    else:\n        colors.append(\"Black\")\ncolors[:10]\n['Black',\n 'Black',\n 'Black',\n 'Black',\n 'Black',\n 'Black',\n 'Black',\n 'Black',\n 'Black',\n 'Red']\nfig = plt.figure(figsize=(12, 8))\n&lt;Figure size 1200x800 with 0 Axes&gt;\nplt.scatter(netflix_movies['release_year'], netflix_movies['duration'], c=colors)\n&lt;matplotlib.collections.PathCollection at 0x7f49c4f3c430&gt;\n\n\n\npng\n\n\nplt.title(\"Movie Duration by Year of Release\")\nplt.xlabel(\"Release year\")\nplt.ylabel(\"Duration (min)\")\nText(0, 0.5, 'Duration (min)')\n\n\n\npng\n\n\nplt.show()\nanswer = \"no\"\n# Importing pandas and matplotlib\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Read in the Netflix CSV as a DataFrame\nnetflix_df = pd.read_csv(\"netflix_data.csv\")\n\n# Subset the DataFrame for type \"Movie\"\nnetflix_subset = netflix_df[netflix_df[\"type\"] == \"Movie\"]\n\n# Select only the columns of interest\nnetflix_movies = netflix_subset[[\"title\", \"country\", \"genre\", \"release_year\", \"duration\"]]\n\n# Filter for durations shorter than 60 minutes\nshort_movies = netflix_movies[netflix_movies.duration &lt; 60]\n\n# Define an empty list\ncolors = []\n\n# Iterate over rows of netflix_movies\nfor label, row in netflix_movies.iterrows() :\n    if row[\"genre\"] == \"Children\" :\n        colors.append(\"red\")\n    elif row[\"genre\"] == \"Documentaries\" :\n        colors.append(\"blue\")\n    elif row[\"genre\"] == \"Stand-Up\":\n        colors.append(\"green\")\n    else:\n        colors.append(\"black\")\n        \n# Inspect the first 10 values in your list        \ncolors[:10]\n\n# Set the figure style and initalize a new figure\nfig = plt.figure(figsize=(12,8))\n\n# Create a scatter plot of duration versus release_year\nplt.scatter(netflix_movies.release_year, netflix_movies.duration, c=colors)\n\n# Create a title and axis labels\nplt.title(\"Movie Duration by Year of Release\")\nplt.xlabel(\"Release year\")\nplt.ylabel(\"Duration (min)\")\n\n# Show the plot\nplt.show()\n\n# Are we certain that movies are getting shorter?\nanswer = \"no\"\n\n\n\npng",
    "crumbs": [
      "Data Science and Machine Learning",
      "Netflix Movies"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/titanic.html",
    "href": "pages/projects/data_science/posts/titanic.html",
    "title": "Titanic Disaster",
    "section": "",
    "text": "The Kaggle Titanic dataset and ML competition is one that many people are familiar with, and if they‚Äôre like me, it was also their first ML project. I redid this after 3 years to get familiar with my current workflow of using git, notebooks, venvs, etc. Below I included some notes to myself. While it was overkill, I used a Dockerized environment for this project, just to increase my familiarity with containers and the docker toolset.",
    "crumbs": [
      "Data Science and Machine Learning",
      "Titanic Disaster"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/titanic.html#general-overview",
    "href": "pages/projects/data_science/posts/titanic.html#general-overview",
    "title": "Titanic Disaster",
    "section": "",
    "text": "The Kaggle Titanic dataset and ML competition is one that many people are familiar with, and if they‚Äôre like me, it was also their first ML project. I redid this after 3 years to get familiar with my current workflow of using git, notebooks, venvs, etc. Below I included some notes to myself. While it was overkill, I used a Dockerized environment for this project, just to increase my familiarity with containers and the docker toolset.",
    "crumbs": [
      "Data Science and Machine Learning",
      "Titanic Disaster"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/titanic.html#notes-on-the-titanic-model-and-the-process-for-using-the-jupyter-kernel-on-the-ec2-server",
    "href": "pages/projects/data_science/posts/titanic.html#notes-on-the-titanic-model-and-the-process-for-using-the-jupyter-kernel-on-the-ec2-server",
    "title": "Titanic Disaster",
    "section": "Notes on the Titanic model and the process for using the Jupyter Kernel on the EC2 Server",
    "text": "Notes on the Titanic model and the process for using the Jupyter Kernel on the EC2 Server\n\n\nFirst, navigate to Local-Scripts/.AWS/.EC2_Scripts and run the ec2_start.sh script zsh ec2_start.sh\nNext, execute the unix_test_dns.sh script to store the EC2 public DNS in the /etc/hosts file ./unix_test_dns.sh\nUse the ssh -i command to connect to the EC2 server, then run the Jupyter kernel image docker run -p 8888:8888 titanic-env Look into adding a volume mount command here to persist model/file changes in the EC2\nNow that it‚Äôs running. Use a different terminal window (or the VS Code IDE) and test the DNS name. ping unix_test.local Need to make this DNS dynamic\nIn VS Code, open the .ipynb file in the model folder and continue work.\n\nIf you need to reconnect to a kernel, use the Titanic preset.\nVS Code connects to the EC2 IPv4 address, even though the Kernel tells you 127.0.0.1\n\nThe format for connecting to the Public IPv4 is http://IPv4:8888/\nTo pull the file out of the container and store it on the EC2 server\n\ndocker cp 786853360d97:/home/files/titanic_submission.csv files/titanic_submission.csv\ndocker copy instance-id:/path/to/file local/path\n\n\n\n\nIf you need to modify the container\n\nDo so locally, or anywhere, and then push the change to the GitHub repoistory\nThen, pull the changes into the EC2 server\nClear the Docker library/cache, and then rebuild the image from scratch, use the following docker build -t titanic-env -f .config/Dockerfile .\nEnsure this is done from the main project folder and uses those flags\n\nFor Titanic, this is in the admin/Kaggle/Titanic folder in the EC2 instance\n--no-cache ensures it‚Äôs a fresh build (This will take a while, not worth it in the smaller environment. Rebuild with cache)\n-t sets the name of the image\n-f lets you specify the Dockerfile location\n. lets Docker know that your current working directory is where the build context should take place",
    "crumbs": [
      "Data Science and Machine Learning",
      "Titanic Disaster"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/titanic.html#the-code-portion-of-this-notebook",
    "href": "pages/projects/data_science/posts/titanic.html#the-code-portion-of-this-notebook",
    "title": "Titanic Disaster",
    "section": "The Code Portion of this notebook",
    "text": "The Code Portion of this notebook\n\n\n\n\n\n\nCaution\n\n\n\nWhen I most recently completed this competition, I didn‚Äôt do it with the goal in mind of doing a nice write-up. This is really just an amalgamation of the notes I made to myself on how to use/modify the Docker container I ran my model on and the actual notebook + code + notes.\n\n\nimport numpy as np\nimport pandas as pd\nimport duckdb\nimport sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n\n# Initialize a connection and create a persistent database\n# Worth noting that due to the workflow I'm using, the database was/should be created externally, and then built into the Docker container\n# This way, the raw database files are saved locally, but file size won't grow exponentially\ncon = duckdb.connect(\"files/titanic.duckdb\")\n# Using this to DROP and Recreate train_raw, ensuring a fresh process\ncon.sql(\"DROP TABLE train_raw;\")\ncon.sql(\"DROP TABLE test_raw;\")\ncon.sql(\"CREATE TABLE train_raw AS SELECT * FROM 'files/train.csv'\")\ncon.sql(\"CREATE TABLE test_raw AS SELECT * FROM 'files/test.csv'\")\n# Create working tables\n#con.sql(\"DROP TABLE train;\")\n#con.sql(\"DROP TABLE test;\")\ncon.sql(\"CREATE TABLE train AS SELECT * FROM train_raw\")\ncon.sql(\"CREATE TABLE test AS SELECT * FROM test_raw\")\n# Verify the proper tables are loaded\ncon.sql(\"SELECT * FROM duckdb_tables()\")\n# Generate summary statistics\ncon.sql(\"SUMMARIZE train\")\n#con.sql(\"SUMMARIZE test\")\n# Examine Nulls for the Age, Cabin, and Embarked columns (do this for test as well)\ncon.sql(\"SELECT * FROM train WHERE Age IS NULL\") # Seems to make the most sense to use the average age here\ncon.sql(\"SELECT * FROM train WHERE Cabin IS NULL\") # Seems likely Cabins not as strictly recorded for lower class guests, probably unnecessary for model\ncon.sql(\"SELECT * FROM train WHERE Embarked IS NULL\") # This only comprises 2 records and it's unclear if they made it on in the first place, not a high enough percentage of 1st class survivors to consider keeping\n# Update the Age column, replace NULL values with the average Age\ncon.sql(\"\"\"UPDATE train AS train_clean\n        SET Age = (\n            SELECT\n                avg(raw.Age) AS cleanAge\n            FROM train as raw\n            WHERE raw.Age IS NOT NULL\n        )\n        WHERE Age IS NULL\"\"\")\n# Update the Sex column, change the VARCHAR type to BOOLEAN\ncon.sql(\"\"\"ALTER TABLE train ALTER Sex \n        SET DATA TYPE BOOLEAN USING CASE\n            WHEN Sex = 'female' THEN 1 ELSE 0 END\n        \"\"\")\n# Update the Age column in the test dataset\ncon.sql(\"\"\"UPDATE test AS test_clean\n        SET Age = (\n            SELECT\n                avg(raw.Age) AS cleanAge\n            FROM test as raw\n            WHERE raw.Age IS NOT NULL\n        )\n        WHERE Age IS NULL\"\"\")\n# Update the Sex column, change the VARCHAR type to BOOLEAN\ncon.sql(\"\"\"ALTER TABLE test ALTER Sex \n        SET DATA TYPE BOOLEAN USING CASE\n            WHEN Sex = 'female' THEN 1 ELSE 0 END\n        \"\"\")\n# Remove the PassengerId, Name, Cabin, Embarked, Fare, and Ticket columns\ncon.sql(\"ALTER TABLE train DROP PassengerId\") # Has no bearing on the outcome of the model\ncon.sql(\"ALTER TABLE train DROP Name\") # Has to be numeric data\ncon.sql(\"ALTER TABLE train DROP Cabin\")\ncon.sql(\"ALTER TABLE train DROP Embarked\")\ncon.sql(\"ALTER TABLE train DROP Fare\") # Dropping because there are nulls in the test file\ncon.sql(\"ALTER TABLE train DROP Ticket\") # Dropping because of inconsistent values\n\n# Remove the PassengerId, Name, Cabin, Embarked, Fare, and Ticket columns\ncon.sql(\"ALTER TABLE test DROP Name\") # Has to be numeric data\ncon.sql(\"ALTER TABLE test DROP Cabin\")\ncon.sql(\"ALTER TABLE test DROP Embarked\")\ncon.sql(\"ALTER TABLE test DROP Fare\") # Dropping because there are nulls in the test file\ncon.sql(\"ALTER TABLE test DROP Ticket\") # Dropping because of inconsistent values\n\n# Creating dataframes for testing/training, I'll be using sklearn here, which needs both\ntrain = con.sql(\"SELECT * FROM train\").df()\ntest = con.sql(\"SELECT * FROM test\").df()\n# Create features and target\nX = train.drop(\"Survived\", axis = 1).values\ny = train[\"Survived\"].values\n\nX_test = test.drop(\"PassengerId\", axis = 1).values\n# Initialize Regression object and split data\nlogreg = LogisticRegression(penalty = 'l2', tol = np.float64(0.083425), C = np.float64(0.43061224489795924), class_weight = 'balanced')\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.7, random_state = 123)\n# Fit and predict\nlogreg.fit(X_train, y_train)\n\n\n\nLogisticRegression(C=np.float64(0.43061224489795924), class_weight='balanced',\n                   tol=np.float64(0.083425))\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n¬†¬†LogisticRegression?Documentation for LogisticRegressioniFitted\n\nLogisticRegression(C=np.float64(0.43061224489795924), class_weight='balanced',\n                   tol=np.float64(0.083425))\n\n\n\n\n\n# Predict and measure output\ny_pred = logreg.predict(X_val)\ny_pred_probs = logreg.predict_proba(X_val)[:, 1]\nprint(roc_auc_score(y_val, y_pred_probs))\n0.8149262043998886\n# Create Parameter Dictionary for Model Tuning\nkf = KFold(n_splits = 5, shuffle = True, random_state = 123)\nparams = {\n    \"penalty\": [\"l1\", \"l2\"],\n    \"tol\": np.linspace(0.0001, 1.0, 25),\n    \"C\": np.linspace(0.1, 1.0, 50),\n    \"class_weight\": [\"balanced\", {0:0.8, 1:0.2}]\n}\nlogreg_cv = RandomizedSearchCV(logreg, params, cv=kf)\n# Run the parameter search, fit the object, print the output\nlogreg_cv = RandomizedSearchCV(logreg, params, cv=kf)\nlogreg_cv.fit(X_train, y_train)\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\nprint(\"Tuned Logistic Regression Best Accuracy Score: {}\".format(logreg_cv.best_score_))\n# Apply the model to the test set\npredictions = logreg.predict(X_test)\n\nsubmission = pd.DataFrame({\n    'PassengerId': test['PassengerId'],\n    'Survived': predictions\n})\n\nsubmission.head()\n\n\n\n\n\n\n\n\nPassengerId\n\n\nSurvived\n\n\n\n\n\n\n0\n\n\n892\n\n\n0\n\n\n\n\n1\n\n\n893\n\n\n1\n\n\n\n\n2\n\n\n894\n\n\n0\n\n\n\n\n3\n\n\n895\n\n\n0\n\n\n\n\n4\n\n\n896\n\n\n1\n\n\n\n\n\n# Write the file to .csv and submit\ncon.sql(\"SELECT * FROM submission\").write_csv(\"files/titanic_submission.csv\")",
    "crumbs": [
      "Data Science and Machine Learning",
      "Titanic Disaster"
    ]
  },
  {
    "objectID": "pages/blogs/posts/first_blog.html",
    "href": "pages/blogs/posts/first_blog.html",
    "title": "1. Experimenting with GitHub Pages and WhiteWind",
    "section": "",
    "text": "This is the first part of a blog post I originally wrote over a month ago and posted to WhiteWind.\nWhiteWind is a blogging application built on Bluesky‚Äôs foundational, decentralied network. Half of that post is now the beginning of the NFL Big Data Bowl 2025 project write up.\nEventually, I‚Äôd like to find a way to write a post on WhiteWind and have it automatically update my personal site. That way I can do quick, shower thoughts style blog posts there, without having to do the longer Quarto workflow.",
    "crumbs": [
      "1. Experimenting with GitHub Pages and WhiteWind"
    ]
  },
  {
    "objectID": "pages/blogs/posts/first_blog.html#original-post",
    "href": "pages/blogs/posts/first_blog.html#original-post",
    "title": "1. Experimenting with GitHub Pages and WhiteWind",
    "section": "Original Post",
    "text": "Original Post\nIn March of 2024, I took a break from social media (Twitter/Instagram/TikTok/Etc.) because the polarizing algorithms and toxicity were driving me nuts. I took to LinkedIn, in search of a community of professional data nerds, that develop and learn in their freetime. Luckily, I found some great people on there, but my feed was quickly inundated with LinkedInfluencers reposting the same content whether recycling their own, other‚Äôs, or generating posts with AI. Then, in October, I heard that data people were all jumping over to Bluesky, so I thought I‚Äôd give it a shot. Needless to say, I love the data and developer community here. Furthermore, the fundamental design and decentralization of the platform make this that much cooler. Then, I learned about WhiteWind and some other integrations, and decided this is a great way to post updates and keep myself accountable.\nA little about me, I‚Äôm a Tulane Unviersity graduate (BSM ‚Äô20, MS ‚Äô21) and currently a Data (Analytics) Engineer at GM. In my current role I wear many hats! In my 3.5 years at General Motors I‚Äôve gained experience desinging database architecture, building ETL pipelines, being an IT admin, owning products, managing projects, automating tests, and providing training/mentorship for Power BI/Databricks; however, most of my experience is in BI reporting and legacy application upgrades.\nSo, this past April I decided I wanted to really learn the ins and outs of modern data engineering and data science. So, I bought a MacBook Air and started coding! From my Masters degree at Tulane, I had experience with R, Python, SQL for coding, as well as the statistical knwoledge needed for machine learning. Instead of those areas, I began with zsh/bash scripting (using the Terminal to do everything I could), then I learned the basics of git for version control, then I jumped into dbt for data modeling, and finally Docker so I could understand containers. Following that, I began Harvard‚Äôs CS50p course in Python programming. I wanted to understand Python, the programming language, because I had learned Python using Jupyter notebooks instead of scripts, packages, testing, etc.\nIn the few month or so since then, I‚Äôve worked on a few public projects which you can view on my GitHub. These projects are pretty varied, and narrow in scope.\n\nDataCamp projects based on simple ETL, analytics, or programming.\nMy CS50p lecture notes and problem set submissions.\nA simple Introduction to DuckDB using the NFL Big Data Bowl CSV files.\nA private repository with bash scripts and configuration files for starting up my AWS EC2 instance, connecting via ssh, Docker containers, quarto, and the GitHub CLI.",
    "crumbs": [
      "1. Experimenting with GitHub Pages and WhiteWind"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chris Kornaros",
    "section": "",
    "text": "This site is the personal portfolio and homepage for Chris Kornaros. I‚Äôm a Tulane Graduate and professional Data Engineer. On here, you‚Äôll find projects and code samples that I can share publicly, guides for various tools or workflows, journal or blog posts, and any independent research or professional updates (including my resume).\nThis website is a work in progress, so things are going to move around and break. Additionally, there is a lot that I have to still add: past guides, projects, etc. Follow me on social media (links above) to stay up to date with what I‚Äôm working on. To learn more about my background visit About, for guides on various open source tools visit Guides, to see my current and past (public) work visit Projects, and to see any blog posts (available on WhiteWind) visit Blogs."
  },
  {
    "objectID": "pages/blogs/blogs.html",
    "href": "pages/blogs/blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "A landing page for various blogs, journals, or random thoughts. Some of these will be focused on specific tools or technology, others will be random thoughts or research notes.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 14, 2024\n\n\n1. Experimenting with GitHub Pages and WhiteWind\n\n\nChris Kornaros\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Blogs"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/gym_market.html",
    "href": "pages/projects/data_science/posts/gym_market.html",
    "title": "Gym Market Analysis",
    "section": "",
    "text": "gym\n\n\nYou are a product manager for a fitness studio and are interested in understanding the current demand for digital fitness classes. You plan to conduct a market analysis in Python to gauge demand and identify potential areas for growth of digital products and services.\n\nThe Data\nYou are provided with a number of CSV files in the ‚ÄúFiles/data‚Äù folder, which offer international and national-level data on Google Trends keyword searches related to fitness and related products.\n\n\nworkout.csv\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\n'month'\nMonth when the data was measured.\n\n\n'workout_worldwide'\nIndex representing the popularity of the keyword ‚Äòworkout‚Äô, on a scale of 0 to 100.\n\n\n\n\n\nthree_keywords.csv\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\n'month'\nMonth when the data was measured.\n\n\n'home_workout_worldwide'\nIndex representing the popularity of the keyword ‚Äòhome workout‚Äô, on a scale of 0 to 100.\n\n\n'gym_workout_worldwide'\nIndex representing the popularity of the keyword ‚Äògym workout‚Äô, on a scale of 0 to 100.\n\n\n'home_gym_worldwide'\nIndex representing the popularity of the keyword ‚Äòhome gym‚Äô, on a scale of 0 to 100.\n\n\n\n\n\nworkout_geo.csv\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\n'country'\nCountry where the data was measured.\n\n\n'workout_2018_2023'\nIndex representing the popularity of the keyword ‚Äòworkout‚Äô during the 5 year period.\n\n\n\n\n\nthree_keywords_geo.csv\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\n'country'\nCountry where the data was measured.\n\n\n'home_workout_2018_2023'\nIndex representing the popularity of the keyword ‚Äòhome workout‚Äô during the 5 year period.\n\n\n'gym_workout_2018_2023'\nIndex representing the popularity of the keyword ‚Äògym workout‚Äô during the 5 year period.\n\n\n'home_gym_2018_2023'\nIndex representing the popularity of the keyword ‚Äòhome gym‚Äô during the 5 year period.\n\n\n\n# Import the necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# Start coding here\nimport pandas as pd\n\nworkout = pd.read_csv('data/workout.csv')\nthree_kw = pd.read_csv('data/three_keywords.csv')\nworkout_geo = pd.read_csv('data/workout_geo.csv')\nkw_geo = pd.read_csv('data/three_keywords_geo.csv')\nworkout.head()\n\n\n\n\n\n\n\n\nmonth\n\n\nworkout_worldwide\n\n\n\n\n\n\n0\n\n\n2018-03\n\n\n59\n\n\n\n\n1\n\n\n2018-04\n\n\n61\n\n\n\n\n2\n\n\n2018-05\n\n\n57\n\n\n\n\n3\n\n\n2018-06\n\n\n56\n\n\n\n\n4\n\n\n2018-07\n\n\n51\n\n\n\n\n\npeak = workout.loc[workout['workout_worldwide'].idxmax()]\nyear_str = peak.str.split('-')[0][0]\nyear_str\n'2020'\nworkout.dtypes\nmonth                object\nworkout_worldwide     int64\ndtype: object\ncovid = workout.loc[(workout['month'] &gt; '2019-12') & (workout['month'] &lt;= '2022-12')]\npost_covid = workout.loc[workout['month'] &gt; '2022-12']\npeak_covid = three_kw.loc[(workout['month'] &gt; '2019-12') & (workout['month'] &lt;= '2022-12')][['home_workout_worldwide', 'gym_workout_worldwide', 'home_gym_worldwide']].max().idxmax()\ncurrent = three_kw.loc[workout['month'] &gt; '2022-12'][['home_workout_worldwide', 'gym_workout_worldwide', 'home_gym_worldwide']].max().idxmax()\npeak_covid\ncurrent\n'gym_workout_worldwide'\ntop_country = workout_geo.loc[workout_geo['workout_2018_2023'].idxmax()]['country']\ntop_country\n'United States'\nkw_geo1 = kw_geo.loc[(kw_geo['Country']=='Philippines') | (kw_geo['Country']=='Malaysia')]\nkw_geo1\n\n\n\n\n\n\n\n\nCountry\n\n\nhome_workout_2018_2023\n\n\ngym_workout_2018_2023\n\n\nhome_gym_2018_2023\n\n\n\n\n\n\n23\n\n\nPhilippines\n\n\n52.0\n\n\n38.0\n\n\n10.0\n\n\n\n\n61\n\n\nMalaysia\n\n\n47.0\n\n\n38.0\n\n\n15.0\n\n\n\n\n\nhome_workout_geo = kw_geo1.loc[kw_geo1['home_workout_2018_2023'] == kw_geo1['home_workout_2018_2023'].max(), 'Country'].values[0]\nhome_workout_geo\n'Philippines'",
    "crumbs": [
      "Data Science and Machine Learning",
      "Gym Market Analysis"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html",
    "title": "NFL Big Data Bowl 2025",
    "section": "",
    "text": "I will add to this later. Currently, this is a rough combination of my early notebooks.",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#introduction",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#introduction",
    "title": "NFL Big Data Bowl 2025",
    "section": "",
    "text": "I will add to this later. Currently, this is a rough combination of my early notebooks.",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#ongoing-thoughts",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#ongoing-thoughts",
    "title": "NFL Big Data Bowl 2025",
    "section": "Ongoing Thoughts",
    "text": "Ongoing Thoughts\nThis is the first time I‚Äôm adding to a unified document, it‚Äôs December 13th, or about 1 month into my project. As of now, Random Forest definitely seems like the best path forward; however, the intial version certainly overfit. I believe the model overfit because some of the plays columns are the pre/post snap home/away team win probability values. In my next iteration, I‚Äôm going to remove those values, and in the future I might even try to recreate them. That being said, there‚Äôs a little under one month to go, so I‚Äôm going to focus on putting together some kind of deliverable/submission, before I go off the deep end. That said, this page and the website in general are going to be sloppy as I figure things out and slowly improve the organization and UI.",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#exploratory-data-analysis-and-initial-thoughts",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#exploratory-data-analysis-and-initial-thoughts",
    "title": "NFL Big Data Bowl 2025",
    "section": "Exploratory Data Analysis and Initial Thoughts",
    "text": "Exploratory Data Analysis and Initial Thoughts\n\n\n\n\n\n\nNote\n\n\n\nThis was written on November 26th, 2024. It was later added to this site on December 13th, 2024. This is a general write up on the project, you can see the full notebooks below. The full repo is available here.\n\n\nCurrently, I‚Äôve made solid progress with my initial exploratory data analysis and project configuration. Here are some quick notes about the setup of my project environment (from IDE to tools/versions). - Using VS Code with the Jupyter, Jinja, YAML, Quarto (for notes/project submissions), and dbt extensions. - DuckDB is my primary database tool (for now), with dbt for the data modeling - Then, I‚Äôm using Python and Jupyter Notebooks for the analysis/ML component\nThe reason I may switch to PostgreSQL for the primary Database is to just gain experience with DuckDB as a DEV environement and Postgres for PROD. Realistically, however, for the scope of this project DuckDB accomplishes everything I need it to.\nFor the forseeable future, the only side project I‚Äôll be working on is this, so my next few posts will only look at the project progress and my thoughts about the Big Data Bowl, feel free to checkout the GitHub repository where I‚Äôm saving my work.\nSome notes about my current project progress: - The project folder has a few subdirectories, including nfl_dbt which is the dbt project folder - The raw data came in the form of 13 CSVs from Kaggle. 4 of which are 50mb or less, 9 of which are ~1gb. - I‚Äôm using Databricks‚Äô ‚ÄúMedallion Architecture‚Äù to guide my data modeling workflow. - I built the initial dbt models, using DuckDB as the DEV target (enabling 4 threads) and loaded the ‚Äúbronze‚Äù schema which contains the 13 raw tables - I aggregated the data into the ‚Äúsilver‚Äù schema, which contains an aggregated play data table - I further aggregated the data into the ‚Äúgold‚Äù schema, which provides basic analytic tables - Currently, I completed an initial analysis using an EDA notebook where I looked at using a LinearRegression and KNN to compare pre-snap play data with play outcomes. - I settled on a KNN model, but I‚Äôm only seeing about a 61.1% accuracy rate (confusion matrix and explanation below).\nSo, I‚Äôm at a bit of a crossroads, with a few ways forward. It may be simpler (for the initial project/submission) to build a linear regression model that takes pre-snap play data as features, and then looks at yards gained (or loss) for the output. Conversely, if I stick with the KNN model I‚Äôll need to make some changes. The majority of the outputs are either Gain or Completed, which refer to a positive rushing play and a completed pass, respectively. The issue here, the model overwhelmingly predicts those values, but fails to accurately predict things like Touchdowns, Sacks, or Interceptions.\nSo, I may need to limit possible play outcomes, or at least combine some categories (i.e.¬†Turnover for Fumble + Interception). Or, add some more presnap data, such as down and distance (I currently only use starting yard line, along with categorical data). If you made it this far, thank you! Below is the confusion matrix output from my current KNN model. I‚Äôll add some hashtags at the end as an experiment too, because I‚Äôm not sure if that will help with post discoverability and/or integrate with Bluesky feeds.",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#knn-classifier-notebook-first-model",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#knn-classifier-notebook-first-model",
    "title": "NFL Big Data Bowl 2025",
    "section": "KNN Classifier Notebook (First Model)",
    "text": "KNN Classifier Notebook (First Model)\n# Import dependencies\nimport duckdb\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n# Open the connection to the persistent database\ncon = duckdb.connect(\".config/nfl.duckdb\")\ncon.close()\n# Create the initial dataframe object with DuckDB\ndf = con.sql(\"\"\"\n    SELECT *\n    FROM gold.plays_numeric             \n\"\"\").df()\n\ndf.head()\n\n\n\n\n\n\n\n\ngameId\n\n\nplayId\n\n\npossessionTeam\n\n\nyardlineNumber\n\n\noffenseFormation\n\n\nreceiverAlignment\n\n\nplayType\n\n\ndefensiveFormation\n\n\npff_manZone\n\n\nyardsGained\n\n\nplayOutcome\n\n\n\n\n\n\n0\n\n\n2022102302\n\n\n2655\n\n\nCIN\n\n\n21\n\n\n3\n\n\n8\n\n\n2\n\n\n6\n\n\n2\n\n\n9\n\n\n3\n\n\n\n\n1\n\n\n2022091809\n\n\n3698\n\n\nCIN\n\n\n8\n\n\n3\n\n\n8\n\n\n2\n\n\n13\n\n\n2\n\n\n4\n\n\n3\n\n\n\n\n2\n\n\n2022103004\n\n\n3146\n\n\nHOU\n\n\n20\n\n\n6\n\n\n5\n\n\n2\n\n\n13\n\n\n2\n\n\n6\n\n\n3\n\n\n\n\n3\n\n\n2022110610\n\n\n348\n\n\nKC\n\n\n23\n\n\n6\n\n\n5\n\n\n2\n\n\n13\n\n\n2\n\n\n4\n\n\n3\n\n\n\n\n4\n\n\n2022102700\n\n\n2799\n\n\nBAL\n\n\n27\n\n\n4\n\n\n7\n\n\n1\n\n\n3\n\n\n1\n\n\n-1\n\n\n2\n\n\n\n\n\n# Split the table into features and target\nX = con.sql(\"\"\"\n    SELECT yardlineNumber, offenseFormation, receiverAlignment, playType, defensiveFormation, pff_manZone\n    FROM gold.plays_numeric\n\"\"\").df()\n\ny = np.array(con.sql(\"\"\"\n    SELECT playOutcome\n    FROM gold.plays_numeric\n\"\"\").df()).ravel()\n\nprint(X.shape, y.shape)\n(16124, 6) (16124,)\n# Instantiate the model and split the datasets into training/testing\nknn = KNeighborsClassifier(n_neighbors=7)\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.7, random_state=123)\n# Fit the model\nknn.fit(X_train, y_train)\n\n\n\nKNeighborsClassifier(n_neighbors=7)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n¬†¬†KNeighborsClassifier?Documentation for KNeighborsClassifieriFitted\n\nKNeighborsClassifier(n_neighbors=7)\n\n\n\n\n\n# Basic KNN Performance Metrics\ny_pred = knn.predict(X_val)\n\nprint(knn.score(X_val, y_val))\n0.6114096734187681\n# Datacamp Model performance Loop\n# Create neighbors\nneighbors = np.arange(1, 13)\ntrain_accuracies = {}\ntest_accuracies = {}\n\nfor neighbor in neighbors:\n  \n    # Set up a KNN Classifier\n    knn = KNeighborsClassifier(n_neighbors=neighbor)\n  \n    #¬†Fit the model\n    knn.fit(X_train, y_train)\n  \n    # Compute accuracy\n    train_accuracies[neighbor] = knn.score(X_train, y_train)\n    test_accuracies[neighbor] = knn.score(X_val, y_val)\nprint(neighbors, '\\n', train_accuracies, '\\n', test_accuracies)\n# Visualize model accuracy with various neighbors\n# Add a title\nplt.title(\"KNN: Varying Number of Neighbors\")\n\n#¬†Plot training accuracies\nplt.plot(neighbors, train_accuracies.values(), label=\"Training Accuracy\")\n\n# Plot test accuracies\nplt.plot(neighbors, test_accuracies.values(), label=\"Testing Accuracy\")\n\nplt.legend()\nplt.xlabel(\"Number of Neighbors\")\nplt.ylabel(\"Accuracy\")\n\n# Display the plot\nplt.show()\n# Map the original target variables to the KNN outputs\nplay_outcome_map = con.sql(\"\"\"\n    SELECT\n    CASE\n        WHEN playOutcome = 1 THEN 'Gain'\n        WHEN playOutcome = 2 THEN 'Loss'\n        WHEN playOutcome = 3 THEN 'Completed'\n        WHEN playOutcome = 4 THEN 'Incomplete'\n        WHEN playOutcome = 5 THEN 'Scrambled'\n        WHEN playOutcome = 6 THEN 'Touchdown'\n        WHEN playOutcome = 7 THEN 'Intercepted'\n        WHEN playOutcome = 8 THEN 'Fumbled'\n        WHEN playOutcome = 9 THEN 'Sacked'\n        WHEN playOutcome = 0 THEN 'Penalty'\n        ELSE 'Unknown'  -- Optional, in case there are values not matching any condition\n    END AS playOutcome\nFROM gold.plays_numeric\n\"\"\").df()['playOutcome'].tolist()\n\nplay_outcome_map = np.unique(play_outcome_map).tolist()\n# Create a dictionary to map playOutcome values to corresponding labels\nplay_outcome_dict = {i: play_outcome_map[i] for i in range(len(play_outcome_map))}\n\n# Generate a colormap for the string labels (use 'viridis' colormap)\ncolors = plt.cm.viridis(np.linspace(0, 1, len(play_outcome_map)))\nplay_colors = dict(zip(range(len(play_outcome_map)), colors))\n\n# Create legend patches for each class label\nlegend_patches = [mpatches.Patch(color=play_colors[i], label=play_outcome_map[i]) for i in range(len(play_outcome_map))]\n\n# Assuming `y_pred` is a list of predictions, map numeric predictions to string labels\npred_labels = [play_outcome_dict[val] for val in y_pred]\n# Attempting to conduct sensitivity analysis for feature importance\nfor feature in range(6):\n    plt.figure(figsize=(10, 6))\n    plt.scatter(X_val.iloc[:, feature], y_pred, c=[play_colors[val] for val in y_pred], cmap='viridis', edgecolor='k')\n    plt.xlabel(f\"Feature {feature + 1}\")\n    plt.ylabel(\"Predicted Class\")\n    plt.yticks(range(len(play_outcome_map)), play_outcome_map)\n    plt.title(f\"Predictions by Feature {feature + 1}\")\n    plt.legend(handles = legend_patches, title=\"Actual Class\", bbox_to_anchor=(1.05, 1), loc = 'upper left')\n    plt.tight_layout\n    plt.show()\n# Your play_outcome_dict with correct mapping\nplay_outcome_dict = {\n    1: 'Gain',\n    2: 'Loss',\n    3: 'Completed',\n    4: 'Incomplete',\n    5: 'Scrambled',\n    6: 'Touchdown',\n    7: 'Intercepted',\n    8: 'Fumbled',\n    9: 'Sacked',\n    0: 'Penalty'\n}\n\n# Map the y_pred values to the corresponding labels\npred_labels = [play_outcome_dict[val] for val in y_pred]\n\n# Define the colormap based on the labels\nplay_colors = plt.cm.viridis(np.linspace(0, 1, len(play_outcome_dict)))\n\n# Combine your features (X_val) and the predictions (y_pred) into a single DataFrame\ndf_features = X_val.copy()\ndf_features['Predicted Class'] = [play_outcome_dict[key] for key in y_pred]\n\n# Create a pairplot to visualize pairwise relationships between all features\nsns.pairplot(df_features, hue='Predicted Class', palette=dict(zip(play_outcome_dict.values(), play_colors)), markers='o')\n\n# Customize the plot\nplt.suptitle('Pairplot of Features Colored by Predicted Class', y=1.02)\nplt.legend(handles = legend_patches, title=\"Actual Class\", bbox_to_anchor=(1.05, 1), loc = 'upper left')\nplt.tight_layout()\nplt.show()\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Assuming y_true contains the true labels and y_pred contains the predicted labels\n# Map numerical values to their respective class labels\ny_true_labels = [play_outcome_dict[val] for val in y_val]  # Replace y_true with your actual true labels\ny_pred_labels = [play_outcome_dict[val] for val in y_pred]\n\n# Generate the confusion matrix\ncm = confusion_matrix(y_true_labels, y_pred_labels, labels=list(play_outcome_dict.values()))\n\n# Visualize the confusion matrix\nfig, ax = plt.subplots(figsize=(10, 8))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(play_outcome_dict.values()))\ndisp.plot(cmap='viridis', ax=ax, xticks_rotation=45)\n\n# Customize the plot\nplt.title(\"Confusion Matrix of KNN Model\")\nplt.show()\n\n\n\nKNN Classifier Confusion Matrix",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#linear-regression-notebook-second-model",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#linear-regression-notebook-second-model",
    "title": "NFL Big Data Bowl 2025",
    "section": "Linear Regression Notebook (Second Model)",
    "text": "Linear Regression Notebook (Second Model)\nimport duckdb\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n# Open the DuckDB connection, to the persistent database\ncon = duckdb.connect(\".config/nfl.duckdb\")\ncon.close()\n# Test converting the play outcomes to just yards gained or lost\ncon.sql(\"\"\"\n    SELECT *\n    FROM gold.plays_numeric   \n\"\"\")\n# Can still utilize plays_numeric, just won't use the categorical outcomes as the target\nX = con.sql(\"\"\"\n    SELECT yardlineNumber, offenseFormation, receiverAlignment, playType, defensiveFormation, pff_manZone\n    FROM gold.plays_numeric   \n\"\"\").df()\ny = con.sql(\"\"\"\n    SELECT yardsGained\n    FROM gold.plays_numeric   \n\"\"\").df()\n# Train test split\n# May need to come back and apply a Standard Scaler later\n\nlinreg = LinearRegression()\nscaler = StandardScaler()\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.7, random_state = 123)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\n# Fit the model\nlinreg.fit(X_train_scaled, y_train)\n\n\n\nLinearRegression()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n¬†¬†LinearRegression?Documentation for LinearRegressioniFitted\n\nLinearRegression()\n\n\n\n\n\n# Begin testing and scoring\ny_pred = linreg.predict(X_val_scaled)\n\nmse = mean_squared_error(y_val, y_pred)\nr2 = r2_score(y_val, y_pred)\n\nprint(f\"MSE: {mse}\")\nprint(f\"R2 Score: {r2}\")\nMSE: 80.63310622289697\nR2 Score: 0.02277208083008464\nplt.scatter(y_val, y_pred)\nplt.xlabel(\"Actual Values\")\nplt.ylabel(\"Predicted Values\")\nplt.title(\"Linear Regression: Actual vs Predicted\")\nplt.show()\n\ncoefficients = linreg.coef_\nprint(f\"Coefficients: {coefficients}\")\n\n\n\nLinear Regression Scatter Plot\n\n\nCoefficients: [[ 0.05041481  0.32550574  0.04088819  1.77381568 -0.0198335  -0.16104852]]\nridge = Ridge(alpha=1.0)\nridge.fit(X_train_scaled, y_train)\ny_pred_ridge = ridge.predict(X_val_scaled)\nprint(f\"Ridge MSE: {mean_squared_error(y_val, y_pred_ridge)}\")\nRidge MSE: 80.63311884052399",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#random-forest-notebook-third-model",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#random-forest-notebook-third-model",
    "title": "NFL Big Data Bowl 2025",
    "section": "Random Forest Notebook (Third Model)",
    "text": "Random Forest Notebook (Third Model)\nimport duckdb\nimport tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n# Create the database connection\ncon = duckdb.connect(\"nfl.duckdb\")\n#con.close()\n# Creating dataframes with DuckDB, plays and player_play both have 50 columns, more ideal for a broad random forest\nX = con.sql(\"\"\"\n    SELECT quarter, down, yardsToGo, yardlineNumber, preSnapHomeScore, preSnapVisitorScore,\n    playNullifiedByPenalty, absoluteYardlineNumber, preSnapHomeTeamWinProbability, preSnapVisitorTeamWinProbability, expectedPoints,\n    passResult_complete, passResult_incomplete, passResult_sack, passResult_interception, passResult_scramble, passLength, targetX, targetY,\n    playAction, passTippedAtLine, unblockedPressure, qbSpike, qbKneel, qbSneak, penaltyYards, prePenaltyYardsGained, \n    homeTeamWinProbabilityAdded, visitorTeamWinProbilityAdded, expectedPointsAdded, isDropback, timeToThrow, timeInTackleBox, timeToSack,\n    dropbackDistance, pff_runPassOption, playClockAtSnap, pff_manZone, pff_runConceptPrimary_num, pff_passCoverage_num, pff_runConceptSecondary_num\nFROM silver.plays_rf\n\"\"\").df()\ny = np.array(con.sql(\"\"\"\n    SELECT yardsGained\n    FROM silver.plays_rf\n\"\"\").df()).ravel()\n# Having issues with NA values, the below code does a simple count using pandas, will then go back and change the query\n# As of writing this, the issue is solved; however, the dbt model for this is far from efficient\nna_counts = (X == 'NA').sum()\n\n# Optionally, filter only columns with 'NA' values for easier review\nna_counts_filtered = na_counts[na_counts &gt; 0]\nprint(na_counts_filtered, \"\\n\", X.shape, \"\\n\", y.shape) # playClockAtSnap has only 1 NA value, will just drop that row\nSeries([], dtype: int64) \n (16124, 41) \n (16124,)\n# Instantiate the model and split the data\nrf = RandomForestRegressor(warm_start=True)\n\nselector = RFE(rf, n_features_to_select=10, step=1)\nX_selected = selector.fit_transform(X, y)\n# Begin Interpretation, first with feature importance\nselected_features = X.columns[selector.support_]\nprint(selected_features)\nIndex(['yardlineNumber', 'absoluteYardlineNumber',\n       'preSnapHomeTeamWinProbability', 'expectedPoints',\n       'passResult_scramble', 'penaltyYards', 'prePenaltyYardsGained',\n       'homeTeamWinProbabilityAdded', 'visitorTeamWinProbilityAdded',\n       'expectedPointsAdded'],\n      dtype='object')\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n\n# Train the model\nrf.fit(X_train, y_train)\n\n# Make predictions\ny_pred = rf.predict(X_test)\n\n# Calculate scores\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"R^2 Score: {r2}\")\nMean Squared Error: 1.7769936744186046\nR^2 Score: 0.9766614590863065\n# Continue with the GridSearch\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n}\n\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=4)\ngrid_search.fit(X_train, y_train)\nbest_rf = grid_search.best_estimator_\n\n# Wrap a progress bar for longer Grid Searches\n\"\"\"with tqdm(total=len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf']), desc=\"GridSearch Progress\") as pbar:\n    def callback(*args, **kwargs):\n        pbar.update(1)\n\n    # Add the callback to the grid search\n    grid_search.fit(X, y, callback=callback)\"\"\"\n\nprint(grid_search.best_params_)\n{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n# Continue with the Cross Validation Score\ncv_scores = cross_val_score(rf, X_selected, y, cv=5, scoring='neg_mean_squared_error')\nprint(f\"Cross-validated MSE: {-cv_scores.mean()}\")\nCross-validated MSE: 1.9303851017196607",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/data_science.html",
    "href": "pages/projects/data_science/data_science.html",
    "title": "Data Science and Machine Learning Projects",
    "section": "",
    "text": "Landing page for all my project posts related to Data Science and Machine Learning.",
    "crumbs": [
      "Data Science and Machine Learning"
    ]
  },
  {
    "objectID": "pages/projects/data_science/data_science.html#projects",
    "href": "pages/projects/data_science/data_science.html#projects",
    "title": "Data Science and Machine Learning Projects",
    "section": "Projects",
    "text": "Projects",
    "crumbs": [
      "Data Science and Machine Learning"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/oss_data_arch.html",
    "href": "pages/projects/data_engineering/posts/oss_data_arch.html",
    "title": "Open Source Data and Analytics Architecture",
    "section": "",
    "text": "I will update this when I begin the project. The goal here is to explore and create a tech stack to support modern data and analytical workloads, using entirely open source software. Ideally, I‚Äôll be able to scale it to terabytes and then share that template and the guide as a public resource.\nCurrently, I‚Äôm thinking of the following tools, as part of a non-exhaustive list of the stack:\n\n\nOS/Environment: zsh/bash\nProject and Package Management: uv\nCollaboration and Source Control: Github\nDocumentation: Quarto\nData Modeling: dbt\nContainerization: Docker\nContainer Orchestration: Kubernetes\nOLTP Database: PostgreSQL\nOLAP Database: DuckDB\nBatch Ingestion: Python\nETL: dbt\nTesting: pytest\nData Quality: Great Expectations\nMetadata: Unity Catalog\nETL Orchestration: Airflow and/or Dagster\nStreaming Ingestion: Kafka\n\n\nGeneral workflow I‚Äôm envisioning:\n\nInitialize project with uv, add basic dependencies for the environment\nCreate the repo with the GitHub CLI\nSet the remote as the upstream and do the initial commit\nInitialize the quarto and dbt projects as subdirectories of the main, uv project directory\nCreate the postgres container with docker, use this to initialize the postgres database (Prod)\nIn your uv envionrment, initialize the duckdb (Dev/Test) persistent database\n\nSimpler to work quickly with duckdb, postgres has more configurations/overhead, but is better for long term persistent\n\nUse python and duckdb to ingest the initial batch of raw data\nUse dbt to define the data model, pytest to define the basic tests, and great expectations to define data quality\nInitialize the unity catalog instance, add the connection information (Dev/Test/Prod)\nGenerate metadata and lineage\nStart scheduling and orchestrating jobs\nPotentially scale system up to handle stremaing data",
    "crumbs": [
      "Data Engineering and Architecture",
      "Open Source Data and Analytics Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/oss_data_arch.html#introduction",
    "href": "pages/projects/data_engineering/posts/oss_data_arch.html#introduction",
    "title": "Open Source Data and Analytics Architecture",
    "section": "",
    "text": "I will update this when I begin the project. The goal here is to explore and create a tech stack to support modern data and analytical workloads, using entirely open source software. Ideally, I‚Äôll be able to scale it to terabytes and then share that template and the guide as a public resource.\nCurrently, I‚Äôm thinking of the following tools, as part of a non-exhaustive list of the stack:\n\n\nOS/Environment: zsh/bash\nProject and Package Management: uv\nCollaboration and Source Control: Github\nDocumentation: Quarto\nData Modeling: dbt\nContainerization: Docker\nContainer Orchestration: Kubernetes\nOLTP Database: PostgreSQL\nOLAP Database: DuckDB\nBatch Ingestion: Python\nETL: dbt\nTesting: pytest\nData Quality: Great Expectations\nMetadata: Unity Catalog\nETL Orchestration: Airflow and/or Dagster\nStreaming Ingestion: Kafka\n\n\nGeneral workflow I‚Äôm envisioning:\n\nInitialize project with uv, add basic dependencies for the environment\nCreate the repo with the GitHub CLI\nSet the remote as the upstream and do the initial commit\nInitialize the quarto and dbt projects as subdirectories of the main, uv project directory\nCreate the postgres container with docker, use this to initialize the postgres database (Prod)\nIn your uv envionrment, initialize the duckdb (Dev/Test) persistent database\n\nSimpler to work quickly with duckdb, postgres has more configurations/overhead, but is better for long term persistent\n\nUse python and duckdb to ingest the initial batch of raw data\nUse dbt to define the data model, pytest to define the basic tests, and great expectations to define data quality\nInitialize the unity catalog instance, add the connection information (Dev/Test/Prod)\nGenerate metadata and lineage\nStart scheduling and orchestrating jobs\nPotentially scale system up to handle stremaing data",
    "crumbs": [
      "Data Engineering and Architecture",
      "Open Source Data and Analytics Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/basic_oss.html",
    "href": "pages/projects/data_engineering/posts/basic_oss.html",
    "title": "Basic OSS Architecture",
    "section": "",
    "text": "The purpose of this project is to showcase the power of open source tools when designing a data and analytics system. I will be walking through my workflow step by step, and including both images, code, and notes.",
    "crumbs": [
      "Data Engineering and Architecture",
      "Basic Open Source Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/basic_oss.html#project-initialization",
    "href": "pages/projects/data_engineering/posts/basic_oss.html#project-initialization",
    "title": "Basic OSS Architecture",
    "section": "Project Initialization",
    "text": "Project Initialization\n\nFirst Steps\nLet‚Äôs start from scratch, a blank VS Code IDE. We‚Äôll do everything from the command line, so make sure to open that up (Ctrl-`).\n\n\n\nBlank IDE\n\n\nFirst, I begin in my home directory. Then, I change to my Documents directory, which I use for all of my projects. This is where I‚Äôll begin creating the project directory and initializing the subsequent tools. As you‚Äôll see below, I first initialize the uv repository and change into it. Then, I create the repo on GitHub (because I like generating the license then), pull (my global is set to merge), commit, and make the initial push. Then, I will initialize the Quarto project, to begin documentation as I work.\n\n\n\nProject and Repo Initialization\n\n\n You can see here that I have both jupyter and dbt already installed. That‚Äôs because uv installs tools system wide, because these are typically used from the CLI. That being said, some CLI tools (like Quarto and DuckDB) in my experience don‚Äôt work with uv because it doesn‚Äôt install their executables.\n\n\n\nGit Remote Add and Pull\n\n\n\n\n\nFirst Commit\n\n\n\n\nMoving On\nNow, it‚Äôs time to setup some extra functionality in the project. I‚Äôm going to be using Quarto for documentation, so I‚Äôll run quarto create project. To learn more about Quarto and configuring your documentation in projects, checkout my guide. That being said, if you are ever taking screenshots of your work and want to quickly move them into your images folder, you can do so from the CLI.\n\n\n\nQuarto Project Initialization\n\n\n\n\n\nMoving Images from the CLI\n\n\nNow that you‚Äôve done that, it‚Äôs time to start adding dependencies. As a heads up, don‚Äôt be surprised if you don‚Äôt see the uv.lock or the .venv objects in your directory right away, because uv doesn‚Äôt create those until you add dependencies. Simply run uv add to start adding them. Afterwards, the necessary requirements and lock files will update automatically. If you want to learn more, checkout my uv guide.\n\n\n\nAdd Dependencies with uv\n\n\n\nThe Pyproject.toml file\nOnce that‚Äôs done, uv will update the general dependencies in the pyproject.toml file and the specific versions in uv.lock (think requirements.txt on steroids). The nice thing here, it only lists the actual package you needed, not everything else that the package requires. So, when you want to remove packages you can simply use uv remove and the individual package names listed here to remove everything in your environment. There‚Äôs an example below.\n[project]\nname = \"basic-oss-architecture\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.13\"\ndependencies = [\n    \"dbt&gt;=1.0.0.38.22\",\n    \"duckdb&gt;=1.1.3\",\n    \"great-expectations&gt;=0.18.22\",\n    \"jupyter&gt;=1.1.1\",\n    \"pandas&gt;=2.2.3\",\n    \"pytest&gt;=8.3.4\",\n    \"quarto&gt;=0.1.0\",\n]\n\n\nUsing .gitignore effectively\nYou probably noticed, but when you initialize a project with uv it automatically creates a .gitignore file and populates it with basic files and directories which don‚Äôt need to be checked into source control (like .venv). I take this a step further, and add some Quarto specific files and directories too, .quarto and _files folders. Below is an example of my file at this early project stage.",
    "crumbs": [
      "Data Engineering and Architecture",
      "Basic Open Source Architecture"
    ]
  },
  {
    "objectID": "pages/about/about.html",
    "href": "pages/about/about.html",
    "title": "About",
    "section": "",
    "text": "About myself and this site in general.\nResume"
  },
  {
    "objectID": "pages/about/about.html#about-me",
    "href": "pages/about/about.html#about-me",
    "title": "About",
    "section": "About Me",
    "text": "About Me\nI‚Äôm a 26 year old Data Scientist, Engineer, Analyst, Architect, etc. that has two degrees from Tulane University in New Orleans‚Äì BSM, Marketing and Asian Studies; MS, Business Analytics. During COVID, I did an extra year of school and received a Masters in Data Science. As a result of my education, I fell in love with data science and machine learning, but due to common data challenges, I developed a professional passion and skillset for database architecture and data engineering."
  },
  {
    "objectID": "pages/about/about.html#linkedin-about-me-will-be-updated",
    "href": "pages/about/about.html#linkedin-about-me-will-be-updated",
    "title": "About",
    "section": "LinkedIn About Me (Will be updated)",
    "text": "LinkedIn About Me (Will be updated)\nChris is a seasoned expert in Data Science and Engineering, with a strong focus on driving organizational change and innovation through data strategy. His experience spans leading transformational initiatives and implementing data-driven solutions at an enterprise level. At General Motors, Chris spearheaded the redesign and implementation of a reporting upgrade for global warehouse systems. Additionally, he successfully proposed and designed a critical upgrade to the data architecture of the Accessories Sales Reporting application, an application that supports a $1.2 billion revenue stream by enabling advanced analytics and reporting.\nChris‚Äôs expertise covers the entire data pipeline, from ingestion and ETL to final reporting. He has hands-on experience in designing, coding, and deploying data ingestion and ETL processes using industry-standard tools such as dbt, Airflow, SQL (PostgreSQL, Hive SQL, T-SQL), and Python (Pandas, PySpark). He is certified in Data Engineering with Databricks (using Python, SQL, PySpark), Microsoft Power BI, and Agile Methodologies. His technical skillset is complemented by his proficiency in Unix scripting (zsh) and Docker for building scalable data environments and architectures. Chris has also led successful migrations of data, models, and reports from On-Premise to Cloud environments.\nIn the realm of Data Engineering and Database Management, Chris specializes in designing optimized database and reporting architectures, adhering to industry-standard Star and Snowflake schema structures, as well as the Gold, Silver, and Bronze aggregation principles. His expertise in data science includes ML model design and feature engineering, particularly in linear and logistic regression models. He has developed and implemented models that predict consumer behavior in marketing and sales, as well as risk prediction for financial lenders.\nAt the reporting and Business Intelligence stage, Chris‚Äôs journey began at General Motors as a BI/Data Analyst. He designed and developed reports to support product development, warranty programs, supply chain, and warehouse operations. Collaborating closely with non-technical partners, he translated business needs into technical requirements, ensuring that the final product met both performance and user experience goals. Chris also served as the lead administrator for GM‚Äôs Power BI platform, where he established company-wide policies to optimize performance and usability. As a leader, he managed and mentored developers, applying agile principles to balance project demands with team well-being."
  },
  {
    "objectID": "pages/guides/posts/quarto.html",
    "href": "pages/guides/posts/quarto.html",
    "title": "Quarto",
    "section": "",
    "text": "A non-exhaustive guide on using Quarto for project documentation and personal branding.",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#overview",
    "href": "pages/guides/posts/quarto.html#overview",
    "title": "Quarto",
    "section": "Overview",
    "text": "Overview\nQuarto is:\n\n\nAn open-source scientific and technical publishing system\nAuthor using Jupyter notebooks or with plain text markdown in your favorite editor.\nCreate dynamic content with Python, R, Julia, and Observable.\nPublish reproducible, production quality articles, presentations, dashboards, websites, blogs, and books in HTML, PDF, MS Word ePub, and more.\nShare knowledge and insights organization-wide by publishing to Posit Connect, Confluence, or other publishing systems.\nWrite using Pandoc markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more.\n\n\nDownloading and Updating\nFor simple instructions and a download/install guide using a GUI, visit Quarto - Get Started.\nFor MacOS users, I recommend downloading and learning about Homebrew, the package manager. It drastically simplifies all phases of package management. To install, simply use brew install quarto and you‚Äôre done.",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#projects",
    "href": "pages/guides/posts/quarto.html#projects",
    "title": "Quarto",
    "section": "Projects",
    "text": "Projects\nThis section, and the rest of the guide, assume you‚Äôre familiar with and using the uv package and project manager for Python, git for version control, and the GitHub CLI for collaboration. I‚Äôll be referencing all of these tools throughout the rest of the guide. You can read my guide to learn more about uv\n\nGeneral Workflow\nI‚Äôll be walking through the general workflow, but here‚Äôs a quick note about how I use Quarto for Data related projects. I use GitHub as my collaboration/repo hosting tool, so all of my projects have a README.md file. That way, if anyone visits the actual repo, they can view a nicely rendered markdown file, but when I‚Äôm ready to add a project to my website, I‚Äôll copy the contents into a .qmd file. Then, I can add the Quarto specific formatting.\nThis simplifies my general workflow a lot, and makes it easy to formally share and document my research.\n\n\nInitializing a Project\n\nThe create command\nI‚Äôm going to assume you‚Äôve already run the uv init command to initalize your uv project. From there, it‚Äôs easy to start a project with Quarto from the command line, and there are a few built-in project types to further simplify the startup process. Furthermore, Quarto provides a simple command for creating (or initializing) a project (or extension), quarto create project, and a handy setup guide to help you use it. The following code shows you my terminal input and outputs.\nchriskornaros@chriss-air test % quarto create project\n? Type ‚Ä∫ default\n? Directory ‚Ä∫ docs\n? Title (docs) ‚Ä∫ test_docs\nCreating project at /Users/chriskornaros/Documents/test/docs:\n  - Created _quarto.yml\n  - Created .gitignore\n  - Created test_docs.qmd\n? Open With ‚Ä∫ (don't open)\nFor a quick run through: quarto create project initializes a quarto project directory within your current working directory (the uv parent directory), type lets you choose the type of Quarto documentation (book, website, confluence, etc.), title is the title of your homepage (.qmd) file. Personally, I like to remove the docs/.gitignore file because uv creates one when you initialize a project, in the parent directory. So, having just one .gitignore file helps me keep track of things more easily.\nThe only directories I added to docs after it was created by quarto, was a pages directory for various subpages and a brand directory for .scss files, images, etc. For project, blog, or guide specific media files, I kept those within their subpage folder. Here, I keep the various landing pages and their sub directory structures. Ideally, I won‚Äôt have any files in there, but the _quarto.yml file will point to their locations in my personal GitHub repo.\n\n\n\n\n\n\nFile Context in Quarto\n\n\n\nIn my time developing this site, it seems that Quarto can only pickup on files within the context of the docs (or whatever you name your Quarto project) folder. Furthermore, it struggels with absolute context paths, and at most I could get it to work with ../../file.\n\n\n\n\n\nWorking on your Project\nNow that you‚Äôve initalized your project directory, you can begin work! Head over to Quarto Basics for documentation on the basics of .qmd files and authoring with Quarto markdown.\nJust remember, every webpage will need a .qmd file!\n\n\nRendering a Projects\nThis part is blank for now. Rendering websites have some specific components to websites and GitHub pages, that are covered later on. I will update this for other document types in the future.",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#configurations",
    "href": "pages/guides/posts/quarto.html#configurations",
    "title": "Quarto",
    "section": "Configurations",
    "text": "Configurations\n\nThe _quarto.yml file\nThis YAML file serves as the primary configuration file for your Quarto project. Similar to other Quarto YAML files, this handles document configurations, but adds the Quarto project features to sync those across documents and for more environment control. You have the ability to define project metadata for all of the different document types. In this example, I used it to define the website configurations, but if you‚Äôre working on a book or dashboard, then it could be used to normalize chapters or visuals as well.\nYou can also specify the formatting, which connects with the _brand.yml file and enables cross referencing of variables and values. Learn more with Quarto Projects.\n\n\nThe _brand.yml File\nThis is a new feature with Quarto 1.6 that allows you to define and save your design specifications in a YAML file. While this file is specific to your Quarto project directories, you can store and share the file across projects or with others to maintain brand consistency. Luckily, there is great documentation if you want more details brand.yml. While there is a lot to cover, I‚Äôll go over some basics to get started. It‚Äôs important to remember that if you specify colors for anything within .qmd files, those will overwrite the defaults in the brand file. Furthermore, Quarto and _brand.yml both utilize the Bootstrap web development framework. For a list of its full default values, visit the repo.\n\nColor\nThis is obviously an important part of all branding. There are two main components:\n\npalette\ntheme colors\n\nPalette lets you specify hexcodes and assign those to various strings. Those string values could be generic terms, like green (if there is a specific shade you would like), or terms specific to brand.yml's theme colors. When you set your default colors in this way, you can then customize the output in the _quarto.yml file. To modify, for example, your navigation bar, just define the background and foreground properties under the navbar property.\nAnother thing to keep in mind with color, just because it‚Äôs available in _brand.yml, like tertiary, doesn‚Äôt mean it‚Äôs defined and functional in the _quarto.yml file. So, you may need to be creative with how you use protected terms, like success, danger, or warning. Doing so allows you to take advantage of the programmatic benefits of the brand file, while specifying several, possibly, similar shades that would be tricky to do just be renaming colors, such as red, blue, or yellow.\nIf you aren‚Äôt sure on what colors or palettes to choose, using an LLM based chatbot can be helpful. This allows you to describe the colors and themes you‚Äôre going for, as well as refine them over time.\n\n\nTypography\nThis section lets you control which font families are included in your Quarto project. Then, you can specify where various fonts are used and for some properties, even change their specific color. As a heads up, the _brand.yml documentation seems to be correct and updated; however, bash code blocks don‚Äôt render the monospace-background the same way. So, while in-line monospace backgrounds and monospace backgrounds for Python (at the very least) will be colored as the documentation says. Bash code blocks will have no background, just the code itself in the specified font color.\n\n\nDefaults\nThis section gives you more control over various defaults, for HTML Bootstrap, Quarto, Shiny, etc. When configuring specific design colors, using the bootstrap default section will allow you to keep your Quarto files simple, while providing a high level of control over design.\n\n\nSASS - Syntactically Awesome Style Sheets\nRemember, whatever you can‚Äôt configure simply in your _brand.yml file, you can do so in a .scss file. For example, if you want to create custom light and dark mode themes, just create .scss files with the appropriate code and place this in your docs (main Quarto project) directory. Below is an example of a dark mode theme. I set the default values for the scss bootstrap variables at the top. Then, I specified the specific rules for various parts of the page. For defined variables, blockquote, you don‚Äôt need a ., but for features specific to quarto rendered sites, add a . before. For example, to modify the look of code blocks, you must use the .sourceCode variable. For child classes, for example the .sourceCode css copy-with-code class, if you want to modify that you‚Äôll need to use .sourceCode pre.copy-with-code. To find out the name of a variable you don‚Äôt know, just inspect the specific element on the webpage, and the class name will translate 1:1 with the variable name. Additionally, for any property that you need to specifically update, you can add the !important tag, which means it will override existing rules, but be careful using this.\nFor a list of all CSS variable properties, visit CSS Web Docs.\n/*-- scss:defaults --*/\n$background: #2E4053;\n$foreground: #F7F7F7;\n$primary: #FF9900;\n$secondary: #56B3FA;\n$tertiary: #655D4B;\n$light: #F7F7F7;\n$dark: #1C1F24;\n$success: #33373D;\n$danger: #1A1D23;\n$info: #56B3FA;\n$warning: #FF7575;\n\n\n/*-- scss:rules --*/\nbody {\n  font-family: 'Open Sans', sans-serif;\n  background-color: $background;\n  color: $foreground;\n}\n\nh1, h2, h3 {\n  color: $danger;\n}\n\nh4, h5, h6 {\n  color: $danger;\n  font-weight: bold;\n}\n\nblockquote {\n  background-color: #2E6490; /* added background color */\n  border-color: $dark;\n  color: $danger !important;\n}\n\ncode {\n  background-color: $success;\n  color: $info;\n}\n\n.sourceCode {\n  background-color: $success;\n  color: $info;\n}\n\n.sourceCode pre.code-with-copy{\n  padding: 0;\n}\n\n.callout-title-container.flex-fill {\n  color: $danger;\n}\n\n\n\n\n\n\nCSS Variable Names\n\n\n\nThere are some weird naming convention differences between _brand.yml and Quarto. The big one is monotone being used to reference block quotes, code blocks, and in-line code in _brand, but in Quarto it renders the in-line code as code and the code blocks as sourceCode. Make sure to use inspect element to be sure on what you‚Äôre changing. CSS class names can get long, especially when referncing nested classes, just experiment and take your time with things.",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#websites",
    "href": "pages/guides/posts/quarto.html#websites",
    "title": "Quarto",
    "section": "Websites",
    "text": "Websites\nWebsites really stretch and push the boundaries of what you can accomplish with Quarto. In this section, I‚Äôll walk through a few key points of developing them.\n\nBlogs\nBlogs are a special kind of Quarto website that consists of a collection of posts along with a navigational page that lists them in reverse chronological order. Pretty much all of the information you‚Äôll need about blogs is the same as the parts of this guide covering websites. Just know, it‚Äôs easy to integrate a blog as a subpage of a larger website.\nSimply add the blogs project structure as a subdirectory of pages/. To keep track of things, I made the title of the main blog page blogs.qmd, so it doesn‚Äôt conflict with the index.qmd that is the home page of my whole website. Then, I added post categories within the posts/ directory of the Quarto blogs/ directory.\nThat being said, and I‚Äôm not quite sure why, but the _metadata.yml file\n\n\nRendering Websites\nI run the following code block from my main project directory. My Quarto project directory is a folder called docs. So, I specify to Quarto that I want to render the entire Quarto project docs, but quarto render‚Äôs context is specific to the quarto project directory. Therefore, I need to use the . to specify that I want the rendered .html files put in the Quarto project folder, and sub folder.\nquarto render docs --output-dir .\nConversely, you can specify, within the output property of your _quarto.yml file that output-dir: .\nThis is also the same syntax when previewing your website, using quarto preview docs, the difference is there is no need to specify an output directory. What this does is spin up a jupyter kernel to render your .qmd files, then, it displays the output in a browser. When you hit save on your _quarto.yml, .scss, and .qmd files then the site will automatically update (it doesn‚Äôt for _brand.yml saves).\nOnce you‚Äôve rendered your website, and pushed the commit, the change is reflected in a few mintues.\n\n\n\n\n\n\nquarto preview with uv\n\n\n\nThe ease of using quarto preview is magnified when using uv as your project/package manager. Instead of having to manage various virtual environments and packages, as well as activation and deactivation, uv does it all. Even VS Code picks up on the context uv provides. The terminal will automatically realize you‚Äôre in a uv environment and display output as if you were using a virutal environment (even though you haven‚Äôt activated it).\n\n\n\n\nWebsite Navigation\n\nTop Navigation\nAfter you‚Äôve set your default color values in _brand.yml, make sure to specify the design details at the top of your navbar property. This is useful, even when using .scss files for more specific design control because you can utilize those variables in your light and dark themes.\nFor pages on your top navigation bar that just have a landing page, simple use the following syntax\nnavbar:\n  left:\n    text: \"Page name\"\n    href: path/to/page/file.qmd\n\n\n\n\n\n\nDashes and Intentation Matter in YAML\n\n\n\nNotice when I‚Äôm using a - and not. This is deliberate. In my development, I realized that where you use and specify the dash can affect functionality. Some places require it, some don‚Äôt, and it may depend on the order of various parameters.\n\n\nFor page categories that may have several landing pages, or even subcategories, you‚Äôll need to utilize hybrid navigation which combines Top and Side navigation. On the top, you‚Äôll use the following syntax:\nnavbar:\n  left:\n    text: \"Page group name\"\n    menu:\n      - path/to/page/group/landing.qmd\n      - path/to/page/group/1/landing.qmd\n      - path/to/page/group/2/landing.qmd\nThen, you‚Äôll need to handle the rest in Side Navigation; however, it isn‚Äôt perfect. You can‚Äôt have nested drop down options in your top navigation bar, so the best I came up with was having a landing page for the top level and first tier subcategories, then handled the rest on the sidebar (which only pops up on affiliated pages).\n\n\nSide Navigation\nFor some reason, Side Navigation in Quarto is much more robust and intuitive. That being said, by combining features here with the top bar, you can achieve a fairly dynamic navigation experience.\nThere are a few key differences. To start with, sidebar objects inherit properties from the first defined, so long as none are changed. Second, you‚Äôll want to use an id with the top level landing pages, because this allows you to reference those in your top navigation bar (for more advanced integrations) using the address sidebar:id, although I struggled with this functionality and didn‚Äôt end up using it.\nThe general structure for your first page group is as follows.\nsidebar:\n    - id: guides\n      title: \"Guides\"\n      style: \"docked\"\n      background: dark\n      foreground: light\n      collapse-level: 2\n      contents:\n        - pages/guides/guides.qmd\nNow, if that‚Äôs where things end, you could just list pages on and on using the text: href: syntax. That being said, you probably are going to have a few subcategories, and possibly even further nested subcategories. To enable this, don‚Äôt use the text: syntax, instead use section:. This tells Quarto that you are defining a section, rather than just one single page. As you might guess, you can further nest sections, or specific pages, depending on your use of text: and section: with href:. See an example below.\n- id: projects\n      title: \"Projects\"\n      contents:\n        - pages/projects/projects.qmd\n        - section: \"Data Engineering and Architecture\"\n          href: pages/projects/data_engineering/data_engineering.qmd\n          contents:\n            - section: \"DataCamp Data Engineering Projects\"\n              href: pages/projects/data_engineering/datacamp/datacamp.qmd\n              contents:\n                - text: Bank Marketing ETL\n                  href: pages/projects/data_engineering/datacamp/bank_etl/bank_etl.qmd\n            - section: \"Data Engineering Research Projects\"\n              href: pages/projects/data_engineering/research/research.qmd\nFurthermore, it‚Äôs crucial, for nested subpages, to have the parent landing page‚Äôs .qmd file‚Äôs path as a standalone line in the contents of the _quarto.yml file. Failing to do so will remove the sidebar from the landing page‚Äôs navigation; however, for subsections, those should be specified within an href paramter, under the section line. Additionally, for the collapsable functionality to work consistently in a sidebar, you‚Äôll need it docked. The behavior is inconsistent with floating sidebars. After you‚Äôve set your default color values in _brand.yml, make sure to specify the design details at the top of your sidebar property.\n\n\n\nSharing Websites\nThere are two primary ways to publish your website once you‚Äôre done making edits, assuming you‚Äôre also using GitHub Pages.\n\nquarto render docs\nquarto publish docs\n\nFor simplicity, I chose to use quarto render docs (note that docs is used here because that‚Äôs the name of my main quarto project directory, not because it‚Äôs part of the command itself) because all I need to do is that and then push the changes. With quarto publish docs, it appeared to me that I would need to setup a branch for my git repository and possibly GitHub actions. I will probably do this in the future, for learning purposes, but didn‚Äôt want to for the sake of time.\nThat being said, the official documentation is very straightforward, and regardless of what you choose, there are two common steps:\n\ntouch .nojekyll\n\nThis tells GitHub pages not to do any additional processing of your website, include this in your docs directory\n\nIn a browser go to GitHubPagesRepo &gt; Settings &gt; Code and automation &gt; Pages\n\nThen, make sure Source is set to Deploy from a branch\nSet your branch to the quarto project directory, in your main project folder, docs in my case\n\n\nThen the classic:\n\ngit add docs\ngit commit -m \"Website updates.\"\ngit push",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#other-notes",
    "href": "pages/guides/posts/quarto.html#other-notes",
    "title": "Quarto",
    "section": "Other Notes",
    "text": "Other Notes\nI‚Äôll update this section with more notes and tips that come to mind as I finish building out the site, version 1.0. Then, I‚Äôll reorganize what goes here into the proper places on the document.\n\nIf you want to use past .ipynb files as documentation, or add longer write ups to those files, there is a jupyter command\n\njupyter nbconvert file.ipynb --to markdown --output file.md\nmv file.md &gt; file.qmd\nDone! Just make any quarto specific modifications that you need",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#conclusion",
    "href": "pages/guides/posts/quarto.html#conclusion",
    "title": "Quarto",
    "section": "Conclusion",
    "text": "Conclusion\nNow, you‚Äôre all done with this guide, thank you for reading!\nCurrently, this is only updated to include my notes and thoughts from when I built my personal website. As I use Quarto to create a variety of document types, I will update this Guide with more. Follow me on Bluesky to stay connected with me and up to date with my work.",
    "crumbs": [
      "Quarto"
    ]
  }
]
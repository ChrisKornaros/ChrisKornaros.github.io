[
  {
    "objectID": "pages/guides/guides.html",
    "href": "pages/guides/guides.html",
    "title": "Guides",
    "section": "",
    "text": "Welcome to the Guides section of the website! This is the landing page for step-by-step guides and instructions for various tools and workflows that I‚Äôve used in the past or am currently exploring.",
    "crumbs": [
      "Guides"
    ]
  },
  {
    "objectID": "pages/guides/guides.html#future-guides-planned",
    "href": "pages/guides/guides.html#future-guides-planned",
    "title": "Guides",
    "section": "Future Guides (Planned)",
    "text": "Future Guides (Planned)\nHere are some topics I plan to cover in the future:\n\nDuckDB\ndbt\nPostgreSQL\n\nStay tuned for updates!",
    "crumbs": [
      "Guides"
    ]
  },
  {
    "objectID": "pages/guides/guides.html#current-guides",
    "href": "pages/guides/guides.html#current-guides",
    "title": "Guides",
    "section": "Current Guides",
    "text": "Current Guides",
    "crumbs": [
      "Guides"
    ]
  },
  {
    "objectID": "pages/guides/posts/uv.html",
    "href": "pages/guides/posts/uv.html",
    "title": "uv, the Python Project and Package Manager",
    "section": "",
    "text": "A basic guide on using uv the package and projects manager for Python developers.",
    "crumbs": [
      "Guides",
      "uv"
    ]
  },
  {
    "objectID": "pages/guides/posts/uv.html#introduction",
    "href": "pages/guides/posts/uv.html#introduction",
    "title": "uv, the Python Project and Package Manager",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\nVS Code Shortcuts\n\n\n\nIf you‚Äôre using VS Code, here are some useful shortcuts. - Note, use CMD-K CMD-S to open the keyboard shortcuts. - SHFT-CMD-i inserts a code block\n\n\nuv is an Open Source project by Astral, the makers of ruff, that is self described (and worthy of the title) as an extremely fast Python package and project manager, written in Rust.\n\nüöÄ A single tool to replace pip, pip-tools, pipx, poetry, pyenv, twine, virtualenv, and more.\n‚ö°Ô∏è 10-100x faster than pip.\nüêç Installs and manages Python versions.\nüõ†Ô∏è Runs and installs Python applications.\n‚ùáÔ∏è Runs scripts, with support for inline dependency metadata.\nüóÇÔ∏è Provides comprehensive project management, with a universal lockfile.\nüî© Includes a pip-compatible interface for a performance boost with a familiar CLI.\nüè¢ Supports Cargo-style workspaces for scalable projects.\nüíæ Disk-space efficient, with a global cache for dependency deduplication.\n‚è¨ Installable without Rust or Python via curl or pip.\nüñ•Ô∏è Supports macOS, Linux, and Windows.\n\nI‚Äôm only just beginning to learn and use the tool in my own projects (including converting my existing project environments to uv) and from what I‚Äôve seen it‚Äôs going to make life much easier. That being said, while you overwrite the muscle memory developed for years with pip and venv, there will be some growing pains; however, for those who are less familiar with what I‚Äôm talking about, I‚Äôll still explain some basic concepts and snags that I both run and ran into.",
    "crumbs": [
      "Guides",
      "uv"
    ]
  },
  {
    "objectID": "pages/guides/posts/uv.html#basic-workflow-and-guide",
    "href": "pages/guides/posts/uv.html#basic-workflow-and-guide",
    "title": "uv, the Python Project and Package Manager",
    "section": "Basic workflow and guide",
    "text": "Basic workflow and guide\n\nConcepts to Know Before Getting Started\n\nBasic knowledge of directories, bash (zsh in the case of MacOS), and using the CLI bash\nBasic knowledge of Python, common project structures, and simple workflows Python\nBasic knowledge of git (for local version control) and GitHub (for collaboration) git and GitHub basics\n\n\n\nInitializing a Project\n\nLocal Repository\nThe nice thing about uv is that it‚Äôs designed to make Python development easier, so there aren‚Äôt any head-scratching gotchas.\nFor the sake of this example and entire template, let‚Äôs assume I‚Äôm currently sitting in my main directory. For some that might be home, others app, for MacOS the default is /usr/yourusername, or maybe you prefer to put all projects in a Documents or Projects folder. Anyways, to start up a project you can do one of two things:\n\nHave uv do everything, and then change directories\n\nuv init uv_basic\ncd uv_basic\n\nCreate the directory, change directories, and then have uv do everything\n\nmkdir uv_basic\ncd uv_basic\nuv init\n\n\nThis will create 4 files and initalize a local git repository:\n\n.python-version\n.pyproject.toml\nhello.py\nREADME.md\n.git\n.gitignore\n\n\n\n\n\n\n\nuv and the .gitignore file\n\n\n\nThe nice thing about uv is that it autopopulates your .gitignore file with a few files and patterns, not to mention, it provides some basic tagging for what it puts in there. Just open the file (it‚Äôs plain text) to see. Since I‚Äôm saving my progress with this repo using git, I want to keep the overall file size down. So, I also included the .html and .ipynb file that Quarto generates because they can get large fast. Additionally, when you initialize your GitHub repo with the CLI‚Äôs repo creation process, I don‚Äôt include a README or .gitignore, because those are included in uv init.\n\n\n\n\nRemote Repository\nFor anyone familiar with software development you‚Äôve probably heard of GitHub or GitLab. I‚Äôm more familiar, professionally and personally, with GitHub (which is what I‚Äôll be using in this example); however, there are a large amount of people that prefer GitLab because it is better for some enterprise and personal use cases‚Äì GitHub vs.¬†GitLab. For this, you‚Äôll want to install the GitHub CLI. Then, you can follow along.\n\nVerify the installations and make sure to get your credentials setup, in git\n\nwhich gh and which git\n\nAdd your name and email\n\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your-email@example.com\"\n\nAuthenticate access to GitHub\n\ngh auth login\nUsing the CLI option, follow the instructions\nSelect HTTPS for the easier connection option\n\nVerify you have proper access to your GitHub\n\ngh auth status\n\n\ngithub.com\n  ‚úì Logged in to github.com account itsmeChis (keyring)\n  - Active account: true\n  - Git operations protocol: https\n  - Token: gho_************************************\n  - Token scopes: 'delete_repo', 'gist', 'read:org', 'repo', 'workflow'\n\ngh repo list\nAssuming you haven‚Äôt, create your project repo from the CLI (you can also do so using the GitHub.com GUI, but I prefer this way to reinforce my learning)\n\ngh repo create\nCreate a new repository from scratch\nuv basic\noptional description\nPublic\nGNU Affero General Public License v3.0 Which license do you need? \n\nSet the newly created repo as the local git repo‚Äôs upstream\n\nThis will result in an error (git pull)\nSet the global config to merge git pull\ngit pull with a commit message\ngit status to verify\ngit push\n\n\n\n\n\nAdding and managing dependencies\nThus far, the workflow with uv isn‚Äôt too dissimilar from using pip and venv, but managing dependencies and testing scripts is where uv shines. As you‚Äôll see below, with pip and venv, you have to manually create the virtual environment, activate it, install dependencies, manage requirements files, and then run your script. With uv, however, almost all of that is done automatically and things like uv pip list or uv venv are only there for backwards compatibility. A lot of the tedious pieces of the DevOps workflow are now obsolete or handled in the background.\n\nUsing pip and venv\nWhen using a combination of pip and venv, your typical workflow is straightforward, but becomes complicated if you need to uninstall certain packages or make quick, iterative tests of code.\nmkdir uv_basic\ncd uv_basic\npython -m venv .venv\nsource .venv/bin/activate\npip install duckdb\npip install numpy\npip freeze &gt; requirements.txt\npython script.py\n\n# Realize you don't need numpy, so you want to uninstall it and keep your environment cleaner\ndeactivate\nrm -r .venv\npython -m venv .venv\nsource .venv/bin/activate\n# Two options here, delete numpy from requirements.txt, not scalable with many packages, or reinstall just duckdb, also not scaleable\npip install duckdb\npip freeze &gt; requirements.txt\npython script.py\nAs you can see, the initial workflow isn‚Äôt horrible, but if you need to make a change to the environment or just want to test something small, the number of steps quickly multiplies.\n\n\nUsing uv\nCompare that with the streamlined uv workflow.\nuv init uv_basic\ncd uv_basic\nuv add duckdb\nuv add numpy\nuv run script.py\nuv remove numpy\nuv run script.py\nThe workflow improvements and efficiency should be obvious. The nice thing is that uv functions as your standalone virtual environment, without the need for activation or deactivation. Using uv add will add a dependency to both your pyproject.toml file and your uv.lock file. Additionally, if you are more familiar with verifying using pip, running uv pip list will show that the package is there (although the pip functionality is obsolete and only for backwards compatibility at this point). If you want to remove a package, simply use uv remove and that will also remove it from the .toml and .lock files. The last feature you‚Äôll need to understand (to use uv at a basic level) is uv sync. Simply put, it syncs your environment with the project‚Äôs dependencies/lock file. This ensures that the exact versions specified in your lockfile are used in your environment‚Äì dependencies may be added, removed, or updated if there are updates to the declared dependencies.\nTo cap this off, here are some common use cases for uv sync: - Run uv sync (without ‚Äìfrozen) to keep dependencies up-to-date and to resolve changes. - Use uv sync ‚Äìfrozen to validate dependencies without altering them\n\n\n\nConverting your Legacy Projects to uv\nNow that you‚Äôve seen the benefits of uv, as well as the workflow differences, you probably want to give it a try or even convert entire projects to uv. The good news is that this is simple and only requires a few modifications to get things up and running. The general workflow is the same as I outlined above, you‚Äôll just be cleaning up your local environment and reinstalling things along the way. The project I converted to use uv for this example utilizes DuckDB and dbt for the database and data modeling/ETL. I‚Äôll include some dbt specific information, for example if you move your database file from a subdirectory to the main one, remember to update your dbt profiles in your global dbt location.\n\nChange directories to your specific project directory\nRun uv init, it will create any file or folder that isn‚Äôt currently in the main folder\n\nIf you already have a .git folder and commit history, uv will not delete or overwrite the original folder.\n\nAdd all of the dependencies you need, then remove your requirements file (it‚Äôs no longer needed)\n\nAs of writing this, I wasn‚Äôt sure how to use uv add with the legacy requirements file, uv pip install -r kind of worked, but didn‚Äôt actually add the dependencies to the .toml or .lock files\nThere must be an easier way to bulk add dependencies, but I manually did it\nIn my case, I had to remember to add both dbt and dbt-duckdb, so the adapter would work\n\nInstall all of the CLI tools that you need, and don‚Äôt want or use globally\n\nIn my case, I need jupyter, quarto, and dbt, but I also have the latter two installed globally\n\nVerify that uv can run things correctly\n\nI first used uv run hello.py to verify that the basic functionality is there\nThen, I ran a more complex script, that imports and uses duckdb, to ensure the packages are installing and running as intended\nThen, I used uv tool list to verify which CLI tools are installed\nFinally, I verified that the CLI tools work, by using uv run dbt run --select transform to test dbt model functionality in uv\n\n\n\n\nFinal Thoughts\nSo that‚Äôs it! Overall, uv is incredibly easy to setup and configure because it builds on the classic workflows, while simplifying or abstracting some of the process. You also saw how easy it is to start using uv with older projects that use the legacy workflow. At the time of writing this, I‚Äôve only been using uv for a few days, so I‚Äôm sure there are things I got wrong or missed, please comment to let me know!\nI‚Äôm happy to chat and love learning about data, as well as what folks in this space are working on. Connect with me on Bluesky @chriskornaros.bsky.social to follow along with what I‚Äôm working on, learning, or just to say hi! Below are some other notes and thoughts I had while working on this write up.\n\nGeneral Notes\n\nIt seems that while tools are specific to a uv project instance (i.e.¬†uv_basic returns the .venv dir when asking which jupyter, but test before intalling anything say it can‚Äôt be found), when you use uv tool install it installs it to the system wide uv\nuv pip list defaults to the global (non-uv or non-pip) python environment (in my case it‚Äôs pip and wheel), but once you install something (using add, pip install, etc.) it switches the context to the current parent uv dir (i.e.¬†test, instead of uv_basic)\n\nTools are still listed even after this\n\nuv tool install only works when installing python package specific tools, but DuckDB for Python (for example) doesn‚Äôt come packaged with the DuckDB CLI tools, so uv tool install duckdb won‚Äôt install the DuckDB CLI features\nIt seems that saving variable with duckdb.sql(‚Ä¶).show() and then printing the type of that, just prints the query output, insteaed of the type\nBased on tests, the workflow changes are as follows",
    "crumbs": [
      "Guides",
      "uv"
    ]
  },
  {
    "objectID": "pages/projects/projects.html",
    "href": "pages/projects/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Landing page for the project portfolio portion of this website. Contains all of my public repositories and projects (for now, may include future consulting or paid side work, but I don‚Äôt do that at the moment), including both the code in repositories and write ups (where applicable).\nCurrently, there are two categories of projects I‚Äôm working on:\n\nData Engineering and Architecture\nData Science and Machine Learning",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/bank_etl.html",
    "href": "pages/projects/data_engineering/posts/bank_etl.html",
    "title": "Bank Marketing ETL Project",
    "section": "",
    "text": "Piggy bank\nPersonal loans are a lucrative revenue stream for banks. The typical interest rate of a two-year loan in the United Kingdom is around 10%. This might not sound like a lot, but in September 2022 alone UK consumers borrowed around ¬£1.5 billion, which would mean approximately ¬£300 million in interest generated by banks over two years!\nYou have been asked to work with a bank to clean the data they collected as part of a recent marketing campaign, which aimed to get customers to take out a personal loan. They plan to conduct more marketing campaigns going forward so would like you to ensure it conforms to the specific structure and data types that they specify so that they can then use the cleaned data you provide to set up a PostgreSQL database, which will store this campaign‚Äôs data and allow data from future campaigns to be easily imported.\nThey have supplied you with a csv file called \"bank_marketing.csv\", which you will need to clean, reformat, and split the data, saving three final csv files. Specifically, the three files should have the names and contents as outlined below:",
    "crumbs": [
      "Data Engineering and Architecture",
      "Bank Marketing ETL"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/bank_etl.html#client.csv",
    "href": "pages/projects/data_engineering/posts/bank_etl.html#client.csv",
    "title": "Bank Marketing ETL Project",
    "section": "client.csv",
    "text": "client.csv\n\n\n\n\n\n\n\n\n\ncolumn\ndata type\ndescription\ncleaning requirements\n\n\n\n\nclient_id\ninteger\nClient ID\nN/A\n\n\nage\ninteger\nClient‚Äôs age in years\nN/A\n\n\njob\nobject\nClient‚Äôs type of job\nChange \".\" to \"_\"\n\n\nmarital\nobject\nClient‚Äôs marital status\nN/A\n\n\neducation\nobject\nClient‚Äôs level of education\nChange \".\" to \"_\" and \"unknown\" to np.NaN\n\n\ncredit_default\nbool\nWhether the client‚Äôs credit is in default\nConvert to boolean data type: 1 if \"yes\", otherwise 0\n\n\nmortgage\nbool\nWhether the client has an existing mortgage (housing loan)\nConvert to boolean data type: 1 if \"yes\", otherwise 0",
    "crumbs": [
      "Data Engineering and Architecture",
      "Bank Marketing ETL"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/bank_etl.html#campaign.csv",
    "href": "pages/projects/data_engineering/posts/bank_etl.html#campaign.csv",
    "title": "Bank Marketing ETL Project",
    "section": "campaign.csv",
    "text": "campaign.csv\n\n\n\n\n\n\n\n\n\ncolumn\ndata type\ndescription\ncleaning requirements\n\n\n\n\nclient_id\ninteger\nClient ID\nN/A\n\n\nnumber_contacts\ninteger\nNumber of contact attempts to the client in the current campaign\nN/A\n\n\ncontact_duration\ninteger\nLast contact duration in seconds\nN/A\n\n\nprevious_campaign_contacts\ninteger\nNumber of contact attempts to the client in the previous campaign\nN/A\n\n\nprevious_outcome\nbool\nOutcome of the previous campaign\nConvert to boolean data type: 1 if \"success\", otherwise 0.\n\n\ncampaign_outcome\nbool\nOutcome of the current campaign\nConvert to boolean data type: 1 if \"yes\", otherwise 0.\n\n\nlast_contact_date\ndatetime\nLast date the client was contacted\nCreate from a combination of day, month, and a newly created year column (which should have a value of 2022);  Format = \"YYYY-MM-DD\"",
    "crumbs": [
      "Data Engineering and Architecture",
      "Bank Marketing ETL"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/bank_etl.html#economics.csv",
    "href": "pages/projects/data_engineering/posts/bank_etl.html#economics.csv",
    "title": "Bank Marketing ETL Project",
    "section": "economics.csv",
    "text": "economics.csv\n\n\n\n\n\n\n\n\n\ncolumn\ndata type\ndescription\ncleaning requirements\n\n\n\n\nclient_id\ninteger\nClient ID\nN/A\n\n\ncons_price_idx\nfloat\nConsumer price index (monthly indicator)\nN/A\n\n\neuribor_three_months\nfloat\nEuro Interbank Offered Rate (euribor) three-month rate (daily indicator)\nN/A\n\n\n\nimport pandas as pd\nimport numpy as np\n\n# Start coding here...\ndf = pd.read_csv(\"bank_marketing.csv\")\n\nfor col in [\"credit_default\", \"mortgage\", \"previous_outcome\", \"campaign_outcome\"]:\n    print(col)\n    print(\"--------------\")\n    print(df[col].value_counts())\ncredit_default\n--------------\nno         32588\nunknown     8597\nyes            3\nName: credit_default, dtype: int64\nmortgage\n--------------\nyes        21576\nno         18622\nunknown      990\nName: mortgage, dtype: int64\nprevious_outcome\n--------------\nnonexistent    35563\nfailure         4252\nsuccess         1373\nName: previous_outcome, dtype: int64\ncampaign_outcome\n--------------\nno     36548\nyes     4640\nName: campaign_outcome, dtype: int64\nclient = df[['client_id', 'age', 'job', 'marital', 'education', 'credit_default', 'mortgage']]\ncampaign = df[['client_id', 'number_contacts', 'contact_duration', 'previous_campaign_contacts', 'previous_outcome', 'campaign_outcome', 'day', 'month']]\neconomics = df[['client_id', 'cons_price_idx', 'euribor_three_months']]\n\nprint(client.head())\nprint(campaign.head())\nprint(economics.head())\n   client_id  age        job  marital    education credit_default mortgage\n0          0   56  housemaid  married     basic.4y             no       no\n1          1   57   services  married  high.school        unknown       no\n2          2   37   services  married  high.school             no      yes\n3          3   40     admin.  married     basic.6y             no       no\n4          4   56   services  married  high.school             no       no\n   client_id  number_contacts  contact_duration  ...  campaign_outcome day month\n0          0                1               261  ...                no  13   may\n1          1                1               149  ...                no  19   may\n2          2                1               226  ...                no  23   may\n3          3                1               151  ...                no  27   may\n4          4                1               307  ...                no   3   may\n\n[5 rows x 8 columns]\n   client_id  cons_price_idx  euribor_three_months\n0          0          93.994                 4.857\n1          1          93.994                 4.857\n2          2          93.994                 4.857\n3          3          93.994                 4.857\n4          4          93.994                 4.857\nimport numpy as np\n\nclient_c = client.copy()\nclient_c['job'] = client_c['job'].replace('.', '_')\nclient_c['education'] = client_c['education'].str.replace('.', '_')\nclient_c['education'].replace('unknown', np.NaN, inplace=True)\nclient_c['credit_default'] = client_c['credit_default'].apply(lambda x: 1 if x == 'yes' else 0)\nclient_c['credit_default'] = client_c['credit_default'].astype('bool')\nclient_c['mortgage'] = client_c['mortgage'].apply(lambda x: 1 if x == 'yes' else 0)\nclient_c['mortgage'] = client_c['mortgage'].astype('bool')\n\nprint(client_c.head())\n   client_id  age        job  marital    education  credit_default  mortgage\n0          0   56  housemaid  married     basic_4y           False     False\n1          1   57   services  married  high_school           False     False\n2          2   37   services  married  high_school           False      True\n3          3   40     admin.  married     basic_6y           False     False\n4          4   56   services  married  high_school           False     False\ncampaign_c = campaign.copy()\n\ncampaign_c['previous_outcome'] = campaign_c['previous_outcome'].apply(lambda x: 1 if x == 'success' else 0).astype('bool')\ncampaign_c['campaign_outcome'] = campaign_c['campaign_outcome'].apply(lambda x: 1 if x == 'yes' else 0).astype('bool')\ncampaign_c['year'] = 2022\ncampaign_c['last_contact_date'] = pd.to_datetime(campaign_c['day'].astype(str) + campaign_c['month'] + campaign_c['year'].astype(str), format='%d%b%Y')\n\ncampaign_c.head()\n\n\n\n\n\n\n\n\nclient_id\n\n\nnumber_contacts\n\n\ncontact_duration\n\n\nprevious_campaign_contacts\n\n\nprevious_outcome\n\n\ncampaign_outcome\n\n\nday\n\n\nmonth\n\n\nyear\n\n\nlast_contact_date\n\n\n\n\n\n\n0\n\n\n0\n\n\n1\n\n\n261\n\n\n0\n\n\nFalse\n\n\nFalse\n\n\n13\n\n\nmay\n\n\n2022\n\n\n2022-05-13\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n149\n\n\n0\n\n\nFalse\n\n\nFalse\n\n\n19\n\n\nmay\n\n\n2022\n\n\n2022-05-19\n\n\n\n\n2\n\n\n2\n\n\n1\n\n\n226\n\n\n0\n\n\nFalse\n\n\nFalse\n\n\n23\n\n\nmay\n\n\n2022\n\n\n2022-05-23\n\n\n\n\n3\n\n\n3\n\n\n1\n\n\n151\n\n\n0\n\n\nFalse\n\n\nFalse\n\n\n27\n\n\nmay\n\n\n2022\n\n\n2022-05-27\n\n\n\n\n4\n\n\n4\n\n\n1\n\n\n307\n\n\n0\n\n\nFalse\n\n\nFalse\n\n\n3\n\n\nmay\n\n\n2022\n\n\n2022-05-03\n\n\n\n\n\ncampaign_c['previous_outcome'].value_counts()\nFalse    39815\nTrue      1373\nName: previous_outcome, dtype: int64\nclient = client_c\ncampaign = campaign_c.drop(['month', 'day', 'year'], axis=1)\n\nprint(client.head())\nprint(campaign.head())\nprint(economics.head())\n   client_id  age        job  marital    education  credit_default  mortgage\n0          0   56  housemaid  married     basic_4y           False     False\n1          1   57   services  married  high_school           False     False\n2          2   37   services  married  high_school           False      True\n3          3   40     admin.  married     basic_6y           False     False\n4          4   56   services  married  high_school           False     False\n   client_id  number_contacts  ...  campaign_outcome  last_contact_date\n0          0                1  ...             False         2022-05-13\n1          1                1  ...             False         2022-05-19\n2          2                1  ...             False         2022-05-23\n3          3                1  ...             False         2022-05-27\n4          4                1  ...             False         2022-05-03\n\n[5 rows x 7 columns]\n   client_id  cons_price_idx  euribor_three_months\n0          0          93.994                 4.857\n1          1          93.994                 4.857\n2          2          93.994                 4.857\n3          3          93.994                 4.857\n4          4          93.994                 4.857\nclient.to_csv('client.csv', index=False)\ncampaign.to_csv('campaign.csv', index=False)\neconomics.to_csv('economics.csv', index=False)",
    "crumbs": [
      "Data Engineering and Architecture",
      "Bank Marketing ETL"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/data_engineering.html",
    "href": "pages/projects/data_engineering/data_engineering.html",
    "title": "Data Engineering and Architecture Projects",
    "section": "",
    "text": "Landing page for all my project posts related to Data Engineering and Architecture.",
    "crumbs": [
      "Data Engineering and Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/data_engineering.html#projects",
    "href": "pages/projects/data_engineering/data_engineering.html#projects",
    "title": "Data Engineering and Architecture Projects",
    "section": "Projects",
    "text": "Projects",
    "crumbs": [
      "Data Engineering and Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/login_validation.html",
    "href": "pages/projects/data_science/posts/login_validation.html",
    "title": "User Login Validation",
    "section": "",
    "text": "login_img\n\n\nYou recently joined a small startup as a junior developer. The product managers have come to you for help improving new user sign-ups for the company‚Äôs flagship mobile app.\nThere are lots of invalid and incomplete sign-up attempts crashing the app. Before creating new accounts, you suggest standardizing validation checks by writing reusable Python functions to validate names, emails, passwords, etc. The managers love this idea and task you with coding core validation functions for improving sign-ups. It‚Äôs your job to write these custom functions to check all user inputs to ensure they meet minimum criteria before account creation to reduce crashes.\n# Re-run this cell\n# Preloaded data for validating email domain.\ntop_level_domains = [\n    \".org\",\n    \".net\",\n    \".edu\",\n    \".ac\",\n    \".gov\",\n    \".com\",\n    \".io\"\n]\n# Start coding here. Use as many cells as you need.\ndef validate_name(name):\n    if type(name) != str:\n            return False\n    elif len(name) &lt;= 2: \n        return False\n    else:\n        return True\n\ndef validate_email(email):\n    if '@' not in email:\n        return False\n    for domain in top_level_domains:\n        if domain in email:\n            return True\n    return False",
    "crumbs": [
      "Data Science and Machine Learning",
      "Login Validation"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/netflix.html",
    "href": "pages/projects/data_science/posts/netflix.html",
    "title": "Netflix Movies",
    "section": "",
    "text": "Movie popcorn on red background\nNetflix! What started in 1997 as a DVD rental service has since exploded into one of the largest entertainment and media companies.\nGiven the large number of movies and series available on the platform, it is a perfect opportunity to flex your exploratory data analysis skills and dive into the entertainment industry. Our friend has also been brushing up on their Python skills and has taken a first crack at a CSV file containing Netflix data. They believe that the average duration of movies has been declining. Using your friends initial research, you‚Äôll delve into the Netflix data to see if you can determine whether movie lengths are actually getting shorter and explain some of the contributing factors, if any.\nYou have been supplied with the dataset netflix_data.csv , along with the following table detailing the column names and descriptions. This data does contain null values and some outliers, but handling these is out of scope for the project. Feel free to experiment after submitting!",
    "crumbs": [
      "Data Science and Machine Learning",
      "Netflix Movies"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/netflix.html#the-data",
    "href": "pages/projects/data_science/posts/netflix.html#the-data",
    "title": "Netflix Movies",
    "section": "The data",
    "text": "The data\n\nnetflix_data.csv\n\n\n\nColumn\nDescription\n\n\n\n\nshow_id\nThe ID of the show\n\n\ntype\nType of show\n\n\ntitle\nTitle of the show\n\n\ndirector\nDirector of the show\n\n\ncast\nCast of the show\n\n\ncountry\nCountry of origin\n\n\ndate_added\nDate added to Netflix\n\n\nrelease_year\nYear of Netflix release\n\n\nduration\nDuration of the show in minutes\n\n\ndescription\nDescription of the show\n\n\ngenre\nShow genre\n\n\n\n# Importing pandas and matplotlib\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Start coding!\nnetflix_df = pd.read_csv('netflix_data.csv')\nnetflix_subset = netflix_df[netflix_df[\"type\"] == \"Movie\"]\nnetflix_movies = netflix_subset[[\"title\", \"country\", \"genre\", \"release_year\", \"duration\"]]\nnetflix_movies.head()\n\n\n\n\n\n\n\n\ntitle\n\n\ncountry\n\n\ngenre\n\n\nrelease_year\n\n\nduration\n\n\n\n\n\n\n1\n\n\n7:19\n\n\nMexico\n\n\nDramas\n\n\n2016\n\n\n93\n\n\n\n\n2\n\n\n23:59\n\n\nSingapore\n\n\nHorror Movies\n\n\n2011\n\n\n78\n\n\n\n\n3\n\n\n9\n\n\nUnited States\n\n\nAction\n\n\n2009\n\n\n80\n\n\n\n\n4\n\n\n21\n\n\nUnited States\n\n\nDramas\n\n\n2008\n\n\n123\n\n\n\n\n6\n\n\n122\n\n\nEgypt\n\n\nHorror Movies\n\n\n2019\n\n\n95\n\n\n\n\n\nshort_movies = netflix_movies[netflix_movies[\"duration\"]&lt;60]\nshort_movies.head(20)\n\n\n\n\n\n\n\n\ntitle\n\n\ncountry\n\n\ngenre\n\n\nrelease_year\n\n\nduration\n\n\n\n\n\n\n35\n\n\n#Rucker50\n\n\nUnited States\n\n\nDocumentaries\n\n\n2016\n\n\n56\n\n\n\n\n55\n\n\n100 Things to do Before High School\n\n\nUnited States\n\n\nUncategorized\n\n\n2014\n\n\n44\n\n\n\n\n67\n\n\n13TH: A Conversation with Oprah Winfrey & Ava ‚Ä¶\n\n\nNaN\n\n\nUncategorized\n\n\n2017\n\n\n37\n\n\n\n\n101\n\n\n3 Seconds Divorce\n\n\nCanada\n\n\nDocumentaries\n\n\n2018\n\n\n53\n\n\n\n\n146\n\n\nA 3 Minute Hug\n\n\nMexico\n\n\nDocumentaries\n\n\n2019\n\n\n28\n\n\n\n\n162\n\n\nA Christmas Special: Miraculous: Tales of Lady‚Ä¶\n\n\nFrance\n\n\nUncategorized\n\n\n2016\n\n\n22\n\n\n\n\n171\n\n\nA Family Reunion Christmas\n\n\nUnited States\n\n\nUncategorized\n\n\n2019\n\n\n29\n\n\n\n\n177\n\n\nA Go! Go! Cory Carson Christmas\n\n\nUnited States\n\n\nChildren\n\n\n2020\n\n\n22\n\n\n\n\n178\n\n\nA Go! Go! Cory Carson Halloween\n\n\nNaN\n\n\nChildren\n\n\n2020\n\n\n22\n\n\n\n\n179\n\n\nA Go! Go! Cory Carson Summer Camp\n\n\nNaN\n\n\nChildren\n\n\n2020\n\n\n21\n\n\n\n\n181\n\n\nA Grand Night In: The Story of Aardman\n\n\nUnited Kingdom\n\n\nDocumentaries\n\n\n2015\n\n\n59\n\n\n\n\n200\n\n\nA Love Song for Latasha\n\n\nUnited States\n\n\nDocumentaries\n\n\n2020\n\n\n20\n\n\n\n\n220\n\n\nA Russell Peters Christmas\n\n\nCanada\n\n\nStand-Up\n\n\n2011\n\n\n44\n\n\n\n\n233\n\n\nA StoryBots Christmas\n\n\nUnited States\n\n\nChildren\n\n\n2017\n\n\n26\n\n\n\n\n237\n\n\nA Tale of Two Kitchens\n\n\nUnited States\n\n\nDocumentaries\n\n\n2019\n\n\n30\n\n\n\n\n242\n\n\nA Trash Truck Christmas\n\n\nNaN\n\n\nChildren\n\n\n2020\n\n\n28\n\n\n\n\n247\n\n\nA Very Murray Christmas\n\n\nUnited States\n\n\nComedies\n\n\n2015\n\n\n57\n\n\n\n\n285\n\n\nAbominable Christmas\n\n\nUnited States\n\n\nChildren\n\n\n2012\n\n\n44\n\n\n\n\n295\n\n\nAcross Grace Alley\n\n\nUnited States\n\n\nDramas\n\n\n2013\n\n\n24\n\n\n\n\n305\n\n\nAdam Devine: Best Time of Our Lives\n\n\nUnited States\n\n\nStand-Up\n\n\n2019\n\n\n59\n\n\n\n\n\ncolors = []\nfor lab, row in netflix_movies.iterrows():\n    if row['genre'] == \"Children\":\n        colors.append(\"Blue\")\n    elif row['genre'] == \"Documentaries\":\n        colors.append(\"Red\")\n    elif row['genre'] == \"Stand-Up\":\n        colors.append(\"Green\")\n    else:\n        colors.append(\"Black\")\ncolors[:10]\n['Black',\n 'Black',\n 'Black',\n 'Black',\n 'Black',\n 'Black',\n 'Black',\n 'Black',\n 'Black',\n 'Red']\nfig = plt.figure(figsize=(12, 8))\n&lt;Figure size 1200x800 with 0 Axes&gt;\nplt.scatter(netflix_movies['release_year'], netflix_movies['duration'], c=colors)\n&lt;matplotlib.collections.PathCollection at 0x7f49c4f3c430&gt;\n\n\n\npng\n\n\nplt.title(\"Movie Duration by Year of Release\")\nplt.xlabel(\"Release year\")\nplt.ylabel(\"Duration (min)\")\nText(0, 0.5, 'Duration (min)')\n\n\n\npng\n\n\nplt.show()\nanswer = \"no\"\n# Importing pandas and matplotlib\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Read in the Netflix CSV as a DataFrame\nnetflix_df = pd.read_csv(\"netflix_data.csv\")\n\n# Subset the DataFrame for type \"Movie\"\nnetflix_subset = netflix_df[netflix_df[\"type\"] == \"Movie\"]\n\n# Select only the columns of interest\nnetflix_movies = netflix_subset[[\"title\", \"country\", \"genre\", \"release_year\", \"duration\"]]\n\n# Filter for durations shorter than 60 minutes\nshort_movies = netflix_movies[netflix_movies.duration &lt; 60]\n\n# Define an empty list\ncolors = []\n\n# Iterate over rows of netflix_movies\nfor label, row in netflix_movies.iterrows() :\n    if row[\"genre\"] == \"Children\" :\n        colors.append(\"red\")\n    elif row[\"genre\"] == \"Documentaries\" :\n        colors.append(\"blue\")\n    elif row[\"genre\"] == \"Stand-Up\":\n        colors.append(\"green\")\n    else:\n        colors.append(\"black\")\n        \n# Inspect the first 10 values in your list        \ncolors[:10]\n\n# Set the figure style and initalize a new figure\nfig = plt.figure(figsize=(12,8))\n\n# Create a scatter plot of duration versus release_year\nplt.scatter(netflix_movies.release_year, netflix_movies.duration, c=colors)\n\n# Create a title and axis labels\nplt.title(\"Movie Duration by Year of Release\")\nplt.xlabel(\"Release year\")\nplt.ylabel(\"Duration (min)\")\n\n# Show the plot\nplt.show()\n\n# Are we certain that movies are getting shorter?\nanswer = \"no\"\n\n\n\npng",
    "crumbs": [
      "Data Science and Machine Learning",
      "Netflix Movies"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/titanic.html#general-overview",
    "href": "pages/projects/data_science/posts/titanic.html#general-overview",
    "title": "Titanic Disaster",
    "section": "General Overview",
    "text": "General Overview\nThe Kaggle Titanic dataset and ML competition is one that many people are familiar with, and if they‚Äôre like me, it was also their first ML project. I redid this after 3 years to get familiar with my current workflow of using git, notebooks, venvs, etc. Below I included some notes to myself. While it was overkill, I used a Dockerized environment for this project, just to increase my familiarity with containers and the docker toolset.",
    "crumbs": [
      "Data Science and Machine Learning",
      "Titanic Disaster"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/titanic.html#notes-on-the-titanic-model-and-the-process-for-using-the-jupyter-kernel-on-the-ec2-server",
    "href": "pages/projects/data_science/posts/titanic.html#notes-on-the-titanic-model-and-the-process-for-using-the-jupyter-kernel-on-the-ec2-server",
    "title": "Titanic Disaster",
    "section": "Notes on the Titanic model and the process for using the Jupyter Kernel on the EC2 Server",
    "text": "Notes on the Titanic model and the process for using the Jupyter Kernel on the EC2 Server\n\n\nFirst, navigate to Local-Scripts/.AWS/.EC2_Scripts and run the ec2_start.sh script zsh ec2_start.sh\nNext, execute the unix_test_dns.sh script to store the EC2 public DNS in the /etc/hosts file ./unix_test_dns.sh\nUse the ssh -i command to connect to the EC2 server, then run the Jupyter kernel image docker run -p 8888:8888 titanic-env Look into adding a volume mount command here to persist model/file changes in the EC2\nNow that it‚Äôs running. Use a different terminal window (or the VS Code IDE) and test the DNS name. ping unix_test.local Need to make this DNS dynamic\nIn VS Code, open the .ipynb file in the model folder and continue work.\n\nIf you need to reconnect to a kernel, use the Titanic preset.\nVS Code connects to the EC2 IPv4 address, even though the Kernel tells you 127.0.0.1\n\nThe format for connecting to the Public IPv4 is http://IPv4:8888/\nTo pull the file out of the container and store it on the EC2 server\n\ndocker cp 786853360d97:/home/files/titanic_submission.csv files/titanic_submission.csv\ndocker copy instance-id:/path/to/file local/path\n\n\n\n\nIf you need to modify the container\n\nDo so locally, or anywhere, and then push the change to the GitHub repoistory\nThen, pull the changes into the EC2 server\nClear the Docker library/cache, and then rebuild the image from scratch, use the following docker build -t titanic-env -f .config/Dockerfile .\nEnsure this is done from the main project folder and uses those flags\n\nFor Titanic, this is in the admin/Kaggle/Titanic folder in the EC2 instance\n--no-cache ensures it‚Äôs a fresh build (This will take a while, not worth it in the smaller environment. Rebuild with cache)\n-t sets the name of the image\n-f lets you specify the Dockerfile location\n. lets Docker know that your current working directory is where the build context should take place",
    "crumbs": [
      "Data Science and Machine Learning",
      "Titanic Disaster"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/titanic.html#the-code-portion-of-this-notebook",
    "href": "pages/projects/data_science/posts/titanic.html#the-code-portion-of-this-notebook",
    "title": "Titanic Disaster",
    "section": "The Code Portion of this notebook",
    "text": "The Code Portion of this notebook\n\n\n\n\n\n\nCaution\n\n\n\nWhen I most recently completed this competition, I didn‚Äôt do it with the goal in mind of doing a nice write-up. This is really just an amalgamation of the notes I made to myself on how to use/modify the Docker container I ran my model on and the actual notebook + code + notes.\n\n\nimport numpy as np\nimport pandas as pd\nimport duckdb\nimport sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n\n# Initialize a connection and create a persistent database\n# Worth noting that due to the workflow I'm using, the database was/should be created externally, and then built into the Docker container\n# This way, the raw database files are saved locally, but file size won't grow exponentially\ncon = duckdb.connect(\"files/titanic.duckdb\")\n# Using this to DROP and Recreate train_raw, ensuring a fresh process\ncon.sql(\"DROP TABLE train_raw;\")\ncon.sql(\"DROP TABLE test_raw;\")\ncon.sql(\"CREATE TABLE train_raw AS SELECT * FROM 'files/train.csv'\")\ncon.sql(\"CREATE TABLE test_raw AS SELECT * FROM 'files/test.csv'\")\n# Create working tables\n#con.sql(\"DROP TABLE train;\")\n#con.sql(\"DROP TABLE test;\")\ncon.sql(\"CREATE TABLE train AS SELECT * FROM train_raw\")\ncon.sql(\"CREATE TABLE test AS SELECT * FROM test_raw\")\n# Verify the proper tables are loaded\ncon.sql(\"SELECT * FROM duckdb_tables()\")\n# Generate summary statistics\ncon.sql(\"SUMMARIZE train\")\n#con.sql(\"SUMMARIZE test\")\n# Examine Nulls for the Age, Cabin, and Embarked columns (do this for test as well)\ncon.sql(\"SELECT * FROM train WHERE Age IS NULL\") # Seems to make the most sense to use the average age here\ncon.sql(\"SELECT * FROM train WHERE Cabin IS NULL\") # Seems likely Cabins not as strictly recorded for lower class guests, probably unnecessary for model\ncon.sql(\"SELECT * FROM train WHERE Embarked IS NULL\") # This only comprises 2 records and it's unclear if they made it on in the first place, not a high enough percentage of 1st class survivors to consider keeping\n# Update the Age column, replace NULL values with the average Age\ncon.sql(\"\"\"UPDATE train AS train_clean\n        SET Age = (\n            SELECT\n                avg(raw.Age) AS cleanAge\n            FROM train as raw\n            WHERE raw.Age IS NOT NULL\n        )\n        WHERE Age IS NULL\"\"\")\n# Update the Sex column, change the VARCHAR type to BOOLEAN\ncon.sql(\"\"\"ALTER TABLE train ALTER Sex \n        SET DATA TYPE BOOLEAN USING CASE\n            WHEN Sex = 'female' THEN 1 ELSE 0 END\n        \"\"\")\n# Update the Age column in the test dataset\ncon.sql(\"\"\"UPDATE test AS test_clean\n        SET Age = (\n            SELECT\n                avg(raw.Age) AS cleanAge\n            FROM test as raw\n            WHERE raw.Age IS NOT NULL\n        )\n        WHERE Age IS NULL\"\"\")\n# Update the Sex column, change the VARCHAR type to BOOLEAN\ncon.sql(\"\"\"ALTER TABLE test ALTER Sex \n        SET DATA TYPE BOOLEAN USING CASE\n            WHEN Sex = 'female' THEN 1 ELSE 0 END\n        \"\"\")\n# Remove the PassengerId, Name, Cabin, Embarked, Fare, and Ticket columns\ncon.sql(\"ALTER TABLE train DROP PassengerId\") # Has no bearing on the outcome of the model\ncon.sql(\"ALTER TABLE train DROP Name\") # Has to be numeric data\ncon.sql(\"ALTER TABLE train DROP Cabin\")\ncon.sql(\"ALTER TABLE train DROP Embarked\")\ncon.sql(\"ALTER TABLE train DROP Fare\") # Dropping because there are nulls in the test file\ncon.sql(\"ALTER TABLE train DROP Ticket\") # Dropping because of inconsistent values\n\n# Remove the PassengerId, Name, Cabin, Embarked, Fare, and Ticket columns\ncon.sql(\"ALTER TABLE test DROP Name\") # Has to be numeric data\ncon.sql(\"ALTER TABLE test DROP Cabin\")\ncon.sql(\"ALTER TABLE test DROP Embarked\")\ncon.sql(\"ALTER TABLE test DROP Fare\") # Dropping because there are nulls in the test file\ncon.sql(\"ALTER TABLE test DROP Ticket\") # Dropping because of inconsistent values\n\n# Creating dataframes for testing/training, I'll be using sklearn here, which needs both\ntrain = con.sql(\"SELECT * FROM train\").df()\ntest = con.sql(\"SELECT * FROM test\").df()\n# Create features and target\nX = train.drop(\"Survived\", axis = 1).values\ny = train[\"Survived\"].values\n\nX_test = test.drop(\"PassengerId\", axis = 1).values\n# Initialize Regression object and split data\nlogreg = LogisticRegression(penalty = 'l2', tol = np.float64(0.083425), C = np.float64(0.43061224489795924), class_weight = 'balanced')\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.7, random_state = 123)\n# Fit and predict\nlogreg.fit(X_train, y_train)\n\n\n\nLogisticRegression(C=np.float64(0.43061224489795924), class_weight='balanced',\n                   tol=np.float64(0.083425))\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n¬†¬†LogisticRegression?Documentation for LogisticRegressioniFitted\n\nLogisticRegression(C=np.float64(0.43061224489795924), class_weight='balanced',\n                   tol=np.float64(0.083425))\n\n\n\n\n\n# Predict and measure output\ny_pred = logreg.predict(X_val)\ny_pred_probs = logreg.predict_proba(X_val)[:, 1]\nprint(roc_auc_score(y_val, y_pred_probs))\n0.8149262043998886\n# Create Parameter Dictionary for Model Tuning\nkf = KFold(n_splits = 5, shuffle = True, random_state = 123)\nparams = {\n    \"penalty\": [\"l1\", \"l2\"],\n    \"tol\": np.linspace(0.0001, 1.0, 25),\n    \"C\": np.linspace(0.1, 1.0, 50),\n    \"class_weight\": [\"balanced\", {0:0.8, 1:0.2}]\n}\nlogreg_cv = RandomizedSearchCV(logreg, params, cv=kf)\n# Run the parameter search, fit the object, print the output\nlogreg_cv = RandomizedSearchCV(logreg, params, cv=kf)\nlogreg_cv.fit(X_train, y_train)\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\nprint(\"Tuned Logistic Regression Best Accuracy Score: {}\".format(logreg_cv.best_score_))\n# Apply the model to the test set\npredictions = logreg.predict(X_test)\n\nsubmission = pd.DataFrame({\n    'PassengerId': test['PassengerId'],\n    'Survived': predictions\n})\n\nsubmission.head()\n\n\n\n\n\n\n\n\nPassengerId\n\n\nSurvived\n\n\n\n\n\n\n0\n\n\n892\n\n\n0\n\n\n\n\n1\n\n\n893\n\n\n1\n\n\n\n\n2\n\n\n894\n\n\n0\n\n\n\n\n3\n\n\n895\n\n\n0\n\n\n\n\n4\n\n\n896\n\n\n1\n\n\n\n\n\n# Write the file to .csv and submit\ncon.sql(\"SELECT * FROM submission\").write_csv(\"files/titanic_submission.csv\")",
    "crumbs": [
      "Data Science and Machine Learning",
      "Titanic Disaster"
    ]
  },
  {
    "objectID": "pages/blogs/posts/first_blog.html",
    "href": "pages/blogs/posts/first_blog.html",
    "title": "1. Experimenting with GitHub Pages and WhiteWind",
    "section": "",
    "text": "This is the first part of a blog post I originally wrote over a month ago and posted to WhiteWind.\nWhiteWind is a blogging application built on Bluesky‚Äôs foundational, decentralied network. Half of that post is now the beginning of the NFL Big Data Bowl 2025 project write up.\nEventually, I‚Äôd like to find a way to write a post on WhiteWind and have it automatically update my personal site. That way I can do quick, shower thoughts style blog posts there, without having to do the longer Quarto workflow.",
    "crumbs": [
      "1. Experimenting with GitHub Pages and WhiteWind"
    ]
  },
  {
    "objectID": "pages/blogs/posts/first_blog.html#original-post",
    "href": "pages/blogs/posts/first_blog.html#original-post",
    "title": "1. Experimenting with GitHub Pages and WhiteWind",
    "section": "Original Post",
    "text": "Original Post\nIn March of 2024, I took a break from social media (Twitter/Instagram/TikTok/Etc.) because the polarizing algorithms and toxicity were driving me nuts. I took to LinkedIn, in search of a community of professional data nerds, that develop and learn in their freetime. Luckily, I found some great people on there, but my feed was quickly inundated with LinkedInfluencers reposting the same content whether recycling their own, other‚Äôs, or generating posts with AI. Then, in October, I heard that data people were all jumping over to Bluesky, so I thought I‚Äôd give it a shot. Needless to say, I love the data and developer community here. Furthermore, the fundamental design and decentralization of the platform make this that much cooler. Then, I learned about WhiteWind and some other integrations, and decided this is a great way to post updates and keep myself accountable.\nA little about me, I‚Äôm a Tulane Unviersity graduate (BSM ‚Äô20, MS ‚Äô21) and currently a Data (Analytics) Engineer at GM. In my current role I wear many hats! In my 3.5 years at General Motors I‚Äôve gained experience desinging database architecture, building ETL pipelines, being an IT admin, owning products, managing projects, automating tests, and providing training/mentorship for Power BI/Databricks; however, most of my experience is in BI reporting and legacy application upgrades.\nSo, this past April I decided I wanted to really learn the ins and outs of modern data engineering and data science. So, I bought a MacBook Air and started coding! From my Masters degree at Tulane, I had experience with R, Python, SQL for coding, as well as the statistical knwoledge needed for machine learning. Instead of those areas, I began with zsh/bash scripting (using the Terminal to do everything I could), then I learned the basics of git for version control, then I jumped into dbt for data modeling, and finally Docker so I could understand containers. Following that, I began Harvard‚Äôs CS50p course in Python programming. I wanted to understand Python, the programming language, because I had learned Python using Jupyter notebooks instead of scripts, packages, testing, etc.\nIn the few month or so since then, I‚Äôve worked on a few public projects which you can view on my GitHub. These projects are pretty varied, and narrow in scope.\n\nDataCamp projects based on simple ETL, analytics, or programming.\nMy CS50p lecture notes and problem set submissions.\nA simple Introduction to DuckDB using the NFL Big Data Bowl CSV files.\nA private repository with bash scripts and configuration files for starting up my AWS EC2 instance, connecting via ssh, Docker containers, quarto, and the GitHub CLI.",
    "crumbs": [
      "1. Experimenting with GitHub Pages and WhiteWind"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chris Kornaros",
    "section": "",
    "text": "This site is the personal portfolio and homepage for Chris Kornaros. I‚Äôm a Tulane Graduate and professional Data Engineer. On here, you‚Äôll find projects and code samples that I can share publicly, guides for various tools or workflows, journal or blog posts, and any independent research or professional updates (including my resume).\nThis website is a work in progress, so things are going to move around and break. Additionally, there is a lot that I have to still add: past guides, projects, etc. Follow me on social media (links above) to stay up to date with what I‚Äôm working on. To learn more about my background visit About, for guides on various open source tools visit Guides, to see my current and past (public) work visit Projects, and to see any blog posts (available on WhiteWind) visit Blogs."
  },
  {
    "objectID": "pages/blogs/blogs.html",
    "href": "pages/blogs/blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "A landing page for various blogs, journals, or random thoughts. Some of these will be focused on specific tools or technology, others will be random thoughts or research notes.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 14, 2024\n\n\n1. Experimenting with GitHub Pages and WhiteWind\n\n\nChris Kornaros\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Blogs"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/gym_market.html",
    "href": "pages/projects/data_science/posts/gym_market.html",
    "title": "Gym Market Analysis",
    "section": "",
    "text": "gym\n\n\nYou are a product manager for a fitness studio and are interested in understanding the current demand for digital fitness classes. You plan to conduct a market analysis in Python to gauge demand and identify potential areas for growth of digital products and services.\n\nThe Data\nYou are provided with a number of CSV files in the ‚ÄúFiles/data‚Äù folder, which offer international and national-level data on Google Trends keyword searches related to fitness and related products.\n\n\nworkout.csv\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\n'month'\nMonth when the data was measured.\n\n\n'workout_worldwide'\nIndex representing the popularity of the keyword ‚Äòworkout‚Äô, on a scale of 0 to 100.\n\n\n\n\n\nthree_keywords.csv\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\n'month'\nMonth when the data was measured.\n\n\n'home_workout_worldwide'\nIndex representing the popularity of the keyword ‚Äòhome workout‚Äô, on a scale of 0 to 100.\n\n\n'gym_workout_worldwide'\nIndex representing the popularity of the keyword ‚Äògym workout‚Äô, on a scale of 0 to 100.\n\n\n'home_gym_worldwide'\nIndex representing the popularity of the keyword ‚Äòhome gym‚Äô, on a scale of 0 to 100.\n\n\n\n\n\nworkout_geo.csv\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\n'country'\nCountry where the data was measured.\n\n\n'workout_2018_2023'\nIndex representing the popularity of the keyword ‚Äòworkout‚Äô during the 5 year period.\n\n\n\n\n\nthree_keywords_geo.csv\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\n'country'\nCountry where the data was measured.\n\n\n'home_workout_2018_2023'\nIndex representing the popularity of the keyword ‚Äòhome workout‚Äô during the 5 year period.\n\n\n'gym_workout_2018_2023'\nIndex representing the popularity of the keyword ‚Äògym workout‚Äô during the 5 year period.\n\n\n'home_gym_2018_2023'\nIndex representing the popularity of the keyword ‚Äòhome gym‚Äô during the 5 year period.\n\n\n\n# Import the necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# Start coding here\nimport pandas as pd\n\nworkout = pd.read_csv('data/workout.csv')\nthree_kw = pd.read_csv('data/three_keywords.csv')\nworkout_geo = pd.read_csv('data/workout_geo.csv')\nkw_geo = pd.read_csv('data/three_keywords_geo.csv')\nworkout.head()\n\n\n\n\n\n\n\n\nmonth\n\n\nworkout_worldwide\n\n\n\n\n\n\n0\n\n\n2018-03\n\n\n59\n\n\n\n\n1\n\n\n2018-04\n\n\n61\n\n\n\n\n2\n\n\n2018-05\n\n\n57\n\n\n\n\n3\n\n\n2018-06\n\n\n56\n\n\n\n\n4\n\n\n2018-07\n\n\n51\n\n\n\n\n\npeak = workout.loc[workout['workout_worldwide'].idxmax()]\nyear_str = peak.str.split('-')[0][0]\nyear_str\n'2020'\nworkout.dtypes\nmonth                object\nworkout_worldwide     int64\ndtype: object\ncovid = workout.loc[(workout['month'] &gt; '2019-12') & (workout['month'] &lt;= '2022-12')]\npost_covid = workout.loc[workout['month'] &gt; '2022-12']\npeak_covid = three_kw.loc[(workout['month'] &gt; '2019-12') & (workout['month'] &lt;= '2022-12')][['home_workout_worldwide', 'gym_workout_worldwide', 'home_gym_worldwide']].max().idxmax()\ncurrent = three_kw.loc[workout['month'] &gt; '2022-12'][['home_workout_worldwide', 'gym_workout_worldwide', 'home_gym_worldwide']].max().idxmax()\npeak_covid\ncurrent\n'gym_workout_worldwide'\ntop_country = workout_geo.loc[workout_geo['workout_2018_2023'].idxmax()]['country']\ntop_country\n'United States'\nkw_geo1 = kw_geo.loc[(kw_geo['Country']=='Philippines') | (kw_geo['Country']=='Malaysia')]\nkw_geo1\n\n\n\n\n\n\n\n\nCountry\n\n\nhome_workout_2018_2023\n\n\ngym_workout_2018_2023\n\n\nhome_gym_2018_2023\n\n\n\n\n\n\n23\n\n\nPhilippines\n\n\n52.0\n\n\n38.0\n\n\n10.0\n\n\n\n\n61\n\n\nMalaysia\n\n\n47.0\n\n\n38.0\n\n\n15.0\n\n\n\n\n\nhome_workout_geo = kw_geo1.loc[kw_geo1['home_workout_2018_2023'] == kw_geo1['home_workout_2018_2023'].max(), 'Country'].values[0]\nhome_workout_geo\n'Philippines'",
    "crumbs": [
      "Data Science and Machine Learning",
      "Gym Market Analysis"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html",
    "title": "NFL Big Data Bowl 2025",
    "section": "",
    "text": "I will add to this later. Currently, this is a rough combination of my early notebooks.",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#introduction",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#introduction",
    "title": "NFL Big Data Bowl 2025",
    "section": "",
    "text": "I will add to this later. Currently, this is a rough combination of my early notebooks.",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#ongoing-thoughts",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#ongoing-thoughts",
    "title": "NFL Big Data Bowl 2025",
    "section": "Ongoing Thoughts",
    "text": "Ongoing Thoughts\nThis is the first time I‚Äôm adding to a unified document, it‚Äôs December 13th, or about 1 month into my project. As of now, Random Forest definitely seems like the best path forward; however, the intial version certainly overfit. I believe the model overfit because some of the plays columns are the pre/post snap home/away team win probability values. In my next iteration, I‚Äôm going to remove those values, and in the future I might even try to recreate them. That being said, there‚Äôs a little under one month to go, so I‚Äôm going to focus on putting together some kind of deliverable/submission, before I go off the deep end. That said, this page and the website in general are going to be sloppy as I figure things out and slowly improve the organization and UI.",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#exploratory-data-analysis-and-initial-thoughts",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#exploratory-data-analysis-and-initial-thoughts",
    "title": "NFL Big Data Bowl 2025",
    "section": "Exploratory Data Analysis and Initial Thoughts",
    "text": "Exploratory Data Analysis and Initial Thoughts\n\n\n\n\n\n\nNote\n\n\n\nThis was written on November 26th, 2024. It was later added to this site on December 13th, 2024. This is a general write up on the project, you can see the full notebooks below. The full repo is available here.\n\n\nCurrently, I‚Äôve made solid progress with my initial exploratory data analysis and project configuration. Here are some quick notes about the setup of my project environment (from IDE to tools/versions). - Using VS Code with the Jupyter, Jinja, YAML, Quarto (for notes/project submissions), and dbt extensions. - DuckDB is my primary database tool (for now), with dbt for the data modeling - Then, I‚Äôm using Python and Jupyter Notebooks for the analysis/ML component\nThe reason I may switch to PostgreSQL for the primary Database is to just gain experience with DuckDB as a DEV environement and Postgres for PROD. Realistically, however, for the scope of this project DuckDB accomplishes everything I need it to.\nFor the forseeable future, the only side project I‚Äôll be working on is this, so my next few posts will only look at the project progress and my thoughts about the Big Data Bowl, feel free to checkout the GitHub repository where I‚Äôm saving my work.\nSome notes about my current project progress: - The project folder has a few subdirectories, including nfl_dbt which is the dbt project folder - The raw data came in the form of 13 CSVs from Kaggle. 4 of which are 50mb or less, 9 of which are ~1gb. - I‚Äôm using Databricks‚Äô ‚ÄúMedallion Architecture‚Äù to guide my data modeling workflow. - I built the initial dbt models, using DuckDB as the DEV target (enabling 4 threads) and loaded the ‚Äúbronze‚Äù schema which contains the 13 raw tables - I aggregated the data into the ‚Äúsilver‚Äù schema, which contains an aggregated play data table - I further aggregated the data into the ‚Äúgold‚Äù schema, which provides basic analytic tables - Currently, I completed an initial analysis using an EDA notebook where I looked at using a LinearRegression and KNN to compare pre-snap play data with play outcomes. - I settled on a KNN model, but I‚Äôm only seeing about a 61.1% accuracy rate (confusion matrix and explanation below).\nSo, I‚Äôm at a bit of a crossroads, with a few ways forward. It may be simpler (for the initial project/submission) to build a linear regression model that takes pre-snap play data as features, and then looks at yards gained (or loss) for the output. Conversely, if I stick with the KNN model I‚Äôll need to make some changes. The majority of the outputs are either Gain or Completed, which refer to a positive rushing play and a completed pass, respectively. The issue here, the model overwhelmingly predicts those values, but fails to accurately predict things like Touchdowns, Sacks, or Interceptions.\nSo, I may need to limit possible play outcomes, or at least combine some categories (i.e.¬†Turnover for Fumble + Interception). Or, add some more presnap data, such as down and distance (I currently only use starting yard line, along with categorical data). If you made it this far, thank you! Below is the confusion matrix output from my current KNN model. I‚Äôll add some hashtags at the end as an experiment too, because I‚Äôm not sure if that will help with post discoverability and/or integrate with Bluesky feeds.",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#knn-classifier-notebook-first-model",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#knn-classifier-notebook-first-model",
    "title": "NFL Big Data Bowl 2025",
    "section": "KNN Classifier Notebook (First Model)",
    "text": "KNN Classifier Notebook (First Model)\n# Import dependencies\nimport duckdb\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n# Open the connection to the persistent database\ncon = duckdb.connect(\".config/nfl.duckdb\")\ncon.close()\n# Create the initial dataframe object with DuckDB\ndf = con.sql(\"\"\"\n    SELECT *\n    FROM gold.plays_numeric             \n\"\"\").df()\n\ndf.head()\n\n\n\n\n\n\n\n\ngameId\n\n\nplayId\n\n\npossessionTeam\n\n\nyardlineNumber\n\n\noffenseFormation\n\n\nreceiverAlignment\n\n\nplayType\n\n\ndefensiveFormation\n\n\npff_manZone\n\n\nyardsGained\n\n\nplayOutcome\n\n\n\n\n\n\n0\n\n\n2022102302\n\n\n2655\n\n\nCIN\n\n\n21\n\n\n3\n\n\n8\n\n\n2\n\n\n6\n\n\n2\n\n\n9\n\n\n3\n\n\n\n\n1\n\n\n2022091809\n\n\n3698\n\n\nCIN\n\n\n8\n\n\n3\n\n\n8\n\n\n2\n\n\n13\n\n\n2\n\n\n4\n\n\n3\n\n\n\n\n2\n\n\n2022103004\n\n\n3146\n\n\nHOU\n\n\n20\n\n\n6\n\n\n5\n\n\n2\n\n\n13\n\n\n2\n\n\n6\n\n\n3\n\n\n\n\n3\n\n\n2022110610\n\n\n348\n\n\nKC\n\n\n23\n\n\n6\n\n\n5\n\n\n2\n\n\n13\n\n\n2\n\n\n4\n\n\n3\n\n\n\n\n4\n\n\n2022102700\n\n\n2799\n\n\nBAL\n\n\n27\n\n\n4\n\n\n7\n\n\n1\n\n\n3\n\n\n1\n\n\n-1\n\n\n2\n\n\n\n\n\n# Split the table into features and target\nX = con.sql(\"\"\"\n    SELECT yardlineNumber, offenseFormation, receiverAlignment, playType, defensiveFormation, pff_manZone\n    FROM gold.plays_numeric\n\"\"\").df()\n\ny = np.array(con.sql(\"\"\"\n    SELECT playOutcome\n    FROM gold.plays_numeric\n\"\"\").df()).ravel()\n\nprint(X.shape, y.shape)\n(16124, 6) (16124,)\n# Instantiate the model and split the datasets into training/testing\nknn = KNeighborsClassifier(n_neighbors=7)\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.7, random_state=123)\n# Fit the model\nknn.fit(X_train, y_train)\n\n\n\nKNeighborsClassifier(n_neighbors=7)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n¬†¬†KNeighborsClassifier?Documentation for KNeighborsClassifieriFitted\n\nKNeighborsClassifier(n_neighbors=7)\n\n\n\n\n\n# Basic KNN Performance Metrics\ny_pred = knn.predict(X_val)\n\nprint(knn.score(X_val, y_val))\n0.6114096734187681\n# Datacamp Model performance Loop\n# Create neighbors\nneighbors = np.arange(1, 13)\ntrain_accuracies = {}\ntest_accuracies = {}\n\nfor neighbor in neighbors:\n  \n    # Set up a KNN Classifier\n    knn = KNeighborsClassifier(n_neighbors=neighbor)\n  \n    #¬†Fit the model\n    knn.fit(X_train, y_train)\n  \n    # Compute accuracy\n    train_accuracies[neighbor] = knn.score(X_train, y_train)\n    test_accuracies[neighbor] = knn.score(X_val, y_val)\nprint(neighbors, '\\n', train_accuracies, '\\n', test_accuracies)\n# Visualize model accuracy with various neighbors\n# Add a title\nplt.title(\"KNN: Varying Number of Neighbors\")\n\n#¬†Plot training accuracies\nplt.plot(neighbors, train_accuracies.values(), label=\"Training Accuracy\")\n\n# Plot test accuracies\nplt.plot(neighbors, test_accuracies.values(), label=\"Testing Accuracy\")\n\nplt.legend()\nplt.xlabel(\"Number of Neighbors\")\nplt.ylabel(\"Accuracy\")\n\n# Display the plot\nplt.show()\n# Map the original target variables to the KNN outputs\nplay_outcome_map = con.sql(\"\"\"\n    SELECT\n    CASE\n        WHEN playOutcome = 1 THEN 'Gain'\n        WHEN playOutcome = 2 THEN 'Loss'\n        WHEN playOutcome = 3 THEN 'Completed'\n        WHEN playOutcome = 4 THEN 'Incomplete'\n        WHEN playOutcome = 5 THEN 'Scrambled'\n        WHEN playOutcome = 6 THEN 'Touchdown'\n        WHEN playOutcome = 7 THEN 'Intercepted'\n        WHEN playOutcome = 8 THEN 'Fumbled'\n        WHEN playOutcome = 9 THEN 'Sacked'\n        WHEN playOutcome = 0 THEN 'Penalty'\n        ELSE 'Unknown'  -- Optional, in case there are values not matching any condition\n    END AS playOutcome\nFROM gold.plays_numeric\n\"\"\").df()['playOutcome'].tolist()\n\nplay_outcome_map = np.unique(play_outcome_map).tolist()\n# Create a dictionary to map playOutcome values to corresponding labels\nplay_outcome_dict = {i: play_outcome_map[i] for i in range(len(play_outcome_map))}\n\n# Generate a colormap for the string labels (use 'viridis' colormap)\ncolors = plt.cm.viridis(np.linspace(0, 1, len(play_outcome_map)))\nplay_colors = dict(zip(range(len(play_outcome_map)), colors))\n\n# Create legend patches for each class label\nlegend_patches = [mpatches.Patch(color=play_colors[i], label=play_outcome_map[i]) for i in range(len(play_outcome_map))]\n\n# Assuming `y_pred` is a list of predictions, map numeric predictions to string labels\npred_labels = [play_outcome_dict[val] for val in y_pred]\n# Attempting to conduct sensitivity analysis for feature importance\nfor feature in range(6):\n    plt.figure(figsize=(10, 6))\n    plt.scatter(X_val.iloc[:, feature], y_pred, c=[play_colors[val] for val in y_pred], cmap='viridis', edgecolor='k')\n    plt.xlabel(f\"Feature {feature + 1}\")\n    plt.ylabel(\"Predicted Class\")\n    plt.yticks(range(len(play_outcome_map)), play_outcome_map)\n    plt.title(f\"Predictions by Feature {feature + 1}\")\n    plt.legend(handles = legend_patches, title=\"Actual Class\", bbox_to_anchor=(1.05, 1), loc = 'upper left')\n    plt.tight_layout\n    plt.show()\n# Your play_outcome_dict with correct mapping\nplay_outcome_dict = {\n    1: 'Gain',\n    2: 'Loss',\n    3: 'Completed',\n    4: 'Incomplete',\n    5: 'Scrambled',\n    6: 'Touchdown',\n    7: 'Intercepted',\n    8: 'Fumbled',\n    9: 'Sacked',\n    0: 'Penalty'\n}\n\n# Map the y_pred values to the corresponding labels\npred_labels = [play_outcome_dict[val] for val in y_pred]\n\n# Define the colormap based on the labels\nplay_colors = plt.cm.viridis(np.linspace(0, 1, len(play_outcome_dict)))\n\n# Combine your features (X_val) and the predictions (y_pred) into a single DataFrame\ndf_features = X_val.copy()\ndf_features['Predicted Class'] = [play_outcome_dict[key] for key in y_pred]\n\n# Create a pairplot to visualize pairwise relationships between all features\nsns.pairplot(df_features, hue='Predicted Class', palette=dict(zip(play_outcome_dict.values(), play_colors)), markers='o')\n\n# Customize the plot\nplt.suptitle('Pairplot of Features Colored by Predicted Class', y=1.02)\nplt.legend(handles = legend_patches, title=\"Actual Class\", bbox_to_anchor=(1.05, 1), loc = 'upper left')\nplt.tight_layout()\nplt.show()\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Assuming y_true contains the true labels and y_pred contains the predicted labels\n# Map numerical values to their respective class labels\ny_true_labels = [play_outcome_dict[val] for val in y_val]  # Replace y_true with your actual true labels\ny_pred_labels = [play_outcome_dict[val] for val in y_pred]\n\n# Generate the confusion matrix\ncm = confusion_matrix(y_true_labels, y_pred_labels, labels=list(play_outcome_dict.values()))\n\n# Visualize the confusion matrix\nfig, ax = plt.subplots(figsize=(10, 8))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(play_outcome_dict.values()))\ndisp.plot(cmap='viridis', ax=ax, xticks_rotation=45)\n\n# Customize the plot\nplt.title(\"Confusion Matrix of KNN Model\")\nplt.show()\n\n\n\nKNN Classifier Confusion Matrix",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#linear-regression-notebook-second-model",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#linear-regression-notebook-second-model",
    "title": "NFL Big Data Bowl 2025",
    "section": "Linear Regression Notebook (Second Model)",
    "text": "Linear Regression Notebook (Second Model)\nimport duckdb\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n# Open the DuckDB connection, to the persistent database\ncon = duckdb.connect(\".config/nfl.duckdb\")\ncon.close()\n# Test converting the play outcomes to just yards gained or lost\ncon.sql(\"\"\"\n    SELECT *\n    FROM gold.plays_numeric   \n\"\"\")\n# Can still utilize plays_numeric, just won't use the categorical outcomes as the target\nX = con.sql(\"\"\"\n    SELECT yardlineNumber, offenseFormation, receiverAlignment, playType, defensiveFormation, pff_manZone\n    FROM gold.plays_numeric   \n\"\"\").df()\ny = con.sql(\"\"\"\n    SELECT yardsGained\n    FROM gold.plays_numeric   \n\"\"\").df()\n# Train test split\n# May need to come back and apply a Standard Scaler later\n\nlinreg = LinearRegression()\nscaler = StandardScaler()\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.7, random_state = 123)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\n# Fit the model\nlinreg.fit(X_train_scaled, y_train)\n\n\n\nLinearRegression()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n¬†¬†LinearRegression?Documentation for LinearRegressioniFitted\n\nLinearRegression()\n\n\n\n\n\n# Begin testing and scoring\ny_pred = linreg.predict(X_val_scaled)\n\nmse = mean_squared_error(y_val, y_pred)\nr2 = r2_score(y_val, y_pred)\n\nprint(f\"MSE: {mse}\")\nprint(f\"R2 Score: {r2}\")\nMSE: 80.63310622289697\nR2 Score: 0.02277208083008464\nplt.scatter(y_val, y_pred)\nplt.xlabel(\"Actual Values\")\nplt.ylabel(\"Predicted Values\")\nplt.title(\"Linear Regression: Actual vs Predicted\")\nplt.show()\n\ncoefficients = linreg.coef_\nprint(f\"Coefficients: {coefficients}\")\n\n\n\nLinear Regression Scatter Plot\n\n\nCoefficients: [[ 0.05041481  0.32550574  0.04088819  1.77381568 -0.0198335  -0.16104852]]\nridge = Ridge(alpha=1.0)\nridge.fit(X_train_scaled, y_train)\ny_pred_ridge = ridge.predict(X_val_scaled)\nprint(f\"Ridge MSE: {mean_squared_error(y_val, y_pred_ridge)}\")\nRidge MSE: 80.63311884052399",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/posts/nfl_bowl_2025.html#random-forest-notebook-third-model",
    "href": "pages/projects/data_science/posts/nfl_bowl_2025.html#random-forest-notebook-third-model",
    "title": "NFL Big Data Bowl 2025",
    "section": "Random Forest Notebook (Third Model)",
    "text": "Random Forest Notebook (Third Model)\nimport duckdb\nimport tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n# Create the database connection\ncon = duckdb.connect(\"nfl.duckdb\")\n#con.close()\n# Creating dataframes with DuckDB, plays and player_play both have 50 columns, more ideal for a broad random forest\nX = con.sql(\"\"\"\n    SELECT quarter, down, yardsToGo, yardlineNumber, preSnapHomeScore, preSnapVisitorScore,\n    playNullifiedByPenalty, absoluteYardlineNumber, preSnapHomeTeamWinProbability, preSnapVisitorTeamWinProbability, expectedPoints,\n    passResult_complete, passResult_incomplete, passResult_sack, passResult_interception, passResult_scramble, passLength, targetX, targetY,\n    playAction, passTippedAtLine, unblockedPressure, qbSpike, qbKneel, qbSneak, penaltyYards, prePenaltyYardsGained, \n    homeTeamWinProbabilityAdded, visitorTeamWinProbilityAdded, expectedPointsAdded, isDropback, timeToThrow, timeInTackleBox, timeToSack,\n    dropbackDistance, pff_runPassOption, playClockAtSnap, pff_manZone, pff_runConceptPrimary_num, pff_passCoverage_num, pff_runConceptSecondary_num\nFROM silver.plays_rf\n\"\"\").df()\ny = np.array(con.sql(\"\"\"\n    SELECT yardsGained\n    FROM silver.plays_rf\n\"\"\").df()).ravel()\n# Having issues with NA values, the below code does a simple count using pandas, will then go back and change the query\n# As of writing this, the issue is solved; however, the dbt model for this is far from efficient\nna_counts = (X == 'NA').sum()\n\n# Optionally, filter only columns with 'NA' values for easier review\nna_counts_filtered = na_counts[na_counts &gt; 0]\nprint(na_counts_filtered, \"\\n\", X.shape, \"\\n\", y.shape) # playClockAtSnap has only 1 NA value, will just drop that row\nSeries([], dtype: int64) \n (16124, 41) \n (16124,)\n# Instantiate the model and split the data\nrf = RandomForestRegressor(warm_start=True)\n\nselector = RFE(rf, n_features_to_select=10, step=1)\nX_selected = selector.fit_transform(X, y)\n# Begin Interpretation, first with feature importance\nselected_features = X.columns[selector.support_]\nprint(selected_features)\nIndex(['yardlineNumber', 'absoluteYardlineNumber',\n       'preSnapHomeTeamWinProbability', 'expectedPoints',\n       'passResult_scramble', 'penaltyYards', 'prePenaltyYardsGained',\n       'homeTeamWinProbabilityAdded', 'visitorTeamWinProbilityAdded',\n       'expectedPointsAdded'],\n      dtype='object')\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n\n# Train the model\nrf.fit(X_train, y_train)\n\n# Make predictions\ny_pred = rf.predict(X_test)\n\n# Calculate scores\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"R^2 Score: {r2}\")\nMean Squared Error: 1.7769936744186046\nR^2 Score: 0.9766614590863065\n# Continue with the GridSearch\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n}\n\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=4)\ngrid_search.fit(X_train, y_train)\nbest_rf = grid_search.best_estimator_\n\n# Wrap a progress bar for longer Grid Searches\n\"\"\"with tqdm(total=len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf']), desc=\"GridSearch Progress\") as pbar:\n    def callback(*args, **kwargs):\n        pbar.update(1)\n\n    # Add the callback to the grid search\n    grid_search.fit(X, y, callback=callback)\"\"\"\n\nprint(grid_search.best_params_)\n{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n# Continue with the Cross Validation Score\ncv_scores = cross_val_score(rf, X_selected, y, cv=5, scoring='neg_mean_squared_error')\nprint(f\"Cross-validated MSE: {-cv_scores.mean()}\")\nCross-validated MSE: 1.9303851017196607",
    "crumbs": [
      "Data Science and Machine Learning",
      "NFL Big Data Bowl 2025"
    ]
  },
  {
    "objectID": "pages/projects/data_science/data_science.html",
    "href": "pages/projects/data_science/data_science.html",
    "title": "Data Science and Machine Learning Projects",
    "section": "",
    "text": "Landing page for all my project posts related to Data Science and Machine Learning.",
    "crumbs": [
      "Data Science and Machine Learning"
    ]
  },
  {
    "objectID": "pages/projects/data_science/data_science.html#projects",
    "href": "pages/projects/data_science/data_science.html#projects",
    "title": "Data Science and Machine Learning Projects",
    "section": "Projects",
    "text": "Projects",
    "crumbs": [
      "Data Science and Machine Learning"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/oss_data_arch.html#introduction",
    "href": "pages/projects/data_engineering/posts/oss_data_arch.html#introduction",
    "title": "Open Source Data and Analytics Architecture",
    "section": "Introduction",
    "text": "Introduction\nI will update this when I begin the project. The goal here is to explore and create a tech stack to support modern data and analytical workloads, using entirely open source software. Ideally, I‚Äôll be able to scale it to terabytes and then share that template and the guide as a public resource.\nCurrently, I‚Äôm thinking of the following tools, as part of a non-exhaustive list of the stack:\n\n\nOS/Environment: zsh/bash\nProject and Package Management: uv\nCollaboration and Source Control: Github\nDocumentation: Quarto\nData Modeling: dbt\nContainerization: Docker\nContainer Orchestration: Kubernetes\nOLTP Database: PostgreSQL\nOLAP Database: DuckDB\nBatch Ingestion: Python\nETL: dbt\nTesting: pytest\nData Quality: Great Expectations\nMetadata: Unity Catalog\nETL Orchestration: Airflow and/or Dagster\nStreaming Ingestion: Kafka\n\n\nGeneral workflow I‚Äôm envisioning:\n\nInitialize project with uv, add basic dependencies for the environment\nCreate the repo with the GitHub CLI\nSet the remote as the upstream and do the initial commit\nInitialize the quarto and dbt projects as subdirectories of the main, uv project directory\nCreate the postgres container with docker, use this to initialize the postgres database (Prod)\nIn your uv envionrment, initialize the duckdb (Dev/Test) persistent database\n\nSimpler to work quickly with duckdb, postgres has more configurations/overhead, but is better for long term persistent\n\nUse python and duckdb to ingest the initial batch of raw data\nUse dbt to define the data model, pytest to define the basic tests, and great expectations to define data quality\nInitialize the unity catalog instance, add the connection information (Dev/Test/Prod)\nGenerate metadata and lineage\nStart scheduling and orchestrating jobs\nPotentially scale system up to handle stremaing data",
    "crumbs": [
      "Data Engineering and Architecture",
      "Open Source Data and Analytics Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/basic_oss.html",
    "href": "pages/projects/data_engineering/posts/basic_oss.html",
    "title": "Basic OSS Architecture",
    "section": "",
    "text": "The purpose of this project is to showcase the power of open source tools when designing a data and analytics system. I will be walking through my workflow step by step, and including both images, code, and notes.",
    "crumbs": [
      "Data Engineering and Architecture",
      "Basic Open Source Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/basic_oss.html#project-initialization",
    "href": "pages/projects/data_engineering/posts/basic_oss.html#project-initialization",
    "title": "Basic OSS Architecture",
    "section": "Project Initialization",
    "text": "Project Initialization\n\nFirst Steps\nLet‚Äôs start from scratch, a blank VS Code IDE. I‚Äôll do everything from the command line, so make sure to open that up (Ctrl-`).\n\n\n\nBlank IDE\n\n\nFirst, I begin in my home directory. Then, I change to my Documents directory, which I use for all of my projects. This is where I‚Äôll begin creating the project directory and initializing the subsequent tools. As you‚Äôll see below, I first initialize the uv repository and change into it. Then, I create the repo on GitHub (because I like generating the license then), pull (my global is set to merge), commit, and make the initial push. Then, I will initialize the Quarto project, to begin documentation as I work.\n\n\n\nProject and Repo Initialization\n\n\n You can see here that I have both jupyter and dbt already installed. That‚Äôs because uv installs tools system wide, because these are typically used from the CLI. That being said, some CLI tools (like Quarto and DuckDB) in my experience don‚Äôt work with uv because it doesn‚Äôt install their executables.\n\n\n\nGit Remote Add and Pull\n\n\n\n\n\nFirst Commit\n\n\n\nAdding Quarto\nNow, it‚Äôs time to setup some extra functionality in the project. I‚Äôm going to be using Quarto for documentation, so I‚Äôll run quarto create project. To learn more about Quarto and configuring your documentation in projects, checkout my guide. It‚Äôs a fantastic tool for building beautiful, robust documentation, even in enterprise production environments. Consider it for future papers, websites, dashboards, and reports.\nThat being said, if you are ever taking screenshots of your work and want to quickly move them into your images folder, you can do so from the CLI.\n\n\n\nQuarto Project Initialization\n\n\n\n\n\nMoving Images from the CLI\n\n\nNow that you‚Äôve done that, it‚Äôs time to start adding dependencies. As a heads up, don‚Äôt be surprised if you don‚Äôt see the uv.lock or the .venv objects in your directory right away, because uv doesn‚Äôt create those until you add dependencies. Simply run uv add to start adding them. Afterwards, the necessary requirements and lock files will update automatically. If you want to learn more, checkout my uv guide.\n\n\n\nAdd Dependencies with uv\n\n\n\n\nThe Pyproject.toml file\nOnce that‚Äôs done, uv will update the general dependencies in the pyproject.toml file and the specific versions in uv.lock (think requirements.txt on steroids). The nice thing here, it only lists the actual package you needed, not everything else that the package requires. So, when you want to remove packages you can simply use uv remove and the individual package names listed here to remove everything in your environment. There‚Äôs an example below.\n[project]\nname = \"basic-oss-architecture\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.13\"\ndependencies = [\n    \"dbt-core&gt;=1.9.1\",\n    \"dbt-duckdb&gt;=1.9.1\",\n    \"dbt-postgres&gt;=1.9.0\",\n    \"duckdb&gt;=1.1.3\",\n    \"fsspec&gt;=2024.10.0\",\n    \"great-expectations&gt;=0.18.22\",\n    \"jupyter&gt;=1.1.1\",\n    \"pandas&gt;=2.2.3\",\n    \"pytest&gt;=8.3.4\",\n    \"quarto&gt;=0.1.0\",\n    \"requests&gt;=2.32.3\",\n    \"ruff&gt;=0.8.4\",\n]\n\n\nUsing .gitignore effectively\nYou probably noticed, but when you initialize a project with uv it automatically creates a .gitignore file and populates it with basic files and directories which don‚Äôt need to be checked into source control (like .venv). I take this a step further, and add some Quarto specific files and directories too, .quarto and _files folders. Managing this file effectively can drastically reduce the file size of your commits.\nBelow is an example of my file at this early project stage.\n\n\n\n\nInitializing a Data Environment\nNow, you‚Äôll be setting up dbt. Similar to the other CLI tools, dbt uses the dbt init command to create the folder structure necessary for the program to be effective. As you can see below, the process is very easy. You‚Äôll only enter a name for your project, which will (case sensitively) become the name of the dbt directory. Next, I‚Äôll walk through the fundamental pieces of a dbt project in depth.\n\n\n\n\n\n\n\nLower Case Naming Conventions\n\n\n\nDue to some naming conventions and for simplicity, I later changed the name of the dbt projct in my environment to basic_oss. It‚Äôs easier to type when it‚Äôs all lowercase and it fits with the dbt naming conventions better. So, don‚Äôt be surprised if/when you see it later.\n\n\n\n\nThe Data Build Tool (dbt)\nAs I said, creating a dbt project is easy, but it can get confusing from here on out if you‚Äôre alone with the dbt documentation. In my experience, dbt initalizes a logs/ folder in the project root directory not the dbt root directory. So, I make sure to add that to the gitignore file, because I don‚Äôt think that needs to be checked into version control.\nSo, now that you‚Äôve initialized your folder, let‚Äôs go through the basics:\n\nProject Root: Basic_OSS\nIn the case of my project, the root folder is called Basic_OSS. Here, you‚Äôll find the 6 subdirectories, a .gitignore file, a README.md file, and the dbt_project.yml file. The .gitignore can be deleted, because you have one in the project root directory, and for the same reason, so can the README. The dbt project file is the core of your entire data environment, in the same exact way that a _quarto.yml file is the core of your website, book, or documentation project.\nThis is where you‚Äôll configure the actual structure and hierarchy of your environment, along with things like schemas or variables, or aliases.\n\n\nAnalyses\nContains the SQL files for any analysis done that are not part of the core models. Think of these as the SELECT statements for analytical queries, whereas models handle the DDL statements for database architects. Depending on your workflow, this folder could be unused.\n\n\nMacros\nThis is where you can store custom macros (functions) and reusable code written in either SQL or jinja. This is the Python package equivalent for SQL and it‚Äôs often used to ensure DRY (Don‚Äôt Repeat Yourself) principles for ETL and other database work.\n\n\n\n\n\n\nCustom Schema Macro\n\n\n\nBy default, dbt will use main for the schema name. Furthermore, I found that even when specifying the schema in the YAML files, it would just append that to main either as main.example_schema.table or main_example_schema.table. For enterprise purposes, this behavior is intended and explained in dbt docs. For my purposes, I would rather the format and reference behavior be schema.table. To do this, I found and modified a custom macro.\n\n\n\n\n\nModels\nThis is the core of dbt. Models are the SQL tables themselves, as well as the transformations when cleaning and aggregating data (from raw to reporting). If you have a raw schema (where the raw data is temporarily stored) and a clean schema (where cleaned data is persisted), you would have both a raw and clean folder within the models folder. Then, the individual queries would live within those subfolders as the actual tables and views.\nIt is where most of (if not all) your transformations live. So, can become computationally taxing if you aren‚Äôt careful.\nRun with dbt run or dbt run --select {model_directory_name}.\n\n\nSeeds\nThese are flat files containing static data used for mapping (or reference) data. Only use this if your project needs static data. For more on seeds.\nRun with dbt seed.\n\n\nSnapshots\nStores snapshot definitions for versioning and tracking changes in source data over time. These are commonly used for SCDs (slowly changing dimensions) or auditing.\nRun with dbt snapshot.\n\n\nTests\nFairly self explanatory, but this folder contains custom, SQL-defined tests for your models. Dbt allows for both custom tests defined in .sql files and generic tests defined in a YAML. The tests run on various models are defined in the dbt_project.yml file.\n\n\nExtra Notes\nDbt also has the docs/ and dbt_packages/ folders which are for advanced documentation and shareable, modularized code, respectively. Generally speaking, your workflow will really only involve the following parts of a dbt project:\n\nmodels/\ntests/\nmacros/\ndbt_project.yml\n\nThe others are optional and provide functionality, that while useful and powerful in many cases, is not always needed. Now that I‚Äôve got the local directory all configured, it‚Äôs time to start building the container for my PostgreSQL instance (server, cluster, whatever you want to call it).",
    "crumbs": [
      "Data Engineering and Architecture",
      "Basic Open Source Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/basic_oss.html#docker-and-containers",
    "href": "pages/projects/data_engineering/posts/basic_oss.html#docker-and-containers",
    "title": "Basic OSS Architecture",
    "section": "Docker and Containers",
    "text": "Docker and Containers\nDocker is a powerful open-source platform that simplifies the process of developing, packaging, and deploying applications using containers, which are lightweight, portable environments. Unlike traditional virtualization, which replicates an entire computer system, containers virtualize at the operating system (OS) level, creating isolated spaces where applications run with all their dependencies. By isolating apps in containers, Docker ensures that each environment is consistent across different systems, reducing conflicts caused by mismatched dependencies. This approach accelerates development, enhances portability, and enables scalability, making Docker a cornerstone of modern microservices architectures and containerized workflows.\nYou can learn more about Docker either through their open source documentation or DataCamp‚Äôs course by Tim Sangster!\n\nThe Dockerfile and Configuring Your Image\nThe Dockerfile is the foundation of Docker image creation, serving as a script of instructions to define the environment and behavior of your containerized application. Each instruction in the Dockerfile builds on the previous one, forming layers that together create a Docker image.\nA Dockerfile is composed of various instructions, such as:\n\nFROM: Specifies the base image to start with. Always begin with this instruction.\nFROM postgres\nRUN: Executes shell commands during the build process and creates a new layer.\nRUN apt-get update\nCOPY/ADD: Transfers files from your local system into the image.\nCOPY postgres-password.txt /usr/home/\nWORKDIR: Sets the working directory for subsequent instructions.\nWORKDIR /usr/home/\nCMD: Specifies the default command to run when the container starts. Unlike RUN, CMD is executed at runtime.\nCMD [\"postgres\"]\n\n\nOptimizing Builds with Caching\nDocker employs a layer-caching mechanism to optimize builds. Each instruction in the Dockerfile forms a layer, and Docker reuses unchanged layers in subsequent builds to save time. For example:\nRUN apt-get update\nRUN apt-get install -y libpq-dev\nIf you rebuild and these instructions remain unchanged, Docker uses cached results. However, if the base image or any instruction changes, the cache is invalidated for that layer and subsequent ones.\n\n\n\n\n\n\nMaximize Cache Efficiency\n\n\n\nReorder Dockerfile instructions to maximize cache efficiency. Place less frequently changing instructions higher in the file. Then, place the layers you need to test changes in more frequently, lower in the file.\n\n\n\n\nUsing Variables in Dockerfiles\nVariables make Dockerfiles more flexible and maintainable.\n\nARG: Sets build-time variables.\nARG APP_PORT=5000\nRUN echo \"Application port: $APP_PORT\"\nARG values are accessible only during the build process.\nENV: Sets environment variables for runtime.\nENV APP_ENV=production\nThese variables persist after the image is built and can be overridden when running the container using the --env flag.\n\n\n\n\n\n\n\nCredentials\n\n\n\nAvoid storing sensitive data like credentials in ARG or ENV, as they are visible in the image‚Äôs history.\n\n\n\n\nSecurity Best Practices\n\nUse Official Images: Base your Dockerfile on trusted, well-maintained images from sources like Docker Hub.\nMinimize Packages: Install only what your application needs to reduce potential vulnerabilities.\nAvoid Root Users: Run applications with restricted permissions by creating a non-root user:\nRUN useradd -m appuser\nUSER appuser\nUpdate Regularly: Keep your base images and software dependencies up to date.\n\n\n\n\nWrite a PostgreSQL Dockerfile and Build the Image\nBelow is an example Dockerfile for setting up PostgreSQL, with specified user, password, and database name:\n# Use an official PostgreSQL base image\nFROM postgres\n\n# Set environment variables from a local file\nCOPY .pg-password /etc/postgresql/.pg-password\n\n# Read the password from the file and set it as an environment variable\nRUN POSTGRES_PASSWORD=$(cat /etc/postgresql/.pg-password)\n\n# Set additional environment variables\nENV POSTGRES_USER=chris\nENV POSTGRES_DB=test\n\n# Expose the PostgreSQL port\nEXPOSE 5432\n\n# Run commands to configure the environment\nRUN apt-get update && apt-get install -y \\\n    libpq-dev \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Start PostgreSQL service when the container runs\nCMD [\"postgres\"]\nFor the purposes of simplicity in this guide, I‚Äôm just going to leave the POSTGRES_ variables as they are in the file. The password I‚Äôll keep separate to demonstrate how that would work. That being said, after you‚Äôve writen the image‚Äôs Dockerfile, you‚Äôll build the image.\ndocker build -t test .\nIf you need to remove the image at any point, first make sure there are no containers using it, then run the following:\ndocker rmi test\n\n\n\nRunning the Container\nNext, to run the container, you‚Äôll be adding a few flags, which I‚Äôll explain below. For simplicity sake, it‚Äôs probably easiest to store this in a script somewhere and then execute that on start up. **Note** to myself: Add a section on writing local scripts/executables like this later on in the guide.\ndocker run \\\n  --name pg_test \\\n  -e POSTGRES_PASSWORD_FILE=/etc/postgresql/.pg-password \\\n  -e POSTGRES_USER=chris \\\n  -e POSTGRES_DB=test \\\n  -p 5432:5432 \\\n  test\n\n\nFlags Explained\nThe code above will run a container using the Docker image test as the base.\n\nThe container will have the --name pg_test\nYou‚Äôll use the file located at /etc/postgresql/.pg-password to define the POSTGRES_PASSWORD_FILE environment variable\n\nIn Docker, when you use the -e option to pass environment variables, you typically use POSTGRES_PASSWORD_FILE instead of POSTGRES_PASSWORD for file-based password configuration because of how Docker processes environment variables and how the underlying system uses them.\n\nYou‚Äôll also pass the -environment variables for POSTGRES_USER and POSTGRES_DB\n\nThis isn‚Äôt necessary because they are defined in the Dockerfile, so they are globally available within the container.\nIt‚Äôs useful to specify these values in the run command if you want to override the default values or pass different values.\nSpecifying the user and database in the docker run command allows you to control the environment at runtime. More useful in production envrionments, less so for one-off projects like this\n\nFinally, you‚Äôll map the container‚Äôs -port 5432 to your port:5432\n\n\n\nVerifying a Successful Run\nTo verify your container is running, you can use docker ps to get a list of active containers, their image, and other bits of information.\ndocker ps\nCONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS          PORTS                    NAMES\n5793b64919f0   test      \"docker-entrypoint.s‚Ä¶\"   14 seconds ago   Up 13 seconds   0.0.0.0:5432-&gt;5432/tcp   pg_test\nIf you need to stop the running container, simply type:\ndocker kill pg_test\n\n\n\nConnecting to the PostgreSQL Server from your Command-Line Interface\nNow that the server is up and running, you can test an active connection (and your user permissions) from your CLI. To do so, run the following:\nuv run psql -h localhost -U chris -d test\n\nuv run ensures that the command is run in the context of uv‚Äôs environment\npsql is the CLI command for postgres, in the same way gh is for GitHub\n-h tells postgres that the server‚Äôs host is localhost\n-U tells postgres to use the user chris\n-d tells postgres to connect to the database test\n\nYou can close that connection at any time by typing \\q.\n\n\n\n\n\n\n\nNote\n\n\n\nI‚Äôm working on a MacOS laptop and manage local packages (like Python, Docker, gh, and Postgres) with Homebrew. Even though I had PostgreSQLv17 installed, it wasn‚Äôt added to my PATH for some reason. So, when I ran psql ... I got an error. To fix this, I simply edited the ~/.zshrc (the MacOS default terminal zsh configuration file in my home directory) and added export PATH=\"/opt/homebrew/opt/postgresql@17/bin:$PATH\".\n\n\n\n\nExploring the Environment\nNow that you‚Äôre all setup and connected, it‚Äôs time to explore the environment and verify your privileges. In this section I‚Äôll run through some basic commands that you can use to understand whatever postgres environment you‚Äôre in.\nFirst, the \\du command will tell you which roles are available and their attributes:\ntest=# \\du\n\nSecond, the \\l command lists all databases:\ntest=# \\l\n\nThird, the \\dn command lists all schemas:\ntest=# \\dn\n\nFourth, the \\dt command lists all tables. You should expect this output upon creation of a new database:\ntest=# \\dt\nDid not find any relations.\nHere are some other useful commands that will not be useful with a fresh database:\n\n\\dv lists all views\n\\d table_name lists all columns in a table with name table_name\n\\di table_name lists all indexes for a table with name table_name\n\nFinally, to really dive into a specific database, you‚Äôll use the \\c command, but in my case I connected when I first ran the psql command. If you just type psql without the -d flag, you‚Äôll simply connect to a server in general, not a specific database.\nAt this point, the initial project configuration is just about done. The next steps involve initializing the persistent DuckDB database to use as the Dev/Test environment and defining the raw data model with dbt. For now, you can close the connection to the Postgres server and shutdown container.\nThe basic data model and ingestion will all be handled in DuckDB and Python.",
    "crumbs": [
      "Data Engineering and Architecture",
      "Basic Open Source Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/basic_oss.html#duckdb",
    "href": "pages/projects/data_engineering/posts/basic_oss.html#duckdb",
    "title": "Basic OSS Architecture",
    "section": "DuckDB",
    "text": "DuckDB\nDuckDB is:\n\nSimple: Easy to install and deploy, has zero external dependencies, and runs in-process in its host app or as a single binary. Portable: Runs on Linux, macOS, Windows, and all popular hardware architectures, has client APIs for major programming languages Feature-rich: Offers a rich SQL dialect. Can read/write files (CSV, Parquet, and JSON), to and from the local file system, and remote endpoints such as S3 buckets. Fast: Runs analytical queries at blazing speed due to its columnar engine, which supports parallel execution and can process larger-than-memory workloads. Extensible: DuckDB is extensible by third-party features such as new data types, functions, file formats and new SQL syntax. Free: DuckDB and its core extensions are open-source under the permissive MIT License.\n\nTL;DR DuckDB is awesome.\nAs the block of text tells you, while DuckDB is both simple and portable, it is also feature-rich, which is why it‚Äôs great for a development or test environment. It‚Äôs even easy to spin-up an instance of DuckDB in-memory, while you‚Äôre testing or exploring data, and then you can choose to remove that or persist the file in storage.\n\nDuckDB can operate in both persistent mode, where the data is saved to disk, and in in-memory mode, where the entire data set is stored in the main memory. To create or open a persistent database, set the path of the database file, e.g., database.duckdb, when creating the connection. This path can point to an existing database or to a file that does not yet exist and DuckDB will open or create a database at that location as needed. The file may have an arbitrary extension, but .db or .duckdb are two common choices with .ddb also used sometimes. Starting with v0.10, DuckDB‚Äôs storage format is backwards-compatible, i.e., DuckDB is able to read database files produced by an older versions of DuckDB. DuckDB can operate in in-memory mode. In most clients, this can be activated by passing the special value :memory: as the database file or omitting the database file argument. In in-memory mode, no data is persisted to disk, therefore, all data is lost when the process finishes.\n\nFor more information, here are links to some important pieces of DuckDB:\n\nCompression\nConcurrency\nData Import\nDuckDB SQL\nWhy DuckDB?\n\nAs you‚Äôll see, I‚Äôm planning to use DuckDB with Python to ingest raw data from an API (or flat files) and then build the dbt models based off that (with some help from built-in DuckDB functionality). After that, I‚Äôll connect dbt to Postgres and copy the data model over.\n\nA Persistent Database with DuckDB\nAs you saw above, it‚Äôs very easy to get started with DuckDB. To keep with the structure of the project, I‚Äôm going to put the .duckdb file in the root project directory because the Dockerfile (which serves as the Postgres file) is there as well. By design, it‚Äôs easy to create a persistent database and manage the file (there‚Äôs only one). You have two options:\n\nLaunch DuckDB and create your database file on launch (example below)\nLaunch DuckDB, then run .open test.duckdb\n\nFor the purposes of this workflow, it‚Äôs better to create the file when you initially launch DuckDB; however, if you are using DuckDB in-memory for EDA or an adhoc ask, but want to persist your work the second option is valid.\nBasic_OSS_Architecture % duckdb test.duckdb\nv1.1.3 19864453f7\nEnter \".help\" for usage hints.\nD SELECT * FROM duckdb_tables();\n.exit\n\nYou can then run ls -lh to verify the file exists, and to see some basic file metadata: read/write/executable permissions, author, file size, last edited. Next, I need to configure DuckDB to work with dbt.",
    "crumbs": [
      "Data Engineering and Architecture",
      "Basic Open Source Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/basic_oss.html#dbt",
    "href": "pages/projects/data_engineering/posts/basic_oss.html#dbt",
    "title": "Basic OSS Architecture",
    "section": "dbt",
    "text": "dbt\nUp to this point, I‚Äôve initialized the project environment and outside of a few lines of code, most of the work has been automated:\n\nInitialized the project environment with uv and added Python packages\nInitialized the project repository with git and GitHub\nInitialized the documentation envrionment with Quarto\nInitialized the data model with dbt\nWrote a Docker image to contain the PostgreSQL server\nCreated the test database with DuckDB\n\nNow, it‚Äôs time to start connecting everything, which is exactly what dbt does best.\n\ndbt Core (data build tool) is an open-source framework designed for transforming raw data in a warehouse into a well-structured and actionable format. It operates by enabling analysts and engineers to write modular SQL queries and manage their transformations as code, following software engineering best practices like version control and testing. dbt is used to define, test, and document data pipelines, helping teams standardize and automate transformations in a consistent and scalable way. Key benefits include increased collaboration, improved data quality through built-in testing, and the ability to easily track and manage changes in data models over time. It integrates seamlessly with modern cloud data warehouses, making it a cornerstone for modern data engineering workflows.\n\nLearn more about getting started with dbt.\n\nConfiguring Database Connections with dbt\nThere are two contexts when working with dbt locally: the project dbt directory and the global dbt configuration. The first is what I initialized for the sake of this project using dbt init. The second is the profiles.yml file in the ~/.dbt directory, which is used by dbt to connect to different databases. You‚Äôll also need to install the specific connection type extensions, in this case dbt-duckdb and dbt-postgres‚Äì both of these are added like any other dependency with uv add.\nThe file and folder should be created when you first install dbt-core. If the file isn‚Äôt there, you can simply create one. The general syntax is as follows:\nbasic_oss:\n  target: test # The default output for dbt to target (either test or prod in this case)\n  outputs: # The defined outputs for dbt to target\n    test: # Name of the output\n      type: duckdb # Database type\n      path: /Users/chriskornaros/Documents/Basic_OSS_Architecture/test.duckdb # Database file path\n      schema: ingest # Schema to target\n      threads: 4 # The number of threads dbt is allowed to enable for concurrent use\n    prod:\n      type: postgres\n      host: localhost # Hostname of the PostgreSQL server/container\n      port: 5432 # Port postgres listens on\n      user: chris # User must be specified\n      password: # Unfortunately I had to type this in, I haven't found a way to link the key file to profiles.yml yet\n      database: test # Database must be specified\n      schema: stage # Schema to target\n      threads: 4 # Threads for concurrency\n\n\n\n\n\n\nCPUs and Threads\n\n\n\nThreads in a CPU are the smallest units of execution that can run independently, allowing a processor to handle multiple tasks concurrently. They improve performance by utilizing CPU cores more efficiently, especially in multithreaded applications. With Python 3.13, the GIL (global interpreter lock) is gone, you can now actually utilize multiple cores at once. If you want to find out how many cores you have, run sysctl -n hw.logicalcpu (MacOS) or lscpu (Linux).\nLogical CPUs represents the number of threads that the system can handle concurrently, a more accurate measure of how many threads your system can execute simultaneously than Physical CPUs because of hyperthreading.\n\n\nYou‚Äôll also need to configure the project specific YAML file‚Äì dbt_project.yml. This is easy and, for the most part, the default version of the file that is generated by dbt init is good enough for an initial connection. Later on, I‚Äôll go into some other features and properties that you can define which make dbt even more powerful.\n# Name your project! Project names should contain only lowercase characters\n# and underscores. A good package name should reflect your organization's\n# name or the intended use of these models\nname: 'basic_oss'\nversion: '1.0.0'\n\n# This setting configures which \"profile\" dbt uses for this project.\nprofile: 'basic_oss'\n\n# These configurations specify where dbt should look for different types of files.\n# The `model-paths` config, for example, states that models in this project can be\n# found in the \"models/\" directory. You probably won't need to change these!\nmodel-paths: [\"models\"]\nanalysis-paths: [\"analyses\"]\ntest-paths: [\"tests\"]\nseed-paths: [\"seeds\"]\nmacro-paths: [\"macros\"]\nsnapshot-paths: [\"snapshots\"]\n\nclean-targets:         # directories to be removed by `dbt clean`\n  - \"target\"\n  - \"dbt_packages\"\n  - \"logs\"\n\nmodels:\n  basic_oss:\n    ingest:\n      +schema: ingest\n      +materialized: table\n      +enabled: true\n    stage:\n      +schema: stage\n      +materialized: table\n      +enabled: true\nOnce you have those files configured and saved, you can verify that things were done correctly by running the code below and seeing a similar output to the subsequent image.\nNote: It is important that you use uv tool run to execute the command because the dbt-duckdb extension is installed within uv‚Äôs project context, not system wide. So, the debug will fail if you just use dbt debug. Additionally, when running from the main project directory, you need to specify the dbt project directory. Otherwise, dbt cannot find the dbt_project.yml file, there is a siimilar flag for the profiles.yml file, if yours is not in your ~/.dbt folder.\nFurthermore, while VS Code automatically picks up on uv and its environment, you will need to manually activate the virtual environment if working in a standalone terminal. Even inside of that virtual environment, you will need to use uv tool run\nuv tool run dbt debug --profile basic_oss --target test --project-dir basic_oss\nuv tool run dbt debug --profile basic_oss --target prod --project-dir basic_oss",
    "crumbs": [
      "Data Engineering and Architecture",
      "Basic Open Source Architecture"
    ]
  },
  {
    "objectID": "pages/projects/data_engineering/posts/basic_oss.html#data-selection-and-ingestion",
    "href": "pages/projects/data_engineering/posts/basic_oss.html#data-selection-and-ingestion",
    "title": "Basic OSS Architecture",
    "section": "Data Selection and Ingestion",
    "text": "Data Selection and Ingestion\nNow that I have the environment good to go, the databases configured, and dbt connected, it‚Äôs time to pick a dataset and start building the data model. To do this, I‚Äôm first going to add a directory for notebooks (eventually I‚Äôll add one for scripts) and then create a Jupyter notebook. I prefer developing in notebook simply because I can quickly change and test code as I‚Äôm iterating. Then, as a reminder, if you haven‚Äôt installed the Python dependencies, you can do so with uv add.\nIn the following block, you‚Äôll see me use code. This command is the VS Code CLI tool, you can learn more here.\nmkdir notebooks\ncode notebooks/api_axploration.ipynb\n\nuv add dbt-core # The core dbt package, needed for base functionality\nuv add dbt-duckdb # The DuckDB extension, needed to connect with dbt\nuv add dbt-postgres # The postgres extension, needed to conenct with dbt\nuv add duckdb # DuckDB Python package\nuv add fsspec # Dependency for DuckDB JSON functionality\nuv add great-expectations # GX is for data validation/quality, later on\nuv add jupyter # Jupyter will launch a Python kernel and makes the notebooks possible\nuv add pandas # For standard functionality\nuv add pytest # For testing actualy pipeline/script performance, not so much data quality\nuv add quarto # For documentation\nuv add requests # For making API calls\nuv add ruff # An extremly fast Python linter made by Astral, the makers of uv\n\nConfiguring and Running a Jupyter Server\nBefore you can use the notebook, you‚Äôll need to launch and connect to a jupyter server. When developing locally, and not sharing your code or compiled code anywhere, it‚Äôs much easier to not require an authentication token. To allow this behavior, you‚Äôll need to modify your jupyter configuration.\nI modified my global configuration, because I would have to reconfigure jupyter with every project if I did it in the specific project directory. That being said, I‚Äôll include the local example as well.\n# For the global configuration\ncode ~/.jupyter/jupyter_server_config.py\n\nc.ServerApp.token = '' # Find this in the .py file, set the value to '' which means nothing\n# For the local configuration\nmkdir .jupyter\ncode .jupyter/jupyter_server_config.py\n\nc.IdentityProvider.token = ''\nAs you can see, the process is similar, you‚Äôll just manually create the .jupyter/ folder in your project directory first. THe next step is to start the server. Then, you‚Äôll see an ouput similar to the image below. It will tell you useful information like the server address (localhost in this case), version, et cetera; however, you can now leave that terminal window be, it will continue to run in the background. If you need to stop the server, just go back to the window and hit Ctrl-C.\nuv run jupyter server\n\n\n\nUsing The Notebook\nNow that the server is up and running, you can go back to the notebook you created. To use it, I‚Äôll first need to connect to the server (if you see the term kernel don‚Äôt be surprised, that‚Äôs what a single notebook runs on). Luckily, VS Code provides a really nice UI for connecting a notebook to the Jupyter server. In the images below, I‚Äôll walk you through the connection process. First begin by selecting Select Kernel in the top-right corner of the notebook UI. From there, VS Code‚Äôs command palette (the search bar at the top of the window) will interactively walk you through the next steps.\n    \nThen, you can verify your connection by running the first cell (Ctrl-Enter on MacOS).\n\n\n\nGET-ting my Initial Data\nNow that everything is setup and I can actually run Python code within the notebook, it‚Äôs time to pick an API and make a request to actually get the data. If you aren‚Äôt sure where to start looking for an API, PublicAPI has a great site of publicly available datasets. Many of which are accesible both programmatically or through GUIs.\nBefore I jump into the code, two quick notes about HTTP and GET:\n\nThese are protocols used for transferring data over the internet. HTTP (HyperText Transfer Protocol) and HTTPS (Secure HyperText Transfer Protocol) are foundational protocols that allow web browsers and servers to communicate with each other. HTTPS adds a layer of security by encrypting the data exchanged between the client and the server. GET is one of the most common HTTP methods used to request data from a specified resource. It retrieves information from the server without making any changes. This method is used when you want to fetch data, such as retrieving data from an API endpoint. It is simple and effective for read-only operations, such as querying data from a database or fetching information from a web service.\n\nNext, you‚Äôll choose the API you want to server as your data source. I decided to use NASA‚Äôs NEO (Near Earth Object) Feed for the following reasons:\n\nDataset: Information about NEOs, including size, orbital path, and speed. API Endpoint: https://api.nasa.gov/neo/rest/v1/neo/browse?api_key={API key} Data Characteristics: JSON structure with a list of NEOs, each containing details like id, name, nasa_jpl_url, absolute_magnitude_h, estimated_diameter, close_approach_date, and more. Use Case: Ideal for a beginner, as it provides a simple JSON structure with basic information and is updated regularly.\n\nBesides my interest in space, the other perk of using NASA‚Äôs API is that it‚Äôs free and provides a reasonable amount (1,000) of hourly calls. That being said, the documentation isn‚Äôt perfect, but it‚Äôs definitely easy enough to query. Below, I‚Äôll share the first few cells of my notebook. Learn more on the NASA API Website.\n# Import the required packages\nimport requests\nimport pprint # pprint similar to beautiful soup, but better for my specific use case\nimport duckdb\nimport pandas as pd\n\n\n\n\n\n\nNotebook Context\n\n\n\nIf you‚Äôre using VS Code, Jupyter notebook context is the parent directory that you opened the file from. In my case code notebooks/api_exploration.ipynb resulted in the context being the root dir above notebooks (Basic_OSS_Architecture/), instead of notebooks. So, I was initially connecting to a persistent DuckDB file created in my Documents directory, instead of the file within my project directory because my connection string below was ../test.duckdb instead of test.duckdb. As a result, I wasn‚Äôt able to interact with the environment from my CLI.\n\n\n# Open a connection to the persistent database\ncon = duckdb.connect('test.duckdb')\nduckdb.install_extension(\"httpfs\") # Lets you directly query HTTP endpoints\nduckdb.load_extension(\"httpfs\") # Trying this functionality out to simplify the ETL process\n\ncon.sql(\"SELECT * FROM duckdb_tables()\") # Note: Don't use a semi colon in DuckDB for Python\n\n\n\n\n\n\nhttpfs Extension for DuckDb\n\n\n\nI tried using the httpfs extension from DuckDB to handle the API calls; however, you need a specific file at the location, such as .csv or .parquet. It didn‚Äôt work for me, so I ended up doing things differently.\n\n\n# Store the base API information, for this, I'm using NASA's Near Earth Object (NEO) API\napi_key_path = \"/Users/chriskornaros/Documents/local-scripts/.api_keys/nasa/key.txt\"\nwith open(api_key_path, 'r') as file:\n    api_key = file.read().strip()\nstart_date, end_date = \"2024-12-01\", \"2024-12-04\"\n\n# Then Construct the URL with f-string formatting\nurl = f\"https://api.nasa.gov/neo/rest/v1/feed?start_date={start_date}&end_date={end_date}&api_key={api_key}\"\n# Standalone cell for the API response, ensures I don't call too many times\nresponse = requests.get(url)\n\n# Check if the response was successful, if so, print the data in a human readable format\nif response.status_code == 200:\n    data = response.json()\n    pprint.pprint(data)\nelse:\n    print(f\"Error: {response.status_code}\")\n\n\nCreating the Raw Table\nNow that you‚Äôve successfully connected to the API and made your first GET request, it‚Äôs time to get the API output in a format that DuckDB can read into a table. If your API output is anything like mine, it‚Äôll be messy, even with the help of pprint, as you can see below. So, before I can create the table, I need to flatten the dictionary output.\n\nLuckily, there are some simple functions in both Python and DuckDB that can help.\n# Flatten the output and then read it with DuckDB to get the column data types\nflat_data = pd.json_normalize(data['near_earth_objects']['2024-12-02'])\n\nraw = con.sql(\"\"\"CREATE TABLE asteroids AS\n                SELECT * EXCLUDE(nasa_jpl_url, close_approach_data, 'links.self')\n                , unnest(close_approach_data, recursive := true)\n                FROM flat_data\n                \"\"\")\n\ncon.sql(\"SELECT * FROM duckdb_tables()\") # Shows table metadata, validates the script worked as intended\nWhat flat_data did is tell Python that the dictionary values for the key ‚Äònear_earth_objects‚Äô are what I‚Äôm looking for and to get rid of the rest of the information, while elevating the nested hierarchy. It takes that one step further, by specifying the actual date (which are the keys in this dataset). That results in an output that you can view below; however, there is still a column with the DuckDB [struct](https://duckdb.org/docs/sql/data_types/struct) type. Simply put, the struct type is similar to tuples in Python because the data type is actually an ordered list of columns called entries. These entries can even be other lists or structs.\n\nSo, I‚Äôll need to further flatten my dataset. Luckily DuckDB SQL provides a native function for this exact behavior, UNNEST, and lets me execute it in the same statement I‚Äôm creating a table! This effectively does what flattening with Python functions does, the nice thing is you can set the recursive := parameter to true, letting DuckDB also handle nested structs, within a struct. As you can see, the DuckDB query selects everything except for the data source‚Äôs GUI URL, the original struct data, and the API query for the specific NEO object in that row.\nNow that the table structure is done and ready for data. It‚Äôs time to finish off some final configurations for our dbt project. Both files will go in the models/ folder, namely schema.yml and sources.yml. Here‚Äôs a quick note on the purpose of both.\n\nschema.yml\nThe schema.yml file documents and tests the models and columns created within your dbt project, ensuring data quality and consistency. It allows you to define metadata, such as column descriptions, and add constraints like uniqueness or nullability tests to enforce integrity. This fosters trust in your data pipelines, simplifies debugging, and enhances the readability of your data models for developers and stakeholders.\nversion: 1\n\nmodels:\n  - name: asteroids\n    description: \"A table containing detailed information about near-Earth objects, including their estimated dimensions and close approach data, in metric units.\"\n    columns:\n      - name: id\n        description: \"Unique identifier for the asteroid.\"\n      - name: neo_reference_id\n        description: \"Reference ID assigned by NASA's Near Earth Object program.\"\n      - name: name\n        description: \"Name or designation of the asteroid.\"\n\n\nsources.yml\nThe sources.yml file defines the external data sources your dbt project depends on, such as raw tables or views in your database, and ensures their existence and structure are tested before being used. It helps you document where your data originates, making your project more transparent and easier to maintain. By validating source data and organizing dependencies, it reduces the risk of downstream errors and improves collaboration across teams.\nversion: 1\n\nsources:\n  - name: ingest\n    description: \"Data in its base relational form, after being unnested from the API call\"\n    database: basic_oss\n    schema: ingest\n    tables:\n        - name: raw\n          description: \"A table containing the non-formatted API data\"\n          columns:\n            - name: id\n              description: \"Unique identifier for the asteroid.\"\n            - name: neo_reference_id\n              description: \"Reference ID assigned by NASA's Near Earth Object program.\"\n            - name: name\n              description: \"Name or designation of the asteroid.\"\n\n\n\nCreating dbt models\nNow that the dbt project is configured and I did some feature testing in a notebook, it‚Äôs time to actually write the models which will serve as the tables in our database. The sources and schema files live in the models/ directory. Within the models folder, you‚Äôll create subfolders that will house your actual models. I use these subfolders to group models by schema. I‚Äôm going to have two schemas with one model each, initially, to showcase some dbt features.\n\ningest/ will house the model that calls the API and creates the table.\nstage/ will house the data transformations necessary to get the table in a useful state\n\nThe nice thing about DuckDB is that you can directly query files on the internet. Unfortunately, the NASA API I‚Äôm calling returns the data as text, not a file. So, I‚Äôll need to use Python to make the initial call and fetch the raw data. Luckily, as of dbt-core v1.3, you can now add .py files as dbt models. So, the first step, after creating the models subdirectories, is to convert the code in the Jupyter notebook to a Python script.\nFortunately, as of writing this, GitHub Copilot is now free. So, I was able to easily convert the file and then check it using ruff in about 10 seconds. Copilot lets you define the context you‚Äôre working in, so I had it look at the notebook file and the raw file. Then, after making some slight changes (because of specific dbt functionality) I was able to click a button and have Copilot paste the code in the editor. Finally, I used ruff to check the file for any errors and format it. I‚Äôll share images below.\n\nPython Model\nIt‚Äôs important to know that the actual model has a specific format and output requirement. As you‚Äôll see below, instead of a main function, I created a model(dbt, session) function. It‚Äôs similar to a main function, but is specific for enabling dbt to read Python. Furthermore, the entire .py file needs to output one single DataFrame.\n# Import the required dependencies\nimport requests\nimport duckdb\nimport pandas as pd\nfrom datetime import datetime\n\n\ndef read_api_key(file_path):\n    \"\"\"\n    Reads the API key from a file.\n\n    Args:\n        file_path (str): Path to the file containing the API key.\n\n    Returns:\n        str: The API key.\n    \"\"\"\n    with open(file_path, \"r\") as file:\n        return file.read().strip()\n\n\ndef construct_url(api_key, start_date, end_date):\n    \"\"\"\n    Constructs the API URL.\n\n    Args:\n        api_key (str): The API key.\n        start_date (str): The start date for the API query.\n        end_date (str): The end date for the API query.\n\n    Returns:\n        str: The constructed API URL.\n    \"\"\"\n    return f\"https://api.nasa.gov/neo/rest/v1/feed?start_date={start_date}&end_date={end_date}&api_key={api_key}\"\n\n\ndef fetch_data(url):\n    \"\"\"\n    Fetches data from the API.\n\n    Args:\n        url (str): The API URL.\n\n    Returns:\n        dict: The JSON response from the API.\n\n    Raises:\n        Exception: If the API request fails.\n    \"\"\"\n    response = requests.get(url)\n    if response.status_code == 200:\n        return response.json()\n    else:\n        raise Exception(f\"Error: {response.status_code}\")\n\n\ndef process_data(data, date):\n    \"\"\"\n    Processes the API response data using Pandas and DuckDB.\n\n    Args:\n        data (dict): The JSON response from the API.\n        date (str): The date for which to process the data.\n\n    Returns:\n        DataFrame: The processed data as a pandas DataFrame.\n    \"\"\"\n    flat_data = pd.json_normalize(data[\"near_earth_objects\"][date])\n    unnested_data = duckdb.sql(\"\"\"SELECT * EXCLUDE(nasa_jpl_url, close_approach_data, 'links.self')\n                                , unnest(close_approach_data, recursive := true)\n                                FROM {a}\n                                \"\"\").fetchdf().format(a=flat_data) # Test this\n\n    return unnested_data\n\n\ndef model(dbt, session):\n    \"\"\"\n    Main function to execute the data ingestion process.\n    Returns a single DataFrame object.\n    \"\"\"\n    api_key_path = \"/Users/chriskornaros/Documents/local-scripts/.api_keys/nasa/key.txt\"\n    api_key = read_api_key(api_key_path)\n    start_date = \"1900-01-01\"\n    end_date = datetime.today().strftime(\"%Y-%m-%d\")\n\n    url = construct_url(api_key, start_date, end_date)\n\n    try:\n        data = fetch_data(url)\n        final_data = process_data(data)\n    except Exception as e:\n        print(f\"Failed to fetch data: {e}\")\n        final_data = pd.DataFrame()\n\n    return final_data\n\n\nif __name__ == \"__main__\":\n    result = model()\n    print(result)\n  \nruff check\n# All checks passed!\n\n\nSQL Model\nNext, I‚Äôm going to create a simple SQL model to only use the metric data from the table. Additionally, the model cleans up some column names.\n-- Column names and data types taken from the API Exploration Notebook\nWITH a_raw AS (\n    SELECT *\n    FROM main.asteroids\n)\n\nSELECT id, neo_reference_id, name, close_approach_date, close_approach_date_full, epoch_date_close_approach\n    , absolute_magnitude_h, is_potentially_hazardous_asteroid, is_sentry_object    \n    , kilometers_per_second, kilometers_per_hour, kilometers_miss, orbiting_body\n    , \"estimated_diameter.kilometers.estimated_diameter_min\" AS km_estimated_diameter_min\n    , \"estimated_diameter.kilometers.estimated_diameter_max\"AS km_estimated_diameter_max                                                                                 \n    , \"estimated_diameter.meters.estimated_diameter_min\" AS meters_estimated_diameter_min\n    , \"estimated_diameter.meters.estimated_diameter_max\" AS meters_estimated_diameter_max\nFROM a_raw\n\n\n\nRunning your models\nNow that everything is ready, it‚Äôs finally time for dbt to shine! To ensure functionality, as I expect it, I like to cd dbt_dir/ before running the models. Then, once I‚Äôm in the dbt project directory, all I have to do is execute dbt run from my CLI for DBT to start exectuing scripts. That being said, if you only want to test or run a specific model, you can tell dbt exactly which folder to look at using dbt run --select ingest. Just know, even though you may only run one specific folder, dbt will throw an error if there is an issue with a model in another subfolder.",
    "crumbs": [
      "Data Engineering and Architecture",
      "Basic Open Source Architecture"
    ]
  },
  {
    "objectID": "pages/about/about.html",
    "href": "pages/about/about.html",
    "title": "About",
    "section": "",
    "text": "Thanks for visiting! As you can see, this site is pretty messy. That is because my programming skills lie with data, not web design. I‚Äôve thrown this together with the help of Quarto documentation, SASS documentation, Bootstrap Documentation, _brand.yml documentation, and of course LLMs. Feel free to report issue as you find them, or suggest improvements because there is a link to the source repo on every page."
  },
  {
    "objectID": "pages/about/about.html#about-me",
    "href": "pages/about/about.html#about-me",
    "title": "About",
    "section": "About Me",
    "text": "About Me\nI‚Äôm a 26 year old Data Scientist, Engineer, Analyst, Architect, etc. that has two degrees from Tulane University in New Orleans‚Äì BSM, Marketing and Asian Studies; MS, Business Analytics. During COVID, I did an extra year of school and received a Masters in Data Science. As a result of my education, I fell in love with data science and machine learning, but due to common data challenges, I developed a professional passion and skillset for database architecture and data engineering.\nI currently work at General Motors in the Global Markets IT organization. Most of the projects I work on outside of GM use different tools and skillsets than I do during my day job. The reason being that I want to stay up to date with all of the tools and changes in the industry. As a result, I get to play with all of the cool licensed tools, like Databricks and Power BI, at work, but I get to build and learn with open source tools externally.\nSince really jumping in to open source development and learning this year, I‚Äôve come to enjoy the study and field of data even more. I know that long term I want to move into a data strategy and innovation role. Designing and architecting systems that enable people to get more out of their data is something I‚Äôm teaching myself now. That being said, I‚Äôm enthralled with both the process and my progress.\nIn the long run, I want to continue working full time as a Data Engineer (or something similar). That being said, I am interested in advising or consulting both individuals and companies on data strategy.\nYou can take a look at my resume down below. My email is listed."
  },
  {
    "objectID": "pages/guides/posts/quarto.html",
    "href": "pages/guides/posts/quarto.html",
    "title": "Quarto",
    "section": "",
    "text": "A non-exhaustive guide on using Quarto for project documentation and personal branding.",
    "crumbs": [
      "Guides",
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#overview",
    "href": "pages/guides/posts/quarto.html#overview",
    "title": "Quarto",
    "section": "Overview",
    "text": "Overview\nQuarto is:\n\n\nAn open-source scientific and technical publishing system\nAuthor using Jupyter notebooks or with plain text markdown in your favorite editor.\nCreate dynamic content with Python, R, Julia, and Observable.\nPublish reproducible, production quality articles, presentations, dashboards, websites, blogs, and books in HTML, PDF, MS Word ePub, and more.\nShare knowledge and insights organization-wide by publishing to Posit Connect, Confluence, or other publishing systems.\nWrite using Pandoc markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more.\n\n\nDownloading and Updating\nFor simple instructions and a download/install guide using a GUI, visit Quarto - Get Started.\nFor MacOS users, I recommend downloading and learning about Homebrew, the package manager. It drastically simplifies all phases of package management. To install, simply use brew install quarto and you‚Äôre done.",
    "crumbs": [
      "Guides",
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#projects",
    "href": "pages/guides/posts/quarto.html#projects",
    "title": "Quarto",
    "section": "Projects",
    "text": "Projects\nThis section, and the rest of the guide, assume you‚Äôre familiar with and using the uv package and project manager for Python, git for version control, and the GitHub CLI for collaboration. I‚Äôll be referencing all of these tools throughout the rest of the guide. You can read my guide to learn more about uv\n\nGeneral Workflow\nI‚Äôll be walking through the general workflow, but here‚Äôs a quick note about how I use Quarto for Data related projects. I use GitHub as my collaboration/repo hosting tool, so all of my projects have a README.md file. That way, if anyone visits the actual repo, they can view a nicely rendered markdown file, but when I‚Äôm ready to add a project to my website, I‚Äôll copy the contents into a .qmd file. Then, I can add the Quarto specific formatting.\nThis simplifies my general workflow a lot, and makes it easy to formally share and document my research.\n\n\nInitializing a Project\n\nThe create command\nI‚Äôm going to assume you‚Äôve already run the uv init command to initalize your uv project. From there, it‚Äôs easy to start a project with Quarto from the command line, and there are a few built-in project types to further simplify the startup process. Furthermore, Quarto provides a simple command for creating (or initializing) a project (or extension), quarto create project, and a handy setup guide to help you use it. The following code shows you my terminal input and outputs.\nchriskornaros@chriss-air test % quarto create project\n? Type ‚Ä∫ default\n? Directory ‚Ä∫ docs\n? Title (docs) ‚Ä∫ test_docs\nCreating project at /Users/chriskornaros/Documents/test/docs:\n  - Created _quarto.yml\n  - Created .gitignore\n  - Created test_docs.qmd\n? Open With ‚Ä∫ (don't open)\nFor a quick run through: quarto create project initializes a quarto project directory within your current working directory (the uv parent directory), type lets you choose the type of Quarto documentation (book, website, confluence, etc.), title is the title of your homepage (.qmd) file. Personally, I like to remove the docs/.gitignore file because uv creates one when you initialize a project, in the parent directory. So, having just one .gitignore file helps me keep track of things more easily.\nThe only directories I added to docs after it was created by quarto, was a pages directory for various subpages and a brand directory for .scss files, images, etc. For project, blog, or guide specific media files, I kept those within their subpage folder. Here, I keep the various landing pages and their sub directory structures. Ideally, I won‚Äôt have any files in there, but the _quarto.yml file will point to their locations in my personal GitHub repo.\n\n\n\n\n\n\nFile Context in Quarto\n\n\n\nIn my time developing this site, it seems that Quarto can only pickup on files within the context of the docs (or whatever you name your Quarto project) folder. Furthermore, it struggels with absolute context paths, and at most I could get it to work with ../../file.\n\n\n\n\n\nWorking on your Project\nNow that you‚Äôve initalized your project directory, you can begin work! Head over to Quarto Basics for documentation on the basics of .qmd files and authoring with Quarto markdown.\nJust remember, every webpage will need a .qmd file!\n\n\nRendering a Projects\nThis part is blank for now. Rendering websites have some specific components to websites and GitHub pages, that are covered later on. I will update this for other document types in the future.",
    "crumbs": [
      "Guides",
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#configurations",
    "href": "pages/guides/posts/quarto.html#configurations",
    "title": "Quarto",
    "section": "Configurations",
    "text": "Configurations\n\nThe _quarto.yml file\nThis YAML file serves as the primary configuration file for your Quarto project. Similar to other Quarto YAML files, this handles document configurations, but adds the Quarto project features to sync those across documents and for more environment control. You have the ability to define project metadata for all of the different document types. In this example, I used it to define the website configurations, but if you‚Äôre working on a book or dashboard, then it could be used to normalize chapters or visuals as well.\nYou can also specify the formatting, which connects with the _brand.yml file and enables cross referencing of variables and values. Learn more with Quarto Projects.\n\n\nThe _brand.yml File\nThis is a new feature with Quarto 1.6 that allows you to define and save your design specifications in a YAML file. While this file is specific to your Quarto project directories, you can store and share the file across projects or with others to maintain brand consistency. Luckily, there is great documentation if you want more details brand.yml. While there is a lot to cover, I‚Äôll go over some basics to get started. It‚Äôs important to remember that if you specify colors for anything within .qmd files, those will overwrite the defaults in the brand file. Furthermore, Quarto and _brand.yml both utilize the Bootstrap web development framework. For a list of its full default values, visit the repo.\n\nColor\nThis is obviously an important part of all branding. There are two main components:\n\npalette\ntheme colors\n\nPalette lets you specify hexcodes and assign those to various strings. Those string values could be generic terms, like green (if there is a specific shade you would like), or terms specific to brand.yml's theme colors. When you set your default colors in this way, you can then customize the output in the _quarto.yml file. To modify, for example, your navigation bar, just define the background and foreground properties under the navbar property.\nAnother thing to keep in mind with color, just because it‚Äôs available in _brand.yml, like tertiary, doesn‚Äôt mean it‚Äôs defined and functional in the _quarto.yml file. So, you may need to be creative with how you use protected terms, like success, danger, or warning. Doing so allows you to take advantage of the programmatic benefits of the brand file, while specifying several, possibly, similar shades that would be tricky to do just be renaming colors, such as red, blue, or yellow.\nIf you aren‚Äôt sure on what colors or palettes to choose, using an LLM based chatbot can be helpful. This allows you to describe the colors and themes you‚Äôre going for, as well as refine them over time.\n\n\nTypography\nThis section lets you control which font families are included in your Quarto project. Then, you can specify where various fonts are used and for some properties, even change their specific color. As a heads up, the _brand.yml documentation seems to be correct and updated; however, bash code blocks don‚Äôt render the monospace-background the same way. So, while in-line monospace backgrounds and monospace backgrounds for Python (at the very least) will be colored as the documentation says. Bash code blocks will have no background, just the code itself in the specified font color.\n\n\nDefaults\nThis section gives you more control over various defaults, for HTML Bootstrap, Quarto, Shiny, etc. When configuring specific design colors, using the bootstrap default section will allow you to keep your Quarto files simple, while providing a high level of control over design.\n\n\nSASS - Syntactically Awesome Style Sheets\nRemember, whatever you can‚Äôt configure simply in your _brand.yml file, you can do so in a .scss file. For example, if you want to create custom light and dark mode themes, just create .scss files with the appropriate code and place this in your docs (main Quarto project) directory. Below is an example of a dark mode theme. I set the default values for the scss bootstrap variables at the top. Then, I specified the specific rules for various parts of the page. For defined variables, blockquote, you don‚Äôt need a ., but for features specific to quarto rendered sites, add a . before. For example, to modify the look of code blocks, you must use the .sourceCode variable. For child classes, for example the .sourceCode css copy-with-code class, if you want to modify that you‚Äôll need to use .sourceCode pre.copy-with-code. To find out the name of a variable you don‚Äôt know, just inspect the specific element on the webpage, and the class name will translate 1:1 with the variable name. Additionally, for any property that you need to specifically update, you can add the !important tag, which means it will override existing rules, but be careful using this.\nFor a list of all CSS variable properties, visit CSS Web Docs.\n/*-- scss:defaults --*/\n$background: #2E4053;\n$foreground: #F7F7F7;\n$primary: #FF9900;\n$secondary: #56B3FA;\n$tertiary: #655D4B;\n$light: #F7F7F7;\n$dark: #1C1F24;\n$success: #33373D;\n$danger: #1A1D23;\n$info: #56B3FA;\n$warning: #FF7575;\n\n\n/*-- scss:rules --*/\nbody {\n  font-family: 'Open Sans', sans-serif;\n  background-color: $background;\n  color: $foreground;\n}\n\nh1, h2, h3 {\n  color: $danger;\n}\n\nh4, h5, h6 {\n  color: $danger;\n  font-weight: bold;\n}\n\nblockquote {\n  background-color: #2E6490; /* added background color */\n  border-color: $dark;\n  color: $danger !important;\n}\n\ncode {\n  background-color: $success;\n  color: $info;\n}\n\n.sourceCode {\n  background-color: $success;\n  color: $info;\n}\n\n.sourceCode pre.code-with-copy{\n  padding: 0;\n}\n\n.callout-title-container.flex-fill {\n  color: $danger;\n}\n\n\n\n\n\n\nCSS Variable Names\n\n\n\nThere are some weird naming convention differences between _brand.yml and Quarto. The big one is monotone being used to reference block quotes, code blocks, and in-line code in _brand, but in Quarto it renders the in-line code as code and the code blocks as sourceCode. Make sure to use inspect element to be sure on what you‚Äôre changing. CSS class names can get long, especially when referncing nested classes, just experiment and take your time with things.",
    "crumbs": [
      "Guides",
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#websites",
    "href": "pages/guides/posts/quarto.html#websites",
    "title": "Quarto",
    "section": "Websites",
    "text": "Websites\nWebsites really stretch and push the boundaries of what you can accomplish with Quarto. In this section, I‚Äôll walk through a few key points of developing them.\n\nBlogs\nBlogs are a special kind of Quarto website that consists of a collection of posts along with a navigational page that lists them in reverse chronological order. Pretty much all of the information you‚Äôll need about blogs is the same as the parts of this guide covering websites. Just know, it‚Äôs easy to integrate a blog as a subpage of a larger website.\nSimply add the blogs project structure as a subdirectory of pages/. To keep track of things, I made the title of the main blog page blogs.qmd, so it doesn‚Äôt conflict with the index.qmd that is the home page of my whole website. Then, I added post categories within the posts/ directory of the Quarto blogs/ directory.\nThat being said, and I‚Äôm not quite sure why, but the _metadata.yml file\n\n\nRendering Websites\nI run the following code block from my main project directory. My Quarto project directory is a folder called docs. So, I specify to Quarto that I want to render the entire Quarto project docs, but quarto render‚Äôs context is specific to the quarto project directory. Therefore, I need to use the . to specify that I want the rendered .html files put in the Quarto project folder, and sub folder.\nquarto render docs --output-dir .\nConversely, you can specify, within the output property of your _quarto.yml file that output-dir: .\nThis is also the same syntax when previewing your website, using quarto preview docs, the difference is there is no need to specify an output directory. What this does is spin up a jupyter kernel to render your .qmd files, then, it displays the output in a browser. When you hit save on your _quarto.yml, .scss, and .qmd files then the site will automatically update (it doesn‚Äôt for _brand.yml saves).\nOnce you‚Äôve rendered your website, and pushed the commit, the change is reflected in a few mintues.\n\n\n\n\n\n\nquarto preview with uv\n\n\n\nThe ease of using quarto preview is magnified when using uv as your project/package manager. Instead of having to manage various virtual environments and packages, as well as activation and deactivation, uv does it all. Even VS Code picks up on the context uv provides. The terminal will automatically realize you‚Äôre in a uv environment and display output as if you were using a virutal environment (even though you haven‚Äôt activated it).\n\n\n\n\nWebsite Navigation\n\nTop Navigation\nAfter you‚Äôve set your default color values in _brand.yml, make sure to specify the design details at the top of your navbar property. This is useful, even when using .scss files for more specific design control because you can utilize those variables in your light and dark themes.\nFor pages on your top navigation bar that just have a landing page, simple use the following syntax\nnavbar:\n  left:\n    text: \"Page name\"\n    href: path/to/page/file.qmd\n\n\n\n\n\n\nDashes and Intentation Matter in YAML\n\n\n\nNotice when I‚Äôm using a - and not. This is deliberate. In my development, I realized that where you use and specify the dash can affect functionality. Some places require it, some don‚Äôt, and it may depend on the order of various parameters.\n\n\nFor page categories that may have several landing pages, or even subcategories, you‚Äôll need to utilize hybrid navigation which combines Top and Side navigation. On the top, you‚Äôll use the following syntax:\nnavbar:\n  left:\n    text: \"Page group name\"\n    menu:\n      - path/to/page/group/landing.qmd\n      - path/to/page/group/1/landing.qmd\n      - path/to/page/group/2/landing.qmd\nThen, you‚Äôll need to handle the rest in Side Navigation; however, it isn‚Äôt perfect. You can‚Äôt have nested drop down options in your top navigation bar, so the best I came up with was having a landing page for the top level and first tier subcategories, then handled the rest on the sidebar (which only pops up on affiliated pages).\n\n\nSide Navigation\nFor some reason, Side Navigation in Quarto is much more robust and intuitive. That being said, by combining features here with the top bar, you can achieve a fairly dynamic navigation experience.\nThere are a few key differences. To start with, sidebar objects inherit properties from the first defined, so long as none are changed. Second, you‚Äôll want to use an id with the top level landing pages, because this allows you to reference those in your top navigation bar (for more advanced integrations) using the address sidebar:id, although I struggled with this functionality and didn‚Äôt end up using it.\nThe general structure for your first page group is as follows.\nsidebar:\n  - id: guides\n    title: \"Guides\"\n    style: \"docked\"\n    background: dark\n    foreground: light\n    collapse-level: 2\n    contents:\n      - section: \"Guides\"\n        href: pages/guides/guides.qmd\n        contents:\nNow, if that‚Äôs where things end, you could just list pages on and on using the text: href: syntax. That being said, you probably are going to have a few subcategories, and possibly even further nested subcategories. To enable this, don‚Äôt use the text: syntax, instead use section:. This tells Quarto that you are defining a section, rather than just one single page. As you might guess, you can further nest sections, or specific pages, depending on your use of text: and section: with href:. See an example below.\nid: projects\n      title: \"Projects\"\n      contents:\n        - pages/projects/projects.qmd\n        - section: \"Data Engineering and Architecture\"\n          href: pages/projects/data_engineering/data_engineering.qmd\n          contents:\n            - text: \"Bank Marketing ETL\"\n              href: pages/projects/data_engineering/posts/bank_etl.qmd\n            - text: \"Open Source Data and Analytics Architecture\"\n              href: pages/projects/data_engineering/posts/oss_data_arch.qmd\n            - text: \"Basic Open Source Architecture\"\n              href: pages/projects/data_engineering/posts/basic_oss.qmd\nFor subsections, the landing page‚Äôs .qmd file should be specified within an href paramter, under the section line. Additionally, for the collapsable functionality to work consistently in a sidebar, you‚Äôll need it docked. The behavior is inconsistent with floating sidebars. After you‚Äôve set your default color values in _brand.yml, make sure to specify the design details at the top of your sidebar property. Having a section provides the dropdown functionality on your sidebar.\n\n\n\nSharing Websites\nThere are two primary ways to publish your website once you‚Äôre done making edits, assuming you‚Äôre also using GitHub Pages.\n\nquarto render docs\nquarto publish docs\n\nFor simplicity, I chose to use quarto render docs (note that docs is used here because that‚Äôs the name of my main quarto project directory, not because it‚Äôs part of the command itself) because all I need to do is that and then push the changes. With quarto publish docs, it appeared to me that I would need to setup a branch for my git repository and possibly GitHub actions. I will probably do this in the future, for learning purposes, but didn‚Äôt want to for the sake of time.\nThat being said, the official documentation is very straightforward, and regardless of what you choose, there are two common steps:\n\ntouch .nojekyll\n\nThis tells GitHub pages not to do any additional processing of your website, include this in your docs directory\n\nIn a browser go to GitHubPagesRepo &gt; Settings &gt; Code and automation &gt; Pages\n\nThen, make sure Source is set to Deploy from a branch\nSet your branch to the quarto project directory, in your main project folder, docs in my case\n\n\nThen the classic:\n\ngit add docs\ngit commit -m \"Website updates.\"\ngit push\n\n\n\nWebsite Tools\nQuarto offers several out of the box tools to enhance websites. Some of these are incredibly common for marketing or sharing your content, but all add value in their own way: Google Analytics, Twitter Cards, Open Graph, and RSS Feeds to name a few. The nice thing is they are incredibly easy to setup, and begin working immediately. Google Analytics for example tracked me when I was testing changes in the preview mode.\nThat being said, I tried to implement an RSS feed for the website and it broke Quarto. I was still able to render the output, but I was receiving the ‚ÄúSource cannot be Target‚Äù error. To be able to use quarto preview and quarto render (and get a successfull STDOUT) again I had to remove the robot.txt file, the sitemap.xml file, and the feed: true property from the blog landing page files (my guides.qmd for example).",
    "crumbs": [
      "Guides",
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#other-notes",
    "href": "pages/guides/posts/quarto.html#other-notes",
    "title": "Quarto",
    "section": "Other Notes",
    "text": "Other Notes\nI‚Äôll update this section with more notes and tips that come to mind as I finish building out the site, version 1.0. Then, I‚Äôll reorganize what goes here into the proper places on the document.\n\nIf you want to use past .ipynb files as documentation, or add longer write ups to those files, there is a jupyter command\n\njupyter nbconvert file.ipynb --to markdown --output file.md\nmv file.md &gt; file.qmd\nDone! Just make any quarto specific modifications that you need",
    "crumbs": [
      "Guides",
      "Quarto"
    ]
  },
  {
    "objectID": "pages/guides/posts/quarto.html#conclusion",
    "href": "pages/guides/posts/quarto.html#conclusion",
    "title": "Quarto",
    "section": "Conclusion",
    "text": "Conclusion\nNow, you‚Äôre all done with this guide, thank you for reading!\nCurrently, this is only updated to include my notes and thoughts from when I built my personal website. As I use Quarto to create a variety of document types, I will update this Guide with more. Follow me on Bluesky to stay connected with me and up to date with my work.",
    "crumbs": [
      "Guides",
      "Quarto"
    ]
  }
]